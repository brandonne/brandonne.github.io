{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The OpenVoiceOS Project Documentation the OVOS project documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests ! Getting Involved This is an open source project. We would love your help. We have prepared a contributing guide to help you get started. If this is your first PR, or you're not sure where to get started, say hi in OpenVoiceOS Chat and a team member would be happy to mentor you. Join the Discussions for questions and answers. Links Latest Release OpenVoiceOS Chat OpenVoiceOS Website Mycroft Chat Mycroft Forum Mycroft Blog Mycroft Documentation Mycroft API Docs","title":"Introduction"},{"location":"#the-openvoiceos-project-documentation","text":"the OVOS project documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests !","title":"The OpenVoiceOS Project Documentation"},{"location":"#getting-involved","text":"This is an open source project. We would love your help. We have prepared a contributing guide to help you get started. If this is your first PR, or you're not sure where to get started, say hi in OpenVoiceOS Chat and a team member would be happy to mentor you. Join the Discussions for questions and answers.","title":"Getting Involved"},{"location":"#links","text":"Latest Release OpenVoiceOS Chat OpenVoiceOS Website Mycroft Chat Mycroft Forum Mycroft Blog Mycroft Documentation Mycroft API Docs","title":"Links"},{"location":"about/","text":"About OpenVoiceOS Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About"},{"location":"about/#about-openvoiceos","text":"Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About OpenVoiceOS"},{"location":"airplay/","text":"Airplay By default, your OpenVoiceOS device advertises itself as Airplay (v1 - currently) device on your network. This can be used from either the iOS Airplay selection screen if you play some local files, like shown below; Tap / Click the bottom middle Airplay icon on your music player which opens the Airplay devices menu. It should pick up your OpenVoiceOS device automatically from the network. Select the OpenVoiceOS device to re-route your sound output to your OpenVoiceOS device. The Airplay selection menu is also available within other music clients such as the Spotify app. And if that client also supports metadata over MPRIS your OpenVoiceOS device will show it on it's screen as well.","title":"Airplay"},{"location":"airplay/#airplay","text":"By default, your OpenVoiceOS device advertises itself as Airplay (v1 - currently) device on your network. This can be used from either the iOS Airplay selection screen if you play some local files, like shown below; Tap / Click the bottom middle Airplay icon on your music player which opens the Airplay devices menu. It should pick up your OpenVoiceOS device automatically from the network. Select the OpenVoiceOS device to re-route your sound output to your OpenVoiceOS device. The Airplay selection menu is also available within other music clients such as the Spotify app. And if that client also supports metadata over MPRIS your OpenVoiceOS device will show it on it's screen as well.","title":"Airplay"},{"location":"arch_apis/","text":"APIs Skill API The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use. Limitations The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported. Using another Skill's API If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods.","title":"APIs"},{"location":"arch_apis/#apis","text":"","title":"APIs"},{"location":"arch_apis/#skill-api","text":"The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use.","title":"Skill API"},{"location":"arch_apis/#limitations","text":"The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported.","title":"Limitations"},{"location":"arch_apis/#using-another-skills-api","text":"If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods.","title":"Using another Skill's API"},{"location":"arch_backend/","text":"OpenVoiceOS Backends A backend is a service that provides your device with additional tools to function, these could range from managing your skill settings to configuring certain aspects of your device. The backend is optional in OVOS, especially if you're running a single device. A backend can provide: - A nice web ui to configure device, this allows for you to configure once and push it to all your devices. - Free API services for weather and wolfram alpha - Collecting data, e.g. upload ww and utterance audio samples from all your devices Available backends: - Offline: no backend all configuration local - Personal: Self hosted minimal backend - OpenvoiceOS API Service: Not a true backend, provides a proxy for APIs for privacy - Selene: Provided by Mycroft.ai, can be self hosted . The official Mycroft.ai hosting will likely be going away and is deprecated. - Neon: Coming soon Selecting a backend fixme You can go without a backend and go offline and use our free proxy for API services with no accounts. Install the skill-ovos-setup to get the prompt to setup a backend. Without this installed, you will NOT hear the prompt to pick a backend. You can manually change mycroft.conf to enable a different backend if needed, but you will have to install setup skill and add \"setup\" to \"ready_settings\" in mycroft.conf More Information is available in the backend client repo","title":"Backend"},{"location":"arch_backend/#openvoiceos-backends","text":"A backend is a service that provides your device with additional tools to function, these could range from managing your skill settings to configuring certain aspects of your device. The backend is optional in OVOS, especially if you're running a single device. A backend can provide: - A nice web ui to configure device, this allows for you to configure once and push it to all your devices. - Free API services for weather and wolfram alpha - Collecting data, e.g. upload ww and utterance audio samples from all your devices Available backends: - Offline: no backend all configuration local - Personal: Self hosted minimal backend - OpenvoiceOS API Service: Not a true backend, provides a proxy for APIs for privacy - Selene: Provided by Mycroft.ai, can be self hosted . The official Mycroft.ai hosting will likely be going away and is deprecated. - Neon: Coming soon","title":"OpenVoiceOS Backends"},{"location":"arch_backend/#selecting-a-backend","text":"","title":"Selecting a backend"},{"location":"arch_backend/#fixme","text":"You can go without a backend and go offline and use our free proxy for API services with no accounts. Install the skill-ovos-setup to get the prompt to setup a backend. Without this installed, you will NOT hear the prompt to pick a backend. You can manually change mycroft.conf to enable a different backend if needed, but you will have to install setup skill and add \"setup\" to \"ready_settings\" in mycroft.conf More Information is available in the backend client repo","title":"fixme"},{"location":"arch_gui/","text":"GUI Architecture Introduction OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. The display interaction technology is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the Mycroft GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences. Framework Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. Introduction to QML QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements. A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami GUI Extensions OVOS Core supports a GUI Extension framework which allows the GUI service to incorporate additional behaviour for a specific platform. GUI Extensions currently supported: Smartspeaker Extension This extension is responsible for managing the smartspeaker GUI interface behaviour, it supports homescreens and homescreen management. Enabling the smartspeaker GUI extension: Bigscreen Extension This extension is responsible for managing the plasma bigscreen GUI interface behaviour, it supports window management and window behaviour control on specific window managers like Kwin. Enabling the Bigscreen GUI extension: Mobile Extension This extension is responsible for managing the mobile GUI interface behaviour, it supports homescreens and additionally adds support for global page back navigation. Enabling the Mobile GUI extension: Generic Extension This extension provides a generic GUI interface and does not add any additional behaviour, it optionally supports homescreens if the platform or user manually enables it. This extension is enabled by default when no other extension is specified.","title":"GUI Architecture"},{"location":"arch_gui/#gui-architecture","text":"","title":"GUI Architecture"},{"location":"arch_gui/#introduction","text":"OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. The display interaction technology is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the Mycroft GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences.","title":"Introduction"},{"location":"arch_gui/#framework","text":"Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt.","title":"Framework"},{"location":"arch_gui/#introduction-to-qml","text":"QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements. A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami","title":"Introduction to QML"},{"location":"arch_gui/#gui-extensions","text":"OVOS Core supports a GUI Extension framework which allows the GUI service to incorporate additional behaviour for a specific platform. GUI Extensions currently supported:","title":"GUI Extensions"},{"location":"arch_gui/#smartspeaker-extension","text":"This extension is responsible for managing the smartspeaker GUI interface behaviour, it supports homescreens and homescreen management. Enabling the smartspeaker GUI extension:","title":"Smartspeaker Extension"},{"location":"arch_gui/#bigscreen-extension","text":"This extension is responsible for managing the plasma bigscreen GUI interface behaviour, it supports window management and window behaviour control on specific window managers like Kwin. Enabling the Bigscreen GUI extension:","title":"Bigscreen Extension"},{"location":"arch_gui/#mobile-extension","text":"This extension is responsible for managing the mobile GUI interface behaviour, it supports homescreens and additionally adds support for global page back navigation. Enabling the Mobile GUI extension:","title":"Mobile Extension"},{"location":"arch_gui/#generic-extension","text":"This extension provides a generic GUI interface and does not add any additional behaviour, it optionally supports homescreens if the platform or user manually enables it. This extension is enabled by default when no other extension is specified.","title":"Generic Extension"},{"location":"arch_phal/","text":"PHAL Editors Note Lots of cleanup coming here, just placeholder information for now. PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images Plugins Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-mk2 mycroft mark2 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-brightness-control-rpi brightness control ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) ovos-PHAL-plugin-dashboard dashboard control (ovos-shell) ovos-PHAL-plugin-notification-widgets system notifications (ovos-shell) ovos-PHAL-plugin-color-scheme-manager GUI color schemes (ovos-shell) ovos-PHAL-plugin-configuration-provider UI to edit mycroft.conf (ovos-shell) ovos-PHAL-plugin-analog-media-devices video/audio capture devices (OCP) Admin PHAL AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions. Admin Plugins AdminPlugins are just like regular PHAL plugins that run with root privileges Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"PHAL"},{"location":"arch_phal/#phal","text":"Editors Note Lots of cleanup coming here, just placeholder information for now. PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images","title":"PHAL"},{"location":"arch_phal/#plugins","text":"Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-mk2 mycroft mark2 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-brightness-control-rpi brightness control ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) ovos-PHAL-plugin-dashboard dashboard control (ovos-shell) ovos-PHAL-plugin-notification-widgets system notifications (ovos-shell) ovos-PHAL-plugin-color-scheme-manager GUI color schemes (ovos-shell) ovos-PHAL-plugin-configuration-provider UI to edit mycroft.conf (ovos-shell) ovos-PHAL-plugin-analog-media-devices video/audio capture devices (OCP)","title":"Plugins"},{"location":"arch_phal/#admin-phal","text":"AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions.","title":"Admin PHAL"},{"location":"arch_phal/#admin-plugins","text":"AdminPlugins are just like regular PHAL plugins that run with root privileges Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"Admin Plugins"},{"location":"arch_plugins/","text":"Plugin Architecture Coming Soon","title":"Plugins"},{"location":"arch_plugins/#plugin-architecture","text":"Coming Soon","title":"Plugin Architecture"},{"location":"arch_services/","text":"OpenVoiceOS Services Editors Note Major revisions coming here, mostly placeholder information Skills Service The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain Speech Service Speech Client The speech client is responsible for loading STT, VAD and Wake Word plugins Speech is transcribed into text and forwarded to the skills service Hotwords OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . This is usually not desired unless you are looking to completely disabled wake word usage STT Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable Listener You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading VAD Voice Activity Detection is used by the speech client to determine when a user stopped speaking, this indicates the voice command is ready to be executed. Several VAD strategies are supported","title":"Services"},{"location":"arch_services/#openvoiceos-services","text":"Editors Note Major revisions coming here, mostly placeholder information","title":"OpenVoiceOS Services"},{"location":"arch_services/#skills-service","text":"The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain","title":"Skills Service"},{"location":"arch_services/#speech-service","text":"","title":"Speech Service"},{"location":"arch_services/#speech-client","text":"The speech client is responsible for loading STT, VAD and Wake Word plugins Speech is transcribed into text and forwarded to the skills service","title":"Speech Client"},{"location":"arch_services/#hotwords","text":"OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . This is usually not desired unless you are looking to completely disabled wake word usage","title":"Hotwords"},{"location":"arch_services/#stt","text":"Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable","title":"STT"},{"location":"arch_services/#listener","text":"You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading","title":"Listener"},{"location":"arch_services/#vad","text":"Voice Activity Detection is used by the speech client to determine when a user stopped speaking, this indicates the voice command is ready to be executed. Several VAD strategies are supported","title":"VAD"},{"location":"arch_skills/","text":"Skills Architecture OVOS Skills What can a Skill do? Skills give OVOS the ability to perform a variety of functions. They can be installed or removed by the user, and can be easily updated to expand functionality. To get a good idea of what skills to build, let\u2019s talk about the best use cases for a voice assistant, and what types of things OVOS can do. OVOS can run on a variety of platforms from the Linux Desktop to SBCs like the raspberry pi. Different devices will have slightly different use cases. Devices in the home are generally located in the living room or kitchen and are ideal for listening to the news, playing music, general information, using timers while cooking, checking the weather, and other similar activities that are easily accomplished hands free. Basic functions We cover a lot of the basics with our Default Skills, things like Timers, Alarms, Weather, Time and Date, and more. Information We also call this General Question and Answer, and it covers all of those factual questions someone might think to ask a voice assistant. Questions like \u201cwho was the 32nd President of the United States?\u201d, or \u201chow tall is Eiffel Tower?\u201d Although the Default Skills cover a great deal of questions there is room for more. There are many topics that could use a specific skill such as Science, Academics, Movie Info, TV info, and Music info, etc.. Media One of the biggest use cases for Smart Speakers is playing media. The reason media playback is so popular is that it makes playing a song so easy, all you have to do is say \u201cHey Mycroft play the Beatles,\u201d and you can be enjoying music without having to reach for a phone or remote. In addition to listening to music, there are skills that handle videos as well. News Much like listening to music, getting the latest news with a simple voice interaction is extremely convenient. OVOS supports multiple news feeds, and has the ability to support multiple news skills. Smart Home Another popular use case for Voice Assistants is to control Smart Home and IoT products. Within the mycroft ecosystem there are skills for Home Assistant, Wink IoT, Lifx and more, but there are many products that we do not have skill for yet. The open source community has been enthusiastically expanding OVOS's ability to voice control all kinds of smart home products. Games Voice games are becoming more and more popular, especially those that allow multiple users to play together. Trivia games are some of the most popular types of games to develop for voice assistants. There are several games already available for OVOS. There are native voice adventure games, ports of the popular text adventure games from infocom, a Crystal Ball game, a Number Guessing game and much more! OpenVoiceOS Standard Skills Standard Skills Usage Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage. Date / Time skill You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date? Setting an alarm Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM. Setting of timers Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes. Asking the weather You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen. File Browser The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices GUI Framework Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. OVOS uses the standard mycroft-gui framework, you can find the official documentation here","title":"Skills"},{"location":"arch_skills/#skills-architecture","text":"","title":"Skills Architecture"},{"location":"arch_skills/#ovos-skills","text":"","title":"OVOS Skills"},{"location":"arch_skills/#what-can-a-skill-do","text":"Skills give OVOS the ability to perform a variety of functions. They can be installed or removed by the user, and can be easily updated to expand functionality. To get a good idea of what skills to build, let\u2019s talk about the best use cases for a voice assistant, and what types of things OVOS can do. OVOS can run on a variety of platforms from the Linux Desktop to SBCs like the raspberry pi. Different devices will have slightly different use cases. Devices in the home are generally located in the living room or kitchen and are ideal for listening to the news, playing music, general information, using timers while cooking, checking the weather, and other similar activities that are easily accomplished hands free.","title":"What can a Skill do?"},{"location":"arch_skills/#basic-functions","text":"We cover a lot of the basics with our Default Skills, things like Timers, Alarms, Weather, Time and Date, and more.","title":"Basic functions"},{"location":"arch_skills/#information","text":"We also call this General Question and Answer, and it covers all of those factual questions someone might think to ask a voice assistant. Questions like \u201cwho was the 32nd President of the United States?\u201d, or \u201chow tall is Eiffel Tower?\u201d Although the Default Skills cover a great deal of questions there is room for more. There are many topics that could use a specific skill such as Science, Academics, Movie Info, TV info, and Music info, etc..","title":"Information"},{"location":"arch_skills/#media","text":"One of the biggest use cases for Smart Speakers is playing media. The reason media playback is so popular is that it makes playing a song so easy, all you have to do is say \u201cHey Mycroft play the Beatles,\u201d and you can be enjoying music without having to reach for a phone or remote. In addition to listening to music, there are skills that handle videos as well.","title":"Media"},{"location":"arch_skills/#news","text":"Much like listening to music, getting the latest news with a simple voice interaction is extremely convenient. OVOS supports multiple news feeds, and has the ability to support multiple news skills.","title":"News"},{"location":"arch_skills/#smart-home","text":"Another popular use case for Voice Assistants is to control Smart Home and IoT products. Within the mycroft ecosystem there are skills for Home Assistant, Wink IoT, Lifx and more, but there are many products that we do not have skill for yet. The open source community has been enthusiastically expanding OVOS's ability to voice control all kinds of smart home products.","title":"Smart Home"},{"location":"arch_skills/#games","text":"Voice games are becoming more and more popular, especially those that allow multiple users to play together. Trivia games are some of the most popular types of games to develop for voice assistants. There are several games already available for OVOS. There are native voice adventure games, ports of the popular text adventure games from infocom, a Crystal Ball game, a Number Guessing game and much more!","title":"Games"},{"location":"arch_skills/#openvoiceos-standard-skills","text":"","title":"OpenVoiceOS Standard Skills"},{"location":"arch_skills/#standard-skills-usage","text":"Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage.","title":"Standard Skills Usage"},{"location":"arch_skills/#date-time-skill","text":"You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date?","title":"Date / Time skill"},{"location":"arch_skills/#setting-an-alarm","text":"Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM.","title":"Setting an alarm"},{"location":"arch_skills/#setting-of-timers","text":"Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes.","title":"Setting of timers"},{"location":"arch_skills/#asking-the-weather","text":"You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen.","title":"Asking the weather"},{"location":"arch_skills/#file-browser","text":"The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices","title":"File Browser"},{"location":"arch_skills/#gui-framework","text":"Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. OVOS uses the standard mycroft-gui framework, you can find the official documentation here","title":"GUI Framework"},{"location":"architecture/","text":"OpenVoiceOS Architecture Editors Note Most of the text here will not remain, it's my notes so I can create some architecture drawings. todo Bus Service The bus service provides a websocket where all internal events travel You can think of the bus service as OVOS's nervous system The mycroft-bus is considered an internal and private websocket, external clients should not connect directly to it. Please do not expose the messagebus to the outside world! Message A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data . ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for. Sources ovos-core injects the context when it emits an utterance, this can be either typed in the ovos-cli-client or spoken via STT service STT will identify itself as audio ovos-cli-client will identify itself as debug_cli mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests Destinations Output capable services are the cli and the TTS The command line is a debug tool, it will ignore the destination TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" or \"debug_cli\" are the destination A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app) Converse Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False. Active Skill List A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service Making a Skill Active There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.make_active() to manually add our Skill to the front of the Active Skills List. GUI Protocol T[he gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined here The transport protocol works between gui service and the gui clients, mycroft does not directly use the protocol but instead communicates with the gui service via the standard mycroft bus]() OVOS images are powered by ovos-shell , the client side implementation of the gui protocol The GUI library which implements the protocol lives in the mycroft-gui repository. GUI Service OVOS uses the standard mycroft-gui framework, you can find the official documentation here The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core. You can find indepth documentation in the dedicated GUI section of these docs","title":"OpenVoiceOS Architecture"},{"location":"architecture/#openvoiceos-architecture","text":"Editors Note Most of the text here will not remain, it's my notes so I can create some architecture drawings.","title":"OpenVoiceOS Architecture"},{"location":"architecture/#todo","text":"","title":"todo"},{"location":"architecture/#bus-service","text":"The bus service provides a websocket where all internal events travel You can think of the bus service as OVOS's nervous system The mycroft-bus is considered an internal and private websocket, external clients should not connect directly to it. Please do not expose the messagebus to the outside world!","title":"Bus Service"},{"location":"architecture/#message","text":"A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data . ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for.","title":"Message"},{"location":"architecture/#sources","text":"ovos-core injects the context when it emits an utterance, this can be either typed in the ovos-cli-client or spoken via STT service STT will identify itself as audio ovos-cli-client will identify itself as debug_cli mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests","title":"Sources"},{"location":"architecture/#destinations","text":"Output capable services are the cli and the TTS The command line is a debug tool, it will ignore the destination TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" or \"debug_cli\" are the destination A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app)","title":"Destinations"},{"location":"architecture/#converse","text":"Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False.","title":"Converse"},{"location":"architecture/#active-skill-list","text":"A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service","title":"Active Skill List"},{"location":"architecture/#making-a-skill-active","text":"There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.make_active() to manually add our Skill to the front of the Active Skills List.","title":"Making a Skill Active"},{"location":"architecture/#gui-protocol","text":"T[he gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined here The transport protocol works between gui service and the gui clients, mycroft does not directly use the protocol but instead communicates with the gui service via the standard mycroft bus]() OVOS images are powered by ovos-shell , the client side implementation of the gui protocol The GUI library which implements the protocol lives in the mycroft-gui repository.","title":"GUI Protocol"},{"location":"architecture/#gui-service","text":"OVOS uses the standard mycroft-gui framework, you can find the official documentation here The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core. You can find indepth documentation in the dedicated GUI section of these docs","title":"GUI Service"},{"location":"audio_plugins/","text":"Audio Plugins Audio plugins are responsible for handling playback of media, like music and podcasts If mycroft-gui is available these plugins will rarely be used unless ovos is explicitly configured to do so List of Audio plugins Plugin Description ovos-ocp-audio-plugin framework + compatibility layer ovos-audio-plugin-simple sox / aplay / paplay / mpg123 ovos-vlc-plugin vlc audio backend ovos-mplayer-plugin mplayer audio backend","title":"Audio Plugins"},{"location":"audio_plugins/#audio-plugins","text":"Audio plugins are responsible for handling playback of media, like music and podcasts If mycroft-gui is available these plugins will rarely be used unless ovos is explicitly configured to do so","title":"Audio Plugins"},{"location":"audio_plugins/#list-of-audio-plugins","text":"Plugin Description ovos-ocp-audio-plugin framework + compatibility layer ovos-audio-plugin-simple sox / aplay / paplay / mpg123 ovos-vlc-plugin vlc audio backend ovos-mplayer-plugin mplayer audio backend","title":"List of Audio plugins"},{"location":"backend_manager/","text":"","title":"Backend manager"},{"location":"bigscreen/","text":"Introduction: OpenVoiceOS GUI supports various Skills and PHAL plugins that share a voice application interface with Plasma Bigscreen. In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export an environment variable. Exporting the Environment Variable: In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export the environment variable QT_FILE_SELECTORS=mediacenter . This can be done by executing the following command in the terminal: export QT_FILE_SELECTORS=mediacenter This environment variable by default is enabled and added to the Plasma Bigscreen environment. To create your own media center environment store the variable in /etc/environment or /etc/profile.d/99-ovos-media-center.sh Exporting the environment variable QT_FILE_SELECTORS=mediacenter is a necessary step to enable key navigation on Plasma Bigscreen and Media Centers for the Open Voice OS project GUI. With this in place, the user can enjoy seamless key navigation while using the Skills and PHAL plugins on their Plasma Bigscreen and Media Centers.","title":"Bigscreen"},{"location":"bigscreen/#introduction","text":"OpenVoiceOS GUI supports various Skills and PHAL plugins that share a voice application interface with Plasma Bigscreen. In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export an environment variable.","title":"Introduction:"},{"location":"bigscreen/#exporting-the-environment-variable","text":"In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export the environment variable QT_FILE_SELECTORS=mediacenter . This can be done by executing the following command in the terminal: export QT_FILE_SELECTORS=mediacenter This environment variable by default is enabled and added to the Plasma Bigscreen environment. To create your own media center environment store the variable in /etc/environment or /etc/profile.d/99-ovos-media-center.sh Exporting the environment variable QT_FILE_SELECTORS=mediacenter is a necessary step to enable key navigation on Plasma Bigscreen and Media Centers for the Open Voice OS project GUI. With this in place, the user can enjoy seamless key navigation while using the Skills and PHAL plugins on their Plasma Bigscreen and Media Centers.","title":"Exporting the Environment Variable:"},{"location":"btspeaker/","text":"Bluetooth speaker The buildroot edition of OpenVoiceOS by default also acts as a bluetooth speaker. You can find it from any (mobile) device as discoverable within the bluetooth pairing menu. You can pair with it and use your OpenVoiceOS as any other Bluetooth speaker you might own. (NOTE: At the moment, pairing is broken but will be fixed as soon as we get to it)","title":"Bluetooth speaker"},{"location":"btspeaker/#bluetooth-speaker","text":"The buildroot edition of OpenVoiceOS by default also acts as a bluetooth speaker. You can find it from any (mobile) device as discoverable within the bluetooth pairing menu. You can pair with it and use your OpenVoiceOS as any other Bluetooth speaker you might own. (NOTE: At the moment, pairing is broken but will be fixed as soon as we get to it)","title":"Bluetooth speaker"},{"location":"compat/","text":"Compatibility FAQ Do OVOS images run on the mark2? Do OVOS skills work in mycroft-core? Do OPM plugins work in mycroft-core? Does PHAL work with mycroft-core? Known Incompatibilities Do OVOS images run on the mark2? The mark2 developer kit is one of the reference platforms we test and develop against, the ovos-buildroot images can be used in a mark 2 device Do OVOS skills work in mycroft-core? If you are a developer please subclass your skills from OVOSSkill provided in ovos-workshop package We implement all skill development tools under the ovos-workshop library, this allows most OVOS functionality to be used in mycroft-core without problems. This includes intent_layers , decorators and killable_events However some skills may decide to depend on features exclusive to ovos-core, or be missing bug fixes in mycroft-core, therefore we can not ensure 100% compatibility. eg, if a skill depends on the converse deactivated event it may misbehave under mycroft-core because that event is only sent in ovos-core When we introduce new functionality in ovos-core that if used in a skill would cause incompatibilities with mycroft we always make the methods private, as long as a skill does not access any property that starts with an underscore, eg. self._resources , it should work in mycroft-core See the table of Known Incompatibilities Do OPM plugins work in mycroft-core? yes! OPM provides new kinds of plugins not supported by mycroft-core, but STT, TTS, WakeWord and AudioService plugins will work in regular mycroft OPM base classes may contain improvements and new features, such as better caching and automatic viseme generation in TTS plugins, but we try very hard to ensure they remain compatible with mycroft-core dev branch Does PHAL work with mycroft-core? yes! PHAL is a standalone component, it only needs to connect to the mycroft messagebus. You can connect PHAL to vanilla mycroft-core and load any plugin. Depending on the plugin this may make sense or not Known Incompatibilities Here we present a list of know incompatibilities between ovos-core and mycroft-core feature consequence reason workarounds converse deactivated event skill won't know when its no longer active mycroft-core does not emit bus event access private skill property/method skill will not load syntax error port the feature to ovos_workshop using \"mycroft.gui.list.move\" GUI may be messed up mycroft-core does not implement full GUI protocol run ovos-gui, do not run mycroft enclosure","title":"Compatibility"},{"location":"compat/#compatibility-faq","text":"Do OVOS images run on the mark2? Do OVOS skills work in mycroft-core? Do OPM plugins work in mycroft-core? Does PHAL work with mycroft-core? Known Incompatibilities","title":"Compatibility FAQ"},{"location":"compat/#do-ovos-images-run-on-the-mark2","text":"The mark2 developer kit is one of the reference platforms we test and develop against, the ovos-buildroot images can be used in a mark 2 device","title":"Do OVOS images run on the mark2?"},{"location":"compat/#do-ovos-skills-work-in-mycroft-core","text":"If you are a developer please subclass your skills from OVOSSkill provided in ovos-workshop package We implement all skill development tools under the ovos-workshop library, this allows most OVOS functionality to be used in mycroft-core without problems. This includes intent_layers , decorators and killable_events However some skills may decide to depend on features exclusive to ovos-core, or be missing bug fixes in mycroft-core, therefore we can not ensure 100% compatibility. eg, if a skill depends on the converse deactivated event it may misbehave under mycroft-core because that event is only sent in ovos-core When we introduce new functionality in ovos-core that if used in a skill would cause incompatibilities with mycroft we always make the methods private, as long as a skill does not access any property that starts with an underscore, eg. self._resources , it should work in mycroft-core See the table of Known Incompatibilities","title":"Do OVOS skills work in mycroft-core?"},{"location":"compat/#do-opm-plugins-work-in-mycroft-core","text":"yes! OPM provides new kinds of plugins not supported by mycroft-core, but STT, TTS, WakeWord and AudioService plugins will work in regular mycroft OPM base classes may contain improvements and new features, such as better caching and automatic viseme generation in TTS plugins, but we try very hard to ensure they remain compatible with mycroft-core dev branch","title":"Do OPM plugins work in mycroft-core?"},{"location":"compat/#does-phal-work-with-mycroft-core","text":"yes! PHAL is a standalone component, it only needs to connect to the mycroft messagebus. You can connect PHAL to vanilla mycroft-core and load any plugin. Depending on the plugin this may make sense or not","title":"Does PHAL work with mycroft-core?"},{"location":"compat/#known-incompatibilities","text":"Here we present a list of know incompatibilities between ovos-core and mycroft-core feature consequence reason workarounds converse deactivated event skill won't know when its no longer active mycroft-core does not emit bus event access private skill property/method skill will not load syntax error port the feature to ovos_workshop using \"mycroft.gui.list.move\" GUI may be messed up mycroft-core does not implement full GUI protocol run ovos-gui, do not run mycroft enclosure","title":"Known Incompatibilities"},{"location":"config_backend/","text":"OpenVoiceOS Backend Configuration Backend Manager a simple UI for ovos-personal-backend , utility to manage all your devices If you are running ovos-core without a backend OCA provides a similar local interface Install pip install ovos-backend-manager or from source pip install git+https://github.com/OpenVoiceOS/ovos-backend-manager Usage It needs to run on the same machine as the backend, it directly interacts with the databases and configuration files ovos-backend-manager will be available in the command line after installing Available Backend Services Supported Backends ovos-core supports multiple backends under a single unified interface Personal backend - self hosted Selene - https://api.mycroft.ai OpenVoiceOS API Service - https://api.openvoiceos.com Offline - support for setting your own api keys and query services directly Developers do not need to worry about backend details in their applications and skills Identity Information A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe STT Plugin a companion stt plugin is available to use a backend as remote STT provider edit your configuration to use ovos-stt-plugin-selene { \"stt\": { \"module\": \"ovos-stt-plugin-selene\" } } source code Offline Backend OVOS by default runs without a backend, in this case you will need to configure api keys manually This can be done with OCA or by editing mycroft.conf edit your configuration to use the offline backend { \"server\": { \"backend_type\": \"offline\" } } Selene The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. This backend is not considered optional by MycroftAI but is not used by OVOS unless explicitly enabled Selene is AGPL licensed: - backend source code - frontend source code edit your configuration to use the selene backend { \"server\": { \"backend_type\": \"selene\", \"url\": \"https://api.mycroft.ai\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } Personal Backend Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code OVOS API Service OVOS Api Service is not a full backend, it is a set of free proxy services hosted by the OVOS Team for usage in default skills device management functionality and user accounts do not exist, offline mode will be used for these apis edit your configuration to use the OVOS backend { \"server\": { \"backend_type\": \"ovos\", \"url\": \"https://api.openvoiceos.com\" } } source code","title":"Backend Configuration"},{"location":"config_backend/#openvoiceos-backend-configuration","text":"","title":"OpenVoiceOS Backend Configuration"},{"location":"config_backend/#backend-manager","text":"a simple UI for ovos-personal-backend , utility to manage all your devices If you are running ovos-core without a backend OCA provides a similar local interface","title":"Backend Manager"},{"location":"config_backend/#install","text":"pip install ovos-backend-manager or from source pip install git+https://github.com/OpenVoiceOS/ovos-backend-manager","title":"Install"},{"location":"config_backend/#usage","text":"It needs to run on the same machine as the backend, it directly interacts with the databases and configuration files ovos-backend-manager will be available in the command line after installing","title":"Usage"},{"location":"config_backend/#available-backend-services","text":"","title":"Available Backend Services"},{"location":"config_backend/#supported-backends","text":"ovos-core supports multiple backends under a single unified interface Personal backend - self hosted Selene - https://api.mycroft.ai OpenVoiceOS API Service - https://api.openvoiceos.com Offline - support for setting your own api keys and query services directly Developers do not need to worry about backend details in their applications and skills","title":"Supported Backends"},{"location":"config_backend/#identity-information","text":"A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe","title":"Identity Information"},{"location":"config_backend/#stt-plugin","text":"a companion stt plugin is available to use a backend as remote STT provider edit your configuration to use ovos-stt-plugin-selene { \"stt\": { \"module\": \"ovos-stt-plugin-selene\" } } source code","title":"STT Plugin"},{"location":"config_backend/#offline-backend","text":"OVOS by default runs without a backend, in this case you will need to configure api keys manually This can be done with OCA or by editing mycroft.conf edit your configuration to use the offline backend { \"server\": { \"backend_type\": \"offline\" } }","title":"Offline Backend"},{"location":"config_backend/#selene","text":"The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. This backend is not considered optional by MycroftAI but is not used by OVOS unless explicitly enabled Selene is AGPL licensed: - backend source code - frontend source code edit your configuration to use the selene backend { \"server\": { \"backend_type\": \"selene\", \"url\": \"https://api.mycroft.ai\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } }","title":"Selene"},{"location":"config_backend/#personal-backend","text":"Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code","title":"Personal Backend"},{"location":"config_backend/#ovos-api-service","text":"OVOS Api Service is not a full backend, it is a set of free proxy services hosted by the OVOS Team for usage in default skills device management functionality and user accounts do not exist, offline mode will be used for these apis edit your configuration to use the OVOS backend { \"server\": { \"backend_type\": \"ovos\", \"url\": \"https://api.openvoiceos.com\" } } source code","title":"OVOS API Service"},{"location":"config_gui/","text":"OpenVoiceOS GUI Configuration Work in Progress","title":"GUI Configuration"},{"location":"config_gui/#openvoiceos-gui-configuration","text":"Work in Progress","title":"OpenVoiceOS GUI Configuration"},{"location":"config_homescreen/","text":"OpenVoiceOS Home Screen The home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections. Features Night Mode Feature The Night Mode feature lets you quickly switch your home screen into a dark standby clock, reducing the amount of light emitted by your device. This is especially useful if you are using your device in a dark room or at night. You can enable the night mode feature by tapping on the left edge pill button on the home screen. Quick Actions Dashboard The Quick Actions Dashboard provides you with a card-based interface to quickly access and add your most used action. The Quick Actions dashboard comes with a variety of pre-defined actions like the ability to quickly add a new alarm, start a new timer or add a new note. You can also add your own custom actions to the dashboard by tapping on the plus button in the top right corner of the dashboard. The Quick Actions dashboard is accessible by tapping on the right edge pill button on the home screen. Application Launcher OpenVoiceOS comes with support for dedicated voice applications. Voice Applications can be dedicated skills or PHAL plugins, providing their own dedicated user interface. The application launcher will show you a list of all available voice applications. You can access the application launcher by tapping on the center pill button on the bottom of the home screen. Wallpapers The home screen supports custom wallpapers and comes with a bunch of wallpapers to choose from. You can easily change your custom wallpaper by swiping from right to left on the home screen. Widgets Notifications Widget The notifications widget provides you with a quick overview of all your notifications. The notifications bell icon will be displayed in the top left corner of the home screen. You can access the notifications overview by tapping on the bell icon when it is displayed. Timer Widget The timer widget is displayed in top left corner after the notifications bell icon. It will show up when you have an active timer running. Clicking on the timer widget will open the timers overview. Alarm Widget The alarm widget is displayed in top left corner after the timer widget. It will show up when you have an active alarm set. Clicking on the alarm widget will open the alarms overview. Media Player Widget The media player widget is displayed in the bottom of the home screen, It replaces the examples widget when a media player is active. The media player widget will show you the currently playing media and provide you with a quick way to pause, resume or skip the current media. You can also quickly access the media player by tapping the quick display media player button on the right side of the media player widget. Configuration Settings The homescreen has several customizations available. This is sample settings.json file with all of the options explained { \"__mycroft_skill_firstrun\": false, \"weather_skill\": \"skill-weather.openvoiceos\", \"datetime_skill\": \"skill-date-time.mycroftai\", \"examples_skill\": \"ovos-skills-info.openvoiceos\", \"wallpaper\": \"default.jpg\", \"persistent_menu_hint\": false, \"examples_enabled\": true, \"randomize_examples\": true, \"examples_prefix\": true } __mycroft_skill_firstrun: This is automatically set on first load of skill weather_skill: DEPRECATED and has no effect - PR pending datetime_skill: Allows you to use a custom skill to display the date and time. Defaults to skill-ovos-date-time.openvoiceos examples_skill: Allows use of a custom skill for the displayed examples. Defaults to ovos_skills_manager.utils.get_skills_example() function wallpaper: Allows a custom wallpaper to be displayed. Use a complete url without any tilde ~ persistent_menu_hint: When true, displayes a hint of the pull-down menu at the top of the page examples_enabled: When false, the examples at the bottom of the screen will be hidden randomize_examples: When false, the rotation of the examples will follow the way they are loaded examples_prefix: When false, the prefix 'Ask Me' will NOT be displayed with the examples","title":"Homescreen Configuration"},{"location":"config_homescreen/#openvoiceos-home-screen","text":"The home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections.","title":"OpenVoiceOS Home Screen"},{"location":"config_homescreen/#features","text":"","title":"Features"},{"location":"config_homescreen/#night-mode-feature","text":"The Night Mode feature lets you quickly switch your home screen into a dark standby clock, reducing the amount of light emitted by your device. This is especially useful if you are using your device in a dark room or at night. You can enable the night mode feature by tapping on the left edge pill button on the home screen.","title":"Night Mode Feature"},{"location":"config_homescreen/#quick-actions-dashboard","text":"The Quick Actions Dashboard provides you with a card-based interface to quickly access and add your most used action. The Quick Actions dashboard comes with a variety of pre-defined actions like the ability to quickly add a new alarm, start a new timer or add a new note. You can also add your own custom actions to the dashboard by tapping on the plus button in the top right corner of the dashboard. The Quick Actions dashboard is accessible by tapping on the right edge pill button on the home screen.","title":"Quick Actions Dashboard"},{"location":"config_homescreen/#application-launcher","text":"OpenVoiceOS comes with support for dedicated voice applications. Voice Applications can be dedicated skills or PHAL plugins, providing their own dedicated user interface. The application launcher will show you a list of all available voice applications. You can access the application launcher by tapping on the center pill button on the bottom of the home screen.","title":"Application Launcher"},{"location":"config_homescreen/#wallpapers","text":"The home screen supports custom wallpapers and comes with a bunch of wallpapers to choose from. You can easily change your custom wallpaper by swiping from right to left on the home screen.","title":"Wallpapers"},{"location":"config_homescreen/#widgets","text":"","title":"Widgets"},{"location":"config_homescreen/#notifications-widget","text":"The notifications widget provides you with a quick overview of all your notifications. The notifications bell icon will be displayed in the top left corner of the home screen. You can access the notifications overview by tapping on the bell icon when it is displayed.","title":"Notifications Widget"},{"location":"config_homescreen/#timer-widget","text":"The timer widget is displayed in top left corner after the notifications bell icon. It will show up when you have an active timer running. Clicking on the timer widget will open the timers overview.","title":"Timer Widget"},{"location":"config_homescreen/#alarm-widget","text":"The alarm widget is displayed in top left corner after the timer widget. It will show up when you have an active alarm set. Clicking on the alarm widget will open the alarms overview.","title":"Alarm Widget"},{"location":"config_homescreen/#media-player-widget","text":"The media player widget is displayed in the bottom of the home screen, It replaces the examples widget when a media player is active. The media player widget will show you the currently playing media and provide you with a quick way to pause, resume or skip the current media. You can also quickly access the media player by tapping the quick display media player button on the right side of the media player widget.","title":"Media Player Widget"},{"location":"config_homescreen/#configuration","text":"","title":"Configuration"},{"location":"config_homescreen/#settings","text":"The homescreen has several customizations available. This is sample settings.json file with all of the options explained { \"__mycroft_skill_firstrun\": false, \"weather_skill\": \"skill-weather.openvoiceos\", \"datetime_skill\": \"skill-date-time.mycroftai\", \"examples_skill\": \"ovos-skills-info.openvoiceos\", \"wallpaper\": \"default.jpg\", \"persistent_menu_hint\": false, \"examples_enabled\": true, \"randomize_examples\": true, \"examples_prefix\": true } __mycroft_skill_firstrun: This is automatically set on first load of skill weather_skill: DEPRECATED and has no effect - PR pending datetime_skill: Allows you to use a custom skill to display the date and time. Defaults to skill-ovos-date-time.openvoiceos examples_skill: Allows use of a custom skill for the displayed examples. Defaults to ovos_skills_manager.utils.get_skills_example() function wallpaper: Allows a custom wallpaper to be displayed. Use a complete url without any tilde ~ persistent_menu_hint: When true, displayes a hint of the pull-down menu at the top of the page examples_enabled: When false, the examples at the bottom of the screen will be hidden randomize_examples: When false, the rotation of the examples will follow the way they are loaded examples_prefix: When false, the prefix 'Ask Me' will NOT be displayed with the examples","title":"Settings"},{"location":"config_oca/","text":"OCA - OVOS Config Assistant OCA is a user facing interface to configure ovos devices Web UI OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more CLI A command line interface is planned but not yet available to provide equivalent functionality to the Web UI Python utils from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"OVOS Config Assistant"},{"location":"config_oca/#oca-ovos-config-assistant","text":"OCA is a user facing interface to configure ovos devices","title":"OCA - OVOS Config Assistant"},{"location":"config_oca/#web-ui","text":"OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more","title":"Web UI"},{"location":"config_oca/#cli","text":"A command line interface is planned but not yet available to provide equivalent functionality to the Web UI","title":"CLI"},{"location":"config_oca/#python-utils","text":"from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"Python utils"},{"location":"config_ovos_core/","text":"Audio Service The audio service is responsible for loading TTS and Audio plugins All audio playback is handled by this service Native playback Usually playback is triggered by some originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client and ovos cli are considered native for legacy reasons the names for ovos cli and speech client are \"debug_cli\" and \"audio\" respectively TTS Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, \"module\": \"ovos-tts-plugin-mimic2\", \"fallback_module\": \"ovos-tts-plugin-mimic\" }, Audio You can enable additional Audio plugins and define the native sources described above under the \"Audio\" section of mycroft.conf ovos-core uses OCP natively for media playback, you can learn more about OCP here OCP will decide when to call the Audio service and what plugin to use, the main use case is for headless setups without a GUI NOTE: mycroft-core has a \"default-backend\" config option, in ovos-core this option has been deprecated and is always OCP. \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"], \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } }, Configuration Reading Configuration Configuring Configuration protected_keys disable_user_config disable_remote_config Meta Configuration ovos.conf Reading Configuration ovos_config.config.Configuration is a singleton that loads a single config object. The configuration files loaded are determined by ovos.conf as described below and can be in either json or yaml format. if Configuration() is called the following configs would be loaded in this order: {core-path} /configuration/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.mycroft/mycroft.conf (Deprecated) XDG_CONFIG_DIRS + /mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf XDG_CONFIG_HOME (default ~/.config) + /mycroft/mycroft.conf When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files. Configuring Configuration There are a couple of special configuration keys that change the way the configuration stack loads. Default config refers to the config specified at default_config_path in ovos.conf (#1 {core-path}/configuration/mycroft.conf in the stack above). System config refers to the config at /etc/{base_folder}/{config_filename} (#2 /etc/mycroft/mycroft.conf in the stack above). protected_keys A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations. disable_user_config If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths). disable_remote_config If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded. Meta Configuration while mycroft.conf configures the voice assistant, ovos.conf configures the library what this means is that ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this all XDG paths across OpenVoiceOS packages build their paths taking ovos.conf into consideration this feature is what allows downstream voice assistants such as neon-core to change their config files to neon.yaml Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: {core-path} /configuration/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.neon/neon.yaml (Deprecated) XDG_CONFIG_DIRS + /neon/neon.yaml /etc/xdg/neon/neon.yaml XDG_CONFIG_HOME (default ~/.config) + /neon/neon.yaml ovos.conf The ovos_config package determines which config files to load based on ovos.conf . get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf /etc/mycroft/ovos.conf (Deprecated) XDG_CONFIG_DIRS + /OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf XDG_CONFIG_HOME (default ~/.config) + /OpenVoiceOS/ovos.conf A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} } Note : default_config_path should always be an absolute path. This is generally detected automatically, but any manual override must specify an absolute path to a json or yaml config file. Non-Mycroft modules may specify alternate config paths. A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from mycroft with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } }","title":"OVOS Core Configuration"},{"location":"config_ovos_core/#audio-service","text":"The audio service is responsible for loading TTS and Audio plugins All audio playback is handled by this service","title":"Audio Service"},{"location":"config_ovos_core/#native-playback","text":"Usually playback is triggered by some originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client and ovos cli are considered native for legacy reasons the names for ovos cli and speech client are \"debug_cli\" and \"audio\" respectively","title":"Native playback"},{"location":"config_ovos_core/#tts","text":"Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, \"module\": \"ovos-tts-plugin-mimic2\", \"fallback_module\": \"ovos-tts-plugin-mimic\" },","title":"TTS"},{"location":"config_ovos_core/#audio","text":"You can enable additional Audio plugins and define the native sources described above under the \"Audio\" section of mycroft.conf ovos-core uses OCP natively for media playback, you can learn more about OCP here OCP will decide when to call the Audio service and what plugin to use, the main use case is for headless setups without a GUI NOTE: mycroft-core has a \"default-backend\" config option, in ovos-core this option has been deprecated and is always OCP. \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"], \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } },","title":"Audio"},{"location":"config_ovos_core/#configuration","text":"Reading Configuration Configuring Configuration protected_keys disable_user_config disable_remote_config Meta Configuration ovos.conf","title":"Configuration"},{"location":"config_ovos_core/#reading-configuration","text":"ovos_config.config.Configuration is a singleton that loads a single config object. The configuration files loaded are determined by ovos.conf as described below and can be in either json or yaml format. if Configuration() is called the following configs would be loaded in this order: {core-path} /configuration/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.mycroft/mycroft.conf (Deprecated) XDG_CONFIG_DIRS + /mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf XDG_CONFIG_HOME (default ~/.config) + /mycroft/mycroft.conf When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.","title":"Reading Configuration"},{"location":"config_ovos_core/#configuring-configuration","text":"There are a couple of special configuration keys that change the way the configuration stack loads. Default config refers to the config specified at default_config_path in ovos.conf (#1 {core-path}/configuration/mycroft.conf in the stack above). System config refers to the config at /etc/{base_folder}/{config_filename} (#2 /etc/mycroft/mycroft.conf in the stack above).","title":"Configuring Configuration"},{"location":"config_ovos_core/#protected_keys","text":"A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations.","title":"protected_keys"},{"location":"config_ovos_core/#disable_user_config","text":"If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths).","title":"disable_user_config"},{"location":"config_ovos_core/#disable_remote_config","text":"If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded.","title":"disable_remote_config"},{"location":"config_ovos_core/#meta-configuration","text":"while mycroft.conf configures the voice assistant, ovos.conf configures the library what this means is that ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this all XDG paths across OpenVoiceOS packages build their paths taking ovos.conf into consideration this feature is what allows downstream voice assistants such as neon-core to change their config files to neon.yaml Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: {core-path} /configuration/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.neon/neon.yaml (Deprecated) XDG_CONFIG_DIRS + /neon/neon.yaml /etc/xdg/neon/neon.yaml XDG_CONFIG_HOME (default ~/.config) + /neon/neon.yaml","title":"Meta Configuration"},{"location":"config_ovos_core/#ovosconf","text":"The ovos_config package determines which config files to load based on ovos.conf . get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf /etc/mycroft/ovos.conf (Deprecated) XDG_CONFIG_DIRS + /OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf XDG_CONFIG_HOME (default ~/.config) + /OpenVoiceOS/ovos.conf A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} } Note : default_config_path should always be an absolute path. This is generally detected automatically, but any manual override must specify an absolute path to a json or yaml config file. Non-Mycroft modules may specify alternate config paths. A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from mycroft with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } }","title":"ovos.conf"},{"location":"config_security/","text":"OpenVoiceOS Security Securing SSH Most of our guides have you create a user called ovos with a password of ovos, while this makes install easy, it's VERY insecure. As soon as possible, you should secure ssh using a key and disable password authentication. When connecting from a Linux or MacOS client Create a keyfile (you can change ovos to whatever you want) ssh-keygen -t ed25519 -f ~/.ssh/ovos Copy to host (use the same filename as above, specify the user and hostname you are using) ssh-copy-id -i ~/.ssh/ovos ovos@mycroft On your dekstop, edit ~/.ssh/config and add the following lines Host rp2 user ovos IdentityFile ~/.ssh/ovos On your ovos system, edit /etc/ssh/sshd_config and add or uncomment the following line: PasswordAuthentication no restart sshd or reboot sudo systemctl restart sshd Message Bus Security Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem Additionally, it is recommended you change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed","title":"OpenVoiceOS Security"},{"location":"config_security/#openvoiceos-security","text":"","title":"OpenVoiceOS Security"},{"location":"config_security/#securing-ssh","text":"Most of our guides have you create a user called ovos with a password of ovos, while this makes install easy, it's VERY insecure. As soon as possible, you should secure ssh using a key and disable password authentication.","title":"Securing SSH"},{"location":"config_security/#when-connecting-from-a-linux-or-macos-client","text":"Create a keyfile (you can change ovos to whatever you want) ssh-keygen -t ed25519 -f ~/.ssh/ovos Copy to host (use the same filename as above, specify the user and hostname you are using) ssh-copy-id -i ~/.ssh/ovos ovos@mycroft On your dekstop, edit ~/.ssh/config and add the following lines Host rp2 user ovos IdentityFile ~/.ssh/ovos On your ovos system, edit /etc/ssh/sshd_config and add or uncomment the following line: PasswordAuthentication no restart sshd or reboot sudo systemctl restart sshd","title":"When connecting from a Linux or MacOS client"},{"location":"config_security/#message-bus-security","text":"Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem Additionally, it is recommended you change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed","title":"Message Bus Security"},{"location":"config_skillls/","text":"Skill Settings Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional.","title":"Skill Settings"},{"location":"config_skillls/#skill-settings","text":"Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional.","title":"Skill Settings"},{"location":"config_wake_word/","text":"Wake Word Plugins WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes List of Wake Word plugins Plugin Type ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-vosk text samples ovos-ww-plugin-snowboy model ovos-ww-plugin-precise model ovos-ww-plugin-precise-lite model ovos-ww-plugin-nyumaya model ovos-ww-plugin-nyumaya-legacy model neon_ww_plugin_efficientwordnet model mycroft-porcupine-plugin model ovos-ww-plugin-hotkeys keyboard","title":"CHanging the Wake Word"},{"location":"config_wake_word/#wake-word-plugins","text":"WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes","title":"Wake Word Plugins"},{"location":"config_wake_word/#list-of-wake-word-plugins","text":"Plugin Type ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-vosk text samples ovos-ww-plugin-snowboy model ovos-ww-plugin-precise model ovos-ww-plugin-precise-lite model ovos-ww-plugin-nyumaya model ovos-ww-plugin-nyumaya-legacy model neon_ww_plugin_efficientwordnet model mycroft-porcupine-plugin model ovos-ww-plugin-hotkeys keyboard","title":"List of Wake Word plugins"},{"location":"contributing/","text":"Contributed to OpenVoiceOS If this is your first PR, or you're not sure where to get started, say hi in OpenVoiceOS Chat and a team member would be happy to mentor you. Join the Discussions for questions and answers. Links Latest Release OpenVoiceOS Chat Mycroft Chat Mycroft Forum","title":"Contributed to OpenVoiceOS"},{"location":"contributing/#contributed-to-openvoiceos","text":"If this is your first PR, or you're not sure where to get started, say hi in OpenVoiceOS Chat and a team member would be happy to mentor you. Join the Discussions for questions and answers.","title":"Contributed to OpenVoiceOS"},{"location":"contributing/#links","text":"Latest Release OpenVoiceOS Chat Mycroft Chat Mycroft Forum","title":"Links"},{"location":"faq/","text":"Frequently Asked Questions What is OVOS? How did OVOS start? Who is behind OVOS? What is the relationship between OVOS and Mycroft? How does OVOS make money? Where is your website? Does OVOS have any default skills? Does OVOS work offline? Does OVOS depend on any servers? How many voices does OVOS support? Can I change the wake word? Can OVOS run without a wake word? How fast can OVOS respond? How do I run OVOS behind a proxy? What is OVOS? OVOS aims to be a full operating system that is free and open source. The Open Voice Operating System consists of OVOS packages (programs specifically released by the OVOS Project) as well as free software released by third parties such as skills and plugins. OVOS makes it possible to voice enable technology without software that would trample your freedom. Historically OVOS has been used to refer to several things, the team, the github organization and the reference buildroot implementation How did OVOS start? OVOS started as MycroftOS, you can find the original mycroft forums thread here . Over time more mycroft community members joined the project, and it was renamed to OpenVoiceOS to avoid trademark issues. Initially OVOS was focused on bundling mycroft-core and on creating only companion software, but due to contributions not being accepted upstream we now maintain an enhanced reference fork of mycroft-core with extra functionality, while keeping all companion software mycroft-core (dev branch) compatible You can think of OVOS as the unsanctioned \"Mycroft Community Edition\" Who is behind OVOS? Everyone in the OVOS team is a long term mycroft community member and has experience working with the mycroft code base Meet the team: Peter Steenbergen - mycroft community developer since 2018, founder of MycroftOS project Casimiro Ferreira - mycroft community developer since 2017, co-founder of HelloChatterbox Aditya Mehra - mycroft community developer since 2016, mycroft-gui lead developer Daniel McKnight - community developer since 2017, NeonGecko lead developer Parker Seaman - mycroft enthusiast since 2018 Chance - mycroft community developer since 2019, ex-maintainer of lingua_franca currently taking a break, he will be back! What is the relationship between OVOS and Mycroft? Both projects are fully independent, initially OVOS was focused on wrapping mycroft-core with a minimal OS, but as both projects matured, ovos-core was created to include extra functionality and make OVOS development faster and more efficient. OVOS has been committed to keeping our components compatible with Mycroft and many of our changes are submitted to Mycroft to include in their projects at their discretion. How does OVOS make money? We don't, OVOS is a volunteer project with no source of income or business model However, we want to acknowledge Blue Systems and NeonGeckoCom , a lot of the work in OVOS is done on paid company time from these projects Where is your website? website - openvoiceos.com chat - matrix forums - github discussions Does OVOS have any default skills? We provide essential skills and those are bundled in all our reference images. ovos-core does not manage your skills, unlike mycroft it won't install or update anything by itself. if you installed ovos-core manually you also need to install skills manually Does OVOS work offline? yes! by default ovos-core does not require internet. That said individual skills and plugins may require internet and most of the time you will want to use those Does OVOS depend on any servers? no! you can integrate ovos-core with selene or personal backend but that is fully optional we provide some microservices for some of our skills, but you can also use your own api keys How many voices does OVOS support? hundreds! nearly everything in OVOS is modular and configurable, that includes Text To Speech. Voices depend on language and the plugins you have installed, you can find a non-exhaustive list of plugins in the ovos plugins awesome list Can I change the wake word? yes, ovos-core supports several wake word plugins . Additionally, OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word Can OVOS run without a wake word? mostly yes, depending on exactly what you mean by this question OVOS can run without any wake word configured, in this case you will only be able to interact via CLI or button press, best for privacy, not so great for a smart speaker ovos-core also provides a couple experimental settings, if you enable continuous listening then VAD will be used to detect speech and no wake word is needed, just speak to mycroft and it should answer! However, this setting is experimental for a reason, you may find that mycroft answers your TV or even tries to answer itself if your hardware does not have AEC Another experimental setting is hybrid mode, with hybrid mode you can ask follow-up questions, up to 45 seconds after the last mycroft interaction, if you do not interact with mycroft it will go back to waiting for a wake word How fast can OVOS respond? By default, to answer a request: Detects the wake word Records 3 - 10 seconds of audio Transcribes the audio and returns the text transcription , either locally or remotely, depending on the speech-to-text (STT) engine in use Parses the text to understand the intent Sends the text to the intent handler with the highest confidence Allows the Skill to perform some action and provide the text to be spoken Synthesizes audio from the given text, either locally or remotely, depending on the text-to-speech (TTS) engine in use Plays the synthesized spoken audio. Through this process there are a number of factors that can affect the perceived speed of responses: System resources - more processing power and memory never hurts! Network latency - depending on configured plugins, network latency and connection speed can play a significant role in slowing down response times. Streaming STT - we have been experimenting with the use of streaming services. This transcribes audio as it's received rather than waiting for the entire utterance to be finished and sending the resulting audio file to a server to be processed in its entirety. It is possible to switch to a streaming STT service. See STT Plugins for a list of options available. Dialog structure - a long sentence will always take more time to synthesize than a short one. Skill developers can help provide quicker response times by considering the structure of their dialog and breaking that dialog up. TTS Caching - synthesized audio is cached meaning common recently generated phrases don't need to be generated, they can be returned immediately. How do I run OVOS behind a proxy? Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy . If you plan to use OVOS behind a proxy, then you will need to do an additional configuration step. NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic OVOS will be using. We use https traffic on port 443 , primarily for accessing ReST-based APIs. Using OVOS behind a proxy without authentication If you are using OVOS behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://proxy_hostname.com:proxy_port $ export https_port=http://proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\" Using OVOS behind an authenticated proxy If you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://user:password@proxy_hostname.com:proxy_port $ export https_port=http://user:password@proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"What is OVOS? How did OVOS start? Who is behind OVOS? What is the relationship between OVOS and Mycroft? How does OVOS make money? Where is your website? Does OVOS have any default skills? Does OVOS work offline? Does OVOS depend on any servers? How many voices does OVOS support? Can I change the wake word? Can OVOS run without a wake word? How fast can OVOS respond? How do I run OVOS behind a proxy?","title":"Frequently Asked Questions"},{"location":"faq/#what-is-ovos","text":"OVOS aims to be a full operating system that is free and open source. The Open Voice Operating System consists of OVOS packages (programs specifically released by the OVOS Project) as well as free software released by third parties such as skills and plugins. OVOS makes it possible to voice enable technology without software that would trample your freedom. Historically OVOS has been used to refer to several things, the team, the github organization and the reference buildroot implementation","title":"What is OVOS?"},{"location":"faq/#how-did-ovos-start","text":"OVOS started as MycroftOS, you can find the original mycroft forums thread here . Over time more mycroft community members joined the project, and it was renamed to OpenVoiceOS to avoid trademark issues. Initially OVOS was focused on bundling mycroft-core and on creating only companion software, but due to contributions not being accepted upstream we now maintain an enhanced reference fork of mycroft-core with extra functionality, while keeping all companion software mycroft-core (dev branch) compatible You can think of OVOS as the unsanctioned \"Mycroft Community Edition\"","title":"How did OVOS start?"},{"location":"faq/#who-is-behind-ovos","text":"Everyone in the OVOS team is a long term mycroft community member and has experience working with the mycroft code base Meet the team: Peter Steenbergen - mycroft community developer since 2018, founder of MycroftOS project Casimiro Ferreira - mycroft community developer since 2017, co-founder of HelloChatterbox Aditya Mehra - mycroft community developer since 2016, mycroft-gui lead developer Daniel McKnight - community developer since 2017, NeonGecko lead developer Parker Seaman - mycroft enthusiast since 2018 Chance - mycroft community developer since 2019, ex-maintainer of lingua_franca currently taking a break, he will be back!","title":"Who is behind OVOS?"},{"location":"faq/#what-is-the-relationship-between-ovos-and-mycroft","text":"Both projects are fully independent, initially OVOS was focused on wrapping mycroft-core with a minimal OS, but as both projects matured, ovos-core was created to include extra functionality and make OVOS development faster and more efficient. OVOS has been committed to keeping our components compatible with Mycroft and many of our changes are submitted to Mycroft to include in their projects at their discretion.","title":"What is the relationship between OVOS and Mycroft?"},{"location":"faq/#how-does-ovos-make-money","text":"We don't, OVOS is a volunteer project with no source of income or business model However, we want to acknowledge Blue Systems and NeonGeckoCom , a lot of the work in OVOS is done on paid company time from these projects","title":"How does OVOS make money?"},{"location":"faq/#where-is-your-website","text":"website - openvoiceos.com chat - matrix forums - github discussions","title":"Where is your website?"},{"location":"faq/#does-ovos-have-any-default-skills","text":"We provide essential skills and those are bundled in all our reference images. ovos-core does not manage your skills, unlike mycroft it won't install or update anything by itself. if you installed ovos-core manually you also need to install skills manually","title":"Does OVOS have any default skills?"},{"location":"faq/#does-ovos-work-offline","text":"yes! by default ovos-core does not require internet. That said individual skills and plugins may require internet and most of the time you will want to use those","title":"Does OVOS work offline?"},{"location":"faq/#does-ovos-depend-on-any-servers","text":"no! you can integrate ovos-core with selene or personal backend but that is fully optional we provide some microservices for some of our skills, but you can also use your own api keys","title":"Does OVOS depend on any servers?"},{"location":"faq/#how-many-voices-does-ovos-support","text":"hundreds! nearly everything in OVOS is modular and configurable, that includes Text To Speech. Voices depend on language and the plugins you have installed, you can find a non-exhaustive list of plugins in the ovos plugins awesome list","title":"How many voices does OVOS support?"},{"location":"faq/#can-i-change-the-wake-word","text":"yes, ovos-core supports several wake word plugins . Additionally, OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word","title":"Can I change the wake word?"},{"location":"faq/#can-ovos-run-without-a-wake-word","text":"mostly yes, depending on exactly what you mean by this question OVOS can run without any wake word configured, in this case you will only be able to interact via CLI or button press, best for privacy, not so great for a smart speaker ovos-core also provides a couple experimental settings, if you enable continuous listening then VAD will be used to detect speech and no wake word is needed, just speak to mycroft and it should answer! However, this setting is experimental for a reason, you may find that mycroft answers your TV or even tries to answer itself if your hardware does not have AEC Another experimental setting is hybrid mode, with hybrid mode you can ask follow-up questions, up to 45 seconds after the last mycroft interaction, if you do not interact with mycroft it will go back to waiting for a wake word","title":"Can OVOS run without a wake word?"},{"location":"faq/#how-fast-can-ovos-respond","text":"By default, to answer a request: Detects the wake word Records 3 - 10 seconds of audio Transcribes the audio and returns the text transcription , either locally or remotely, depending on the speech-to-text (STT) engine in use Parses the text to understand the intent Sends the text to the intent handler with the highest confidence Allows the Skill to perform some action and provide the text to be spoken Synthesizes audio from the given text, either locally or remotely, depending on the text-to-speech (TTS) engine in use Plays the synthesized spoken audio. Through this process there are a number of factors that can affect the perceived speed of responses: System resources - more processing power and memory never hurts! Network latency - depending on configured plugins, network latency and connection speed can play a significant role in slowing down response times. Streaming STT - we have been experimenting with the use of streaming services. This transcribes audio as it's received rather than waiting for the entire utterance to be finished and sending the resulting audio file to a server to be processed in its entirety. It is possible to switch to a streaming STT service. See STT Plugins for a list of options available. Dialog structure - a long sentence will always take more time to synthesize than a short one. Skill developers can help provide quicker response times by considering the structure of their dialog and breaking that dialog up. TTS Caching - synthesized audio is cached meaning common recently generated phrases don't need to be generated, they can be returned immediately.","title":"How fast can OVOS respond?"},{"location":"faq/#how-do-i-run-ovos-behind-a-proxy","text":"Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy . If you plan to use OVOS behind a proxy, then you will need to do an additional configuration step. NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic OVOS will be using. We use https traffic on port 443 , primarily for accessing ReST-based APIs.","title":"How do I run OVOS behind a proxy?"},{"location":"faq/#using-ovos-behind-a-proxy-without-authentication","text":"If you are using OVOS behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://proxy_hostname.com:proxy_port $ export https_port=http://proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"Using OVOS behind a proxy without authentication"},{"location":"faq/#using-ovos-behind-an-authenticated-proxy","text":"If you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://user:password@proxy_hostname.com:proxy_port $ export https_port=http://user:password@proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"Using OVOS behind an authenticated proxy"},{"location":"friends/","text":"OVOS Friends OpenVoiceOS is part of a larger ecosystem of FOSS voice technology, we work closely with the following projects HiveMind HiveMind is a community-developed superset or extension of OpenVoiceOS With HiveMind, you can extend one (or more, but usually just one!) instance of Mycroft to as many devices as you want, including devices that can't ordinarily run Mycroft! HiveMind's developers have successfully connected to Mycroft from a PinePhone, a 2009 MacBook, and a Raspberry Pi 0, among other devices. Mycroft itself usually runs on our desktop computers or our home servers, but you can use any Mycroft-branded device, or OpenVoiceOS, as your central unit. You find the website here and the source code here Plasma Bigscreen Plasma Bigscreen integrates and uses OpenVoiceOS as voice framework stack to serve voice queries and voice applications (skills with a homescreen), one can easily enable mycroft / ovos integration in the bigscreen launcher by installing ovos core and required services and enabling the integration switch in the bigscreen KCM You find the website here and the source code here NeonGecko Neon was one of the first projects ever to adopt ovos-core as a library to build their own voice assistant, Neon works closely together with OVOS and both projects are mostly compatible You find the website here and the source code here Mycroft Mycroft AI started it all, it was one of the first ever FOSS voice assistants and is the project OVOS descends from. Most applications made for mycroft will work in OVOS and vice-versa You find the website here and the source code here Secret Sauce AI Secret Sauce AI is a coordinated community of tech minded AI enthusiasts working together on projects to identify blockers and improve the basic open source tools and pipeline components in the AI (voice) assistant pipeline (wakeword, ASR, NLU, NLG, TTS). The focus is mostly geared toward deployment on edge devices and self-hosted solutions. This is not a voice assistant project in and of itself, rather Secret Sauce AI helps AI (voice) assistant projects come together as individuals and solve basic problems faced by the entire community. You find the website here and the source code here","title":"Friends"},{"location":"friends/#ovos-friends","text":"OpenVoiceOS is part of a larger ecosystem of FOSS voice technology, we work closely with the following projects","title":"OVOS Friends"},{"location":"friends/#hivemind","text":"HiveMind is a community-developed superset or extension of OpenVoiceOS With HiveMind, you can extend one (or more, but usually just one!) instance of Mycroft to as many devices as you want, including devices that can't ordinarily run Mycroft! HiveMind's developers have successfully connected to Mycroft from a PinePhone, a 2009 MacBook, and a Raspberry Pi 0, among other devices. Mycroft itself usually runs on our desktop computers or our home servers, but you can use any Mycroft-branded device, or OpenVoiceOS, as your central unit. You find the website here and the source code here","title":" HiveMind"},{"location":"friends/#plasma-bigscreen","text":"Plasma Bigscreen integrates and uses OpenVoiceOS as voice framework stack to serve voice queries and voice applications (skills with a homescreen), one can easily enable mycroft / ovos integration in the bigscreen launcher by installing ovos core and required services and enabling the integration switch in the bigscreen KCM You find the website here and the source code here","title":" Plasma Bigscreen"},{"location":"friends/#neongecko","text":"Neon was one of the first projects ever to adopt ovos-core as a library to build their own voice assistant, Neon works closely together with OVOS and both projects are mostly compatible You find the website here and the source code here","title":" NeonGecko"},{"location":"friends/#mycroft","text":"Mycroft AI started it all, it was one of the first ever FOSS voice assistants and is the project OVOS descends from. Most applications made for mycroft will work in OVOS and vice-versa You find the website here and the source code here","title":" Mycroft"},{"location":"friends/#secret-sauce-ai","text":"Secret Sauce AI is a coordinated community of tech minded AI enthusiasts working together on projects to identify blockers and improve the basic open source tools and pipeline components in the AI (voice) assistant pipeline (wakeword, ASR, NLU, NLG, TTS). The focus is mostly geared toward deployment on edge devices and self-hosted solutions. This is not a voice assistant project in and of itself, rather Secret Sauce AI helps AI (voice) assistant projects come together as individuals and solve basic problems faced by the entire community. You find the website here and the source code here","title":" Secret Sauce AI"},{"location":"glossary/","text":"Glossary Editor's Note Some of the more detailed definitions will be moved to other pages, it's just here to keep track of the information for now. The Project The OpenVoiceOS Project (OVOS) All the repositories under OpenVoiceOS organization The OpenVoiceOS Team The team behind OVOS Terms Confirmations Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit. Implicit Confirmation This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. Explicit Confirmation This type of confirmation requires an input from the user to verify everything is correct. Conversations Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. Context Allows for natural conversation by having skills set a \"context\" that can be used by subsequent handlers. Context could be anything from person to location. Context can also create \"bubbles\" of available intent handlers, to make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree . Grapheme All of the letters and letter combinations that represent a phoneme. Home Screen The OpenVoiceOS home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections. Intent When an utterance is classified for its action and entities (e.g. 'turn on the kitchen lights' -> skill: home assistant, action: turn on/off, entity: kitchen lights) mycroft.conf Primary configuration file for the voice assistant. Possible locations: - /home/ovos/.local/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf - /etc/mycroft/mycroft.conf - /home/ovos/.config/mycroft/mycroft.conf - /etc/xdg/mycroft/mycroft.conf - /home/ovos/.mycroft/mycroft.conf More Information OCP OCP stands for OpenVoiceOS Common Play, it is a full fledged media player OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from mycroft-core in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill mycroft-core CommonPlay skill framework is disabled when OCP loads OCP skills have a dedicated MycroftSkill class and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP ovos-core The central repository where the voice assistant \"brain\" is developed OPM OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent from core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Some projects using OPM are ovos-core , hivemind-voice-sat , ovos-personal-backend , ovos-stt-server and ovos-tts-server OVOS-shell The gui service in ovos-core will expose a websocket to the GUI client following the protocol outlined here The GUI library which implements the protocol lives in the mycroft-gui repository, The repository also hosts a development client for skill developers wanting to develop on the desktop. OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images, other distributions may offer alternative implementations such as plasma-bigscreen * or mycroft mark2 OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider PHAL Physical Hardware Abstraction Layer PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images Phoneme The smallest phonetic unit in a language that is capable of conveying a distinction in meaning, as the m of mat and the b of bat in English. Service Prompts and Statements You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. QML Qt Markup Language, the language for Qt Quick UIs. More Information The Mycroft GUI Framework uses QML. STT Speech To Text Also known as ASR, automated speech recognition, the process of converting audio into words TTS Text To Speech The process of generating the audio with the responses Utterance Command, question, or query from a user (eg 'turn on the kitchen lights') Wake Word A specific word or phrase trained used to activate the STT (eg 'hey mycroft') XDG XDG stands for \"Cross-Desktop Group\", and it's a way to help with compatibility between systems. More Information","title":"Glossary"},{"location":"glossary/#glossary","text":"Editor's Note Some of the more detailed definitions will be moved to other pages, it's just here to keep track of the information for now.","title":"Glossary"},{"location":"glossary/#the-project","text":"","title":"The Project"},{"location":"glossary/#the-openvoiceos-project-ovos","text":"All the repositories under OpenVoiceOS organization","title":"The OpenVoiceOS Project (OVOS)"},{"location":"glossary/#the-openvoiceos-team","text":"The team behind OVOS","title":"The OpenVoiceOS Team"},{"location":"glossary/#terms","text":"","title":"Terms"},{"location":"glossary/#confirmations","text":"Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit.","title":"Confirmations"},{"location":"glossary/#implicit-confirmation","text":"This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations.","title":"Implicit Confirmation"},{"location":"glossary/#explicit-confirmation","text":"This type of confirmation requires an input from the user to verify everything is correct.","title":"Explicit Confirmation"},{"location":"glossary/#conversations","text":"Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks.","title":"Conversations"},{"location":"glossary/#context","text":"Allows for natural conversation by having skills set a \"context\" that can be used by subsequent handlers. Context could be anything from person to location. Context can also create \"bubbles\" of available intent handlers, to make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree .","title":"Context"},{"location":"glossary/#grapheme","text":"All of the letters and letter combinations that represent a phoneme.","title":"Grapheme"},{"location":"glossary/#home-screen","text":"The OpenVoiceOS home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections.","title":"Home Screen"},{"location":"glossary/#intent","text":"When an utterance is classified for its action and entities (e.g. 'turn on the kitchen lights' -> skill: home assistant, action: turn on/off, entity: kitchen lights)","title":"Intent"},{"location":"glossary/#mycroftconf","text":"Primary configuration file for the voice assistant. Possible locations: - /home/ovos/.local/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf - /etc/mycroft/mycroft.conf - /home/ovos/.config/mycroft/mycroft.conf - /etc/xdg/mycroft/mycroft.conf - /home/ovos/.mycroft/mycroft.conf More Information","title":"mycroft.conf"},{"location":"glossary/#ocp","text":"OCP stands for OpenVoiceOS Common Play, it is a full fledged media player OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from mycroft-core in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill mycroft-core CommonPlay skill framework is disabled when OCP loads OCP skills have a dedicated MycroftSkill class and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP","title":"OCP"},{"location":"glossary/#ovos-core","text":"The central repository where the voice assistant \"brain\" is developed","title":"ovos-core"},{"location":"glossary/#opm","text":"OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent from core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Some projects using OPM are ovos-core , hivemind-voice-sat , ovos-personal-backend , ovos-stt-server and ovos-tts-server","title":"OPM"},{"location":"glossary/#ovos-shell","text":"The gui service in ovos-core will expose a websocket to the GUI client following the protocol outlined here The GUI library which implements the protocol lives in the mycroft-gui repository, The repository also hosts a development client for skill developers wanting to develop on the desktop. OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images, other distributions may offer alternative implementations such as plasma-bigscreen * or mycroft mark2 OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider","title":"OVOS-shell"},{"location":"glossary/#phal","text":"Physical Hardware Abstraction Layer PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images","title":"PHAL"},{"location":"glossary/#phoneme","text":"The smallest phonetic unit in a language that is capable of conveying a distinction in meaning, as the m of mat and the b of bat in English.","title":"Phoneme"},{"location":"glossary/#service","text":"","title":"Service"},{"location":"glossary/#prompts-and-statements","text":"You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response.","title":"Prompts and Statements"},{"location":"glossary/#qml","text":"Qt Markup Language, the language for Qt Quick UIs. More Information The Mycroft GUI Framework uses QML.","title":"QML"},{"location":"glossary/#stt","text":"Speech To Text Also known as ASR, automated speech recognition, the process of converting audio into words","title":"STT"},{"location":"glossary/#tts","text":"Text To Speech The process of generating the audio with the responses","title":"TTS"},{"location":"glossary/#utterance","text":"Command, question, or query from a user (eg 'turn on the kitchen lights')","title":"Utterance"},{"location":"glossary/#wake-word","text":"A specific word or phrase trained used to activate the STT (eg 'hey mycroft')","title":"Wake Word"},{"location":"glossary/#xdg","text":"XDG stands for \"Cross-Desktop Group\", and it's a way to help with compatibility between systems. More Information","title":"XDG"},{"location":"gs_first_boot/","text":"Booting your OpenVoiceOS device. Depending on which image you downloaded you will first see the boot splash which indicates the Operating System is booting. For the buildroot edition the below boot splash will be shown. If this is the very first time you boot your device, booting might take a bit longer as normal as the system is preparing its local filesystem and extending it over the full size of the sdcard/USB device. Eventually the progress bar will be filled up indicating the Operating System has been fully booted, after which the ovos-shell animated loading screen will be shown. Again, if this is the first time you boot your device this might take a bit longer as the ovos-core configuration is populated and skills are being setup. Setting up your Wi-Fi network connection Depending on which image you downloaded you will be greeted by the network configuration screen with either one or two option. The buildroot image supports setting up the network via two options. On a mobile device On the OpenVoiceOS device itself. You can also skip this step to configure it later or never ask it again if you want your device to run fully offline. (Bear in mind you need to configure your device to use local STT and TTS engines and obvious asking your device things that require internet will not work.) On Mobile Setup Choosing this option will create a temporarily open network - hotspot called \"OVOS\" to which you can connect from your mobile device. On your mobile device go into Settings -> Wi-Fi Settings and the \"OVOS\" open network will appear in its list. Connect your device with the OVOS network and a webpage will open to configure the Wi-Fi network on your OpenVoiceOS device. (NOTE: On newer mobile operating systems the captive portal capture has changed and the website will not automatically be opened. If this is the case you can open a browser manually and go to http://172.16.127.1 ) The following webpage will be shown; Select your Wi-Fi network from the list, insert your password and press the \"Connect\" button. If everything went fine, you will soon see the green \"connected\" screen on your OpenVoiceOS device. On Device Setup Choosing this option allows you to set up your Wi-Fi network straight on your OpenVoiceOS device. If selected a screen with the available networks will be shown on your OpenVoiceOS device. Select your network from the list and tap / click on it to allow you to insert your password. If you have a touch screen an on-screen keyboard will appear when you tap the password field. If not use a keyboard. When you have inserted your password, click / tap the connect button and after a short connecting animation, if all went fine you will see the green \"connected\" screen on your OpenVoiceOS device. (Re)configure your network from the drop-down menu If you have skipped the Wi-Fi network setup before, or you just want to reconfigure your network. On the homescreen of your OpenVoiceOS device swipe down the top menu and click the \"Wi-Fi\" icon. This brings you to the same on-device configuration screen. From here you can select another network or click the configuration icon on the right of connected network for details or to remove it from the configured networks. Selecting Your Backend What is a backend ? Mycroft A.I. - Selene Backend The Pairing Process The GUI will now show you a Pairing Code, This pairing code needs to be entered on the mycroft backend which you can find online at https://account.mycroft.ai Create an account using your email id on https://account.mycroft.ai Head over to https://account.mycroft.ai/devices/add Enter the pairing code, a unique device name, and location settings Click next on the web interface, your device should now be paired No backend - No calling home Select A Text To Speech (TTS) Engine: A text-to-speech (TTS) system converts normal language text into speech, select an engine from the list Select A Speech To Text (STT) Engine: A speech-to-text (STT) system converts human speech to text, select an engine from the list Personal backend - Host your own Personal backend is a reverse engineered alternative to selene and requires the backend to be hosted locally. Install and Configure your personal backend, information available on: https://github.com/OpenVoiceOS/ovos-personal-backend The gui on device will display a setup page to enter the host address of your hosted backend on your device Pairing with your personal backend happens automatically once you hit the confirm button with the correct host address","title":"First boot"},{"location":"gs_first_boot/#booting-your-openvoiceos-device","text":"Depending on which image you downloaded you will first see the boot splash which indicates the Operating System is booting. For the buildroot edition the below boot splash will be shown. If this is the very first time you boot your device, booting might take a bit longer as normal as the system is preparing its local filesystem and extending it over the full size of the sdcard/USB device. Eventually the progress bar will be filled up indicating the Operating System has been fully booted, after which the ovos-shell animated loading screen will be shown. Again, if this is the first time you boot your device this might take a bit longer as the ovos-core configuration is populated and skills are being setup.","title":"Booting your OpenVoiceOS device."},{"location":"gs_first_boot/#setting-up-your-wi-fi-network-connection","text":"Depending on which image you downloaded you will be greeted by the network configuration screen with either one or two option. The buildroot image supports setting up the network via two options. On a mobile device On the OpenVoiceOS device itself. You can also skip this step to configure it later or never ask it again if you want your device to run fully offline. (Bear in mind you need to configure your device to use local STT and TTS engines and obvious asking your device things that require internet will not work.)","title":"Setting up your Wi-Fi network connection"},{"location":"gs_first_boot/#on-mobile-setup","text":"Choosing this option will create a temporarily open network - hotspot called \"OVOS\" to which you can connect from your mobile device. On your mobile device go into Settings -> Wi-Fi Settings and the \"OVOS\" open network will appear in its list. Connect your device with the OVOS network and a webpage will open to configure the Wi-Fi network on your OpenVoiceOS device. (NOTE: On newer mobile operating systems the captive portal capture has changed and the website will not automatically be opened. If this is the case you can open a browser manually and go to http://172.16.127.1 ) The following webpage will be shown; Select your Wi-Fi network from the list, insert your password and press the \"Connect\" button. If everything went fine, you will soon see the green \"connected\" screen on your OpenVoiceOS device.","title":"On Mobile Setup"},{"location":"gs_first_boot/#on-device-setup","text":"Choosing this option allows you to set up your Wi-Fi network straight on your OpenVoiceOS device. If selected a screen with the available networks will be shown on your OpenVoiceOS device. Select your network from the list and tap / click on it to allow you to insert your password. If you have a touch screen an on-screen keyboard will appear when you tap the password field. If not use a keyboard. When you have inserted your password, click / tap the connect button and after a short connecting animation, if all went fine you will see the green \"connected\" screen on your OpenVoiceOS device.","title":"On Device Setup"},{"location":"gs_first_boot/#reconfigure-your-network-from-the-drop-down-menu","text":"If you have skipped the Wi-Fi network setup before, or you just want to reconfigure your network. On the homescreen of your OpenVoiceOS device swipe down the top menu and click the \"Wi-Fi\" icon. This brings you to the same on-device configuration screen. From here you can select another network or click the configuration icon on the right of connected network for details or to remove it from the configured networks.","title":"(Re)configure your network from the drop-down menu"},{"location":"gs_first_boot/#selecting-your-backend","text":"","title":"Selecting Your Backend"},{"location":"gs_first_boot/#what-is-a-backend","text":"","title":"What is a backend ?"},{"location":"gs_first_boot/#mycroft-ai-selene-backend","text":"The Pairing Process The GUI will now show you a Pairing Code, This pairing code needs to be entered on the mycroft backend which you can find online at https://account.mycroft.ai Create an account using your email id on https://account.mycroft.ai Head over to https://account.mycroft.ai/devices/add Enter the pairing code, a unique device name, and location settings Click next on the web interface, your device should now be paired","title":"Mycroft A.I. - Selene Backend"},{"location":"gs_first_boot/#no-backend-no-calling-home","text":"Select A Text To Speech (TTS) Engine: A text-to-speech (TTS) system converts normal language text into speech, select an engine from the list Select A Speech To Text (STT) Engine: A speech-to-text (STT) system converts human speech to text, select an engine from the list","title":"No backend - No calling home"},{"location":"gs_first_boot/#personal-backend-host-your-own","text":"Personal backend is a reverse engineered alternative to selene and requires the backend to be hosted locally. Install and Configure your personal backend, information available on: https://github.com/OpenVoiceOS/ovos-personal-backend The gui on device will display a setup page to enter the host address of your hosted backend on your device Pairing with your personal backend happens automatically once you hit the confirm button with the correct host address","title":"Personal backend - Host your own"},{"location":"gs_hardware/","text":"Welcome To OpenVoice OS Getting Started User Guide - Hardware Currently, OpenVoice OS is supported on: - Raspberry Pi 3,4 - X86_64 - Docker - Mycroft Mark 1 - Mycroft Mark 2 - Mycroft Mark 2 (dev kit) See Comparison for more information #todo","title":"Hardware"},{"location":"gs_hardware/#welcome-to-openvoice-os-getting-started-user-guide-hardware","text":"Currently, OpenVoice OS is supported on: - Raspberry Pi 3,4 - X86_64 - Docker - Mycroft Mark 1 - Mycroft Mark 2 - Mycroft Mark 2 (dev kit) See Comparison for more information #todo","title":"Welcome To OpenVoice OS Getting Started User Guide - Hardware"},{"location":"gs_installation_choices/","text":"Welcome To OpenVoice OS Getting Started User Guide Installation Choices You can find a pdf version of a getting started guide here Where to get OVOS? OVOS is in early stages, we publish our Raspberry Pi images for download but expect new bugs and new fixes on every release, we are not yet stable! These images are development images in alpha stage, bugs and incomplete features are guaranteed. Install choices You can install OVOS either as an image, container, or manually. There are currently three image choices for OVOS. Buildroot, Manjaro and Picroft. You can also build images from scratch using Buildroot and Manjaro A docker container is available for Manjaro. In most cases images provide for the easiest install, if your hardware is supported. Building your own image can provide for a complete custom build in a package, but is more of an involved process. If you're familiar with Docker, then that option can provide a quick install. Finally, building from scratch can be more involved, but provides the most flexibility.","title":"Installation Choices"},{"location":"gs_installation_choices/#welcome-to-openvoice-os-getting-started-user-guide","text":"","title":"Welcome To OpenVoice OS Getting Started User Guide"},{"location":"gs_installation_choices/#installation-choices","text":"You can find a pdf version of a getting started guide here","title":"Installation Choices"},{"location":"gs_installation_choices/#where-to-get-ovos","text":"OVOS is in early stages, we publish our Raspberry Pi images for download but expect new bugs and new fixes on every release, we are not yet stable! These images are development images in alpha stage, bugs and incomplete features are guaranteed.","title":"Where to get OVOS?"},{"location":"gs_installation_choices/#install-choices","text":"You can install OVOS either as an image, container, or manually. There are currently three image choices for OVOS. Buildroot, Manjaro and Picroft. You can also build images from scratch using Buildroot and Manjaro A docker container is available for Manjaro. In most cases images provide for the easiest install, if your hardware is supported. Building your own image can provide for a complete custom build in a package, but is more of an involved process. If you're familiar with Docker, then that option can provide a quick install. Finally, building from scratch can be more involved, but provides the most flexibility.","title":"Install choices"},{"location":"gs_installing_image/","text":"Welcome To OpenVoice OS Getting Started User Guide Installing an Image [Which Image should I choose?] #todo Downloading an Image Buildroot Latest Images - RPi3-64 - RPi4-64 Manjaro Latest Images - devkit Aug 2022 - Generic Mar 2023 - Mark 2 Feb 2023 - Respeaker Feb 2023 Picroft Latest Images - Debian Bullseye Mar 2023 - Raspbian Mar 2023 Build images from scratch: buildroot manjaro Flashing your image Flashing your image to your sdcard or USB drive is not different from flashing any other image. For the non-technical users we advise to use the flashing utility from the Raspberry Pi Foundation which you can find here . Under \"CHOOSE OS\" select custom at the very bottom of the list and browse to the downloaded image file. It is not required to unzip / unpack the image as the Raspberry Pi imager software can do that for you on the fly. Under \"CHOOSE STORAGE\" select your sdcard or USB device. If you have a Raspberry Pi 4 we recommend to use a good USB3.1 device. If you have a Raspberry Pi 3, use a proper sdcard. (From fast to slow: USB3.1 - sdcard - USB2)","title":"Installing an Image"},{"location":"gs_installing_image/#welcome-to-openvoice-os-getting-started-user-guide","text":"","title":"Welcome To OpenVoice OS Getting Started User Guide"},{"location":"gs_installing_image/#installing-an-image","text":"[Which Image should I choose?] #todo","title":"Installing an Image"},{"location":"gs_installing_image/#downloading-an-image","text":"Buildroot Latest Images - RPi3-64 - RPi4-64 Manjaro Latest Images - devkit Aug 2022 - Generic Mar 2023 - Mark 2 Feb 2023 - Respeaker Feb 2023 Picroft Latest Images - Debian Bullseye Mar 2023 - Raspbian Mar 2023","title":"Downloading an Image"},{"location":"gs_installing_image/#build-images-from-scratch","text":"buildroot manjaro","title":"Build images from scratch:"},{"location":"gs_installing_image/#flashing-your-image","text":"Flashing your image to your sdcard or USB drive is not different from flashing any other image. For the non-technical users we advise to use the flashing utility from the Raspberry Pi Foundation which you can find here . Under \"CHOOSE OS\" select custom at the very bottom of the list and browse to the downloaded image file. It is not required to unzip / unpack the image as the Raspberry Pi imager software can do that for you on the fly. Under \"CHOOSE STORAGE\" select your sdcard or USB device. If you have a Raspberry Pi 4 we recommend to use a good USB3.1 device. If you have a Raspberry Pi 3, use a proper sdcard. (From fast to slow: USB3.1 - sdcard - USB2)","title":"Flashing your image"},{"location":"gs_next_steps/","text":"OpenVoiceOS Quickstart - Next Steps Now that your OVOS device is up and running your next steps will be: - Installing Skills - Configuring the TTS engine - Changing the Wake Word - Install your own Services todo","title":"Next Steps"},{"location":"gs_next_steps/#openvoiceos-quickstart-next-steps","text":"Now that your OVOS device is up and running your next steps will be: - Installing Skills - Configuring the TTS engine - Changing the Wake Word - Install your own Services","title":"OpenVoiceOS Quickstart - Next Steps"},{"location":"gs_next_steps/#todo","text":"","title":"todo"},{"location":"hardware_audio/","text":"Audio Hardware Coming Soon Recommendations and notes on speakers and microphones","title":"Audio"},{"location":"hardware_audio/#audio-hardware","text":"Coming Soon Recommendations and notes on speakers and microphones","title":"Audio Hardware"},{"location":"hardware_platforms/","text":"Hardware Platforms coming soon","title":"Platforms"},{"location":"hardware_platforms/#hardware-platforms","text":"coming soon","title":"Hardware Platforms"},{"location":"howto_secure_ssh/","text":"How to Secure SSH Most of our guides have you create a user called ovos with a password of ovos, while this makes install easy, it's VERY insecure. As soon as possible, you should secure ssh using a key and disable password authentication. When connecting from a Linux or MacOS client Create a keyfile (you can change ovos to whatever you want) ssh-keygen -t ed25519 -f ~/.ssh/ovos Copy to host (use the same filename as above, specify the user and hostname you are using) ssh-copy-id -i ~/.ssh/ovos ovos@mycroft On your dekstop, edit ~/.ssh/config and add the following lines Host rp2 user ovos IdentityFile ~/.ssh/ovos On your ovos system, edit /etc/ssh/sshd_config and add or uncomment the following line: PasswordAuthentication no restart sshd or reboot sudo systemctl restart sshd","title":"Howto secure ssh"},{"location":"howto_secure_ssh/#how-to-secure-ssh","text":"Most of our guides have you create a user called ovos with a password of ovos, while this makes install easy, it's VERY insecure. As soon as possible, you should secure ssh using a key and disable password authentication.","title":"How to Secure SSH"},{"location":"howto_secure_ssh/#when-connecting-from-a-linux-or-macos-client","text":"Create a keyfile (you can change ovos to whatever you want) ssh-keygen -t ed25519 -f ~/.ssh/ovos Copy to host (use the same filename as above, specify the user and hostname you are using) ssh-copy-id -i ~/.ssh/ovos ovos@mycroft On your dekstop, edit ~/.ssh/config and add the following lines Host rp2 user ovos IdentityFile ~/.ssh/ovos On your ovos system, edit /etc/ssh/sshd_config and add or uncomment the following line: PasswordAuthentication no restart sshd or reboot sudo systemctl restart sshd","title":"When connecting from a Linux or MacOS client"},{"location":"images/","text":"OpenVoiceOS vs Neon A.I. vs Mycroft A.I. Ready to go images compared OpenVoiceOS ready to use images come in two flavours; The buildroot version, being the minimal consumer type of image and the Manjaro version, being the full distribution easy / easier for developing. OpenVoiceOS (Buildroot) OpenVoiceOS (Manjaro) Neon AI Mark II (Dinkum) Mycroft A.I. (PiCroft) Software - Architecture Core ovos-core ovos-core neon-core Dinkum mycroft-core GUI ovos-shell (mycroft-gui based) ovos-shell (mycroft-gui based) ovos-shell (mycroft-gui based) plasma-nano (mycroft-gui based) N/A Services systemd user session systemd system session systemd system session systemd system session N/A Hardware - Compatibility Raspberry Pi 3/3b/3b/4 4 4 Mark II (only) 3/3b/3b/4 X86_64 planned No WIP No No Virtual Appliance planned No Unknown No No Docker No possibly in future Yes Yes No No Mark-1 Yes WIP No No No No Mark-2 Yes Dev-Kit Retail (WIP) Yes Dev-Kit Retail Yes Dev-Kit Retail Yes Retail ONLY No Hardware - Peripherals ReSpeaker 2-mic 4-mic squared 4-mic linear 6-mic 2-mic 4-mic squared 4-mic linear 6-mic Unknown No Yes manual installation? USB Yes Yes Unknown No Yes manual installation SJ-201 Yes Yes Yes Yes No sandbox image maybe Google AIY v1 Yes manual configuration Yes manual installation Unknown No No manual installation? Google AIY v2 No perhaps in the future Yes manual installation Unknown No No manual installation? Screen - GUI GUI supported Showing a GUI if a screen is attached Yes ovos-shell on eglfs Yes ovos-shell on eglfs Yes ovos-shell on eglfs Yes plasma-nano on X11 No Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect to preset network from phone or pad. Yes No No Yes No On device WiFi Setup Configure the WiFi connection on the device itself Yes Yes Yes No No On screen keyboard Yes Yes Yes Yes No Reconfigure network Easy way to change the network settings Yes Yes Yes No No Configuration - Option Data privacy Yes Yes Yes Partial Partial Offline mode Yes Yes Yes No No Color theming Yes Yes Yes No No Non-Pairing mode Yes Yes Yes No No API Access w/o pairing Yes Yes Yes No No On-Device configuration Yes Yes Yes No No Online configuration Dashboard wip Dashboard wip WIP Yes Yes Customization Open Build System Yes Yes Yes Partial build tools are not public Yes Package manager No No buildtools available. Perhaps opkg in the future Yes (pacman) Yes Yes *limited becuase of read-only filesystem Yes Updating Update mechanism(s) pip In the future: Firmware updates. On-device and Over The Air pip package manager Plugin-based update mechanism OS Updates WIP OTA controlled by Mycroft pip package manager Voice Assistant - Functionality STT - On device Yes Kaldi/Vosk-API WhisperCPP (WIP) Whisper TFlite (WIP) Yes Kaldi/Vosk-API Yes Vosk Deepspeech Yes Vosk Coqui No STT - On premises Yes Ovos STT Server (any plugin) Yes Ovos STT Server (any plugin) Yes Ovos STT Server (any plugin) No No STT - Cloud Yes Ovos Server Proxy Google More...? Yes Ovos Server Proxy Google Yes Google Yes Selene Google Cloud Proxy Yes Selene Google (Chromium) Proxy TTS - On device Yes Mimic 1 More...? Yes Mimic 1 More...? Yes Mimic 1 Mimic 3 Coqui Yes Mimic 3 Yes Mimic 1 TTS - On premises Yes ? Yes ? Yes Coqui Mozilla Larynx No No TTS - Cloud Yes Google Mimic 2 Mimic 3 More...? Yes Google Mimic 2 Mimic 3 More...? Yes Amazon Polly No No Smart Speaker - Functionality Music player connectivity The use of external application on other devices to connect to your device. Yes Airplay Spotifyd Bluetooth Snapcast KDE Connect Unknown Unknown Yes MPD Local Files No manual installation? Music player sync Yes OCP MPRIS Yes OCP MPRIS Yes OCP MPRIS No No HomeAssistant integration unknown Yes HomeAssistant PHAL Plugin WIP Mycroft Skill reported working unknown unknown Camera support Yes wip Yes unknown unknown","title":"Images"},{"location":"images/#openvoiceos-vs-neon-ai-vs-mycroft-ai","text":"","title":"OpenVoiceOS vs Neon A.I. vs Mycroft A.I."},{"location":"images/#ready-to-go-images-compared","text":"OpenVoiceOS ready to use images come in two flavours; The buildroot version, being the minimal consumer type of image and the Manjaro version, being the full distribution easy / easier for developing. OpenVoiceOS (Buildroot) OpenVoiceOS (Manjaro) Neon AI Mark II (Dinkum) Mycroft A.I. (PiCroft) Software - Architecture Core ovos-core ovos-core neon-core Dinkum mycroft-core GUI ovos-shell (mycroft-gui based) ovos-shell (mycroft-gui based) ovos-shell (mycroft-gui based) plasma-nano (mycroft-gui based) N/A Services systemd user session systemd system session systemd system session systemd system session N/A Hardware - Compatibility Raspberry Pi 3/3b/3b/4 4 4 Mark II (only) 3/3b/3b/4 X86_64 planned No WIP No No Virtual Appliance planned No Unknown No No Docker No possibly in future Yes Yes No No Mark-1 Yes WIP No No No No Mark-2 Yes Dev-Kit Retail (WIP) Yes Dev-Kit Retail Yes Dev-Kit Retail Yes Retail ONLY No Hardware - Peripherals ReSpeaker 2-mic 4-mic squared 4-mic linear 6-mic 2-mic 4-mic squared 4-mic linear 6-mic Unknown No Yes manual installation? USB Yes Yes Unknown No Yes manual installation SJ-201 Yes Yes Yes Yes No sandbox image maybe Google AIY v1 Yes manual configuration Yes manual installation Unknown No No manual installation? Google AIY v2 No perhaps in the future Yes manual installation Unknown No No manual installation? Screen - GUI GUI supported Showing a GUI if a screen is attached Yes ovos-shell on eglfs Yes ovos-shell on eglfs Yes ovos-shell on eglfs Yes plasma-nano on X11 No Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect to preset network from phone or pad. Yes No No Yes No On device WiFi Setup Configure the WiFi connection on the device itself Yes Yes Yes No No On screen keyboard Yes Yes Yes Yes No Reconfigure network Easy way to change the network settings Yes Yes Yes No No Configuration - Option Data privacy Yes Yes Yes Partial Partial Offline mode Yes Yes Yes No No Color theming Yes Yes Yes No No Non-Pairing mode Yes Yes Yes No No API Access w/o pairing Yes Yes Yes No No On-Device configuration Yes Yes Yes No No Online configuration Dashboard wip Dashboard wip WIP Yes Yes Customization Open Build System Yes Yes Yes Partial build tools are not public Yes Package manager No No buildtools available. Perhaps opkg in the future Yes (pacman) Yes Yes *limited becuase of read-only filesystem Yes Updating Update mechanism(s) pip In the future: Firmware updates. On-device and Over The Air pip package manager Plugin-based update mechanism OS Updates WIP OTA controlled by Mycroft pip package manager Voice Assistant - Functionality STT - On device Yes Kaldi/Vosk-API WhisperCPP (WIP) Whisper TFlite (WIP) Yes Kaldi/Vosk-API Yes Vosk Deepspeech Yes Vosk Coqui No STT - On premises Yes Ovos STT Server (any plugin) Yes Ovos STT Server (any plugin) Yes Ovos STT Server (any plugin) No No STT - Cloud Yes Ovos Server Proxy Google More...? Yes Ovos Server Proxy Google Yes Google Yes Selene Google Cloud Proxy Yes Selene Google (Chromium) Proxy TTS - On device Yes Mimic 1 More...? Yes Mimic 1 More...? Yes Mimic 1 Mimic 3 Coqui Yes Mimic 3 Yes Mimic 1 TTS - On premises Yes ? Yes ? Yes Coqui Mozilla Larynx No No TTS - Cloud Yes Google Mimic 2 Mimic 3 More...? Yes Google Mimic 2 Mimic 3 More...? Yes Amazon Polly No No Smart Speaker - Functionality Music player connectivity The use of external application on other devices to connect to your device. Yes Airplay Spotifyd Bluetooth Snapcast KDE Connect Unknown Unknown Yes MPD Local Files No manual installation? Music player sync Yes OCP MPRIS Yes OCP MPRIS Yes OCP MPRIS No No HomeAssistant integration unknown Yes HomeAssistant PHAL Plugin WIP Mycroft Skill reported working unknown unknown Camera support Yes wip Yes unknown unknown","title":"Ready to go images compared"},{"location":"images_buildroot/","text":"Open Voice Operating System - Buildroot Edition Auto detection and configuration of HAT's The buildroot OpenVoiceOS editions is considered to be consumer friendly type of device, or as Mycroft A.I. would like to call, a retail version. However as we so not target a specific hardware platform and would like to support custom made systems we are implementing a smart way to detect and configure different type of Raspberry Pi HAT's. At boot the system scan the I2C bus for known and supported HAT's and if found configures the underlying linux sound system. At the moment this is still very much in development, however the below HAT's are or should soon be supported by this system; - ReSpeaker 2-mic HAT - ReSpeaker 4-mic Square HAT - ReSpeaker 4-mic linear / 6-mic HAT - USB devices such as the PS EYE-2 - SJ-201 Dev Kits - SJ-201 Mark2 retail device Snapcast Client & Server TODO - write docs Remote shared folder access (SMB - Windows) Your OpenVoiceOS device is accessible over the network from your Windows computer. This is still a work in process, but you can open a file explorer and navigate to you OpenVoiceOS device. At the moment the following directories within the user's home directory are shared over the network. - Documents - Music - Pictures These folders are also used by KDE Connect file transfer plugins and for instance the Camera skill (Hey Mycroft, take a selfie) and / or Homescreen Skill (Hey Mycroft, take a screenshot) Remote shared folder access (NFS - Linux) In the near future the above Windows network shares are also made available over NFS for Linux clients. This is still a Work In Progress / To Do item. Development. At this moment development is in very early stages and focussed on the Raspberry Pi 3B & 4. As soon as an initial first workable version is created, other hardware might be added. Source code: https://github.com/OpenVoiceOS/ovos-buildroot Build Environment Only use x86_64 based architecture/ hardware to build the image. The following example Build environment has been tested : Architecture: x86_64 Hardware: Intel Core i5 processor, 8GB RAM, 240GB SSD (you can build on less RAM (2GB) and slower storage but more RAM, faster storage = quicker image building) OS: Ubuntu 22.04 LTS desktop Installing System Build Dependencies The following system packages are required to build the image: gcc subversion qttools5-dev qttools5-dev-tools python git make g++ curl wget qtdeclarative5-dev The following firewall ports need to be allowed to the internet. In addition to the usual http/https ports (tcp 80, tcp 443) a couple of other ports need to be allowed to the internet : - tcp 9418 (git). - tcp 21 (ftp PASV) and random ports for DATA channel. This can be optional but better to have this allowed along with the corresponding random data channel ports. (knowledge of firewalls required) Getting the code. First, get the code on your system! The simplest method is via git. - cd ~/ - git clone --recurse-submodules https://github.com/OpenVoiceOS/OpenVoiceOS.git - cd OpenVoiceOS Patching Buildroot. (ONLY at the first clean checkout/clone) If this is the very first time you are going to build an image, you need to execute the following command once; - ./scripts/br-patches.sh This will patch the Buildroot packages. Building the image. Building the image(s) can be done by utilizing a proper Makefile; To see the available commands, just run: 'make help' As example to build the rpi4 version; - make clean - make rpi4_64-gui-config - make rpi4_64-gui Now grab a cup of coffee, go for a walk, sleep and repeat as the build process takes up a long time pulling everything from source and cross compiling everything for the device. Especially the qtwebengine package is taking a LONG time. (At the moment there is an outstanding issue which prevents the build to run completely to the end. The plasma-workspace package will error out, not finding the libGLESv4 properly linked within QT5GUI. When the build stopped because of this error, edit the following file; buildroot/output/host/aarch64-buildroot-linux-gnu/sysroot/usr/lib/cmake/Qt5Gui/Qt5GuiConfigExtras.cmake at the bottom of the file replace this line; _qt5gui_find_extra_libs(OPENGL \"GLESv2\" \"\" \"\") And replace it bit this line; _qt5gui_find_extra_libs(OPENGL \"${CMAKE_SYSROOT}/usr/lib/libGLESv2.so\" \"\" \"${CMAKE_SYSROOT}/usr/include/libdrm\") Then you can continue the build process by re-running the \"make rpi4_64-gui\" command. (DO NOT, run \"make clean\" and/or \"make rpi4_64-gui-config\" again, or you will start from scratch again !!!) When everything goes fine the xz compressed image will be available within the release directory. Booting image from sd card for the first time (setting up Wi-Fi and backend). 1.Ensure all required peripherals (mic, speakers, HDMI, usb mouse etc) are plugged in before powering on your RPI4 for the first time. 2. Skip this step if RPI4 is using an ethernet cable. Once powered on, the screen will present the Wifi setup screen ( a Wifi HotSpot is created). Connect to the Wifi HotSpot (ssid OVOS) from another device and follow the on-screen instructions to set up Wifi. 3.Once Wifi is setup a choice of Mycroft backend and Local backend is presented. Choose the Mycroft backend for now and follow the on-screen instructions, Local backend is not ready to use yet. After the pairing process has completed and skills downloaded it's time to test/ use it. Accessing the CLI. SSH to ip address of RPI4 default credentials 'mycroft/mycroft'","title":"Buildroot"},{"location":"images_buildroot/#open-voice-operating-system-buildroot-edition","text":"","title":" Open Voice Operating System - Buildroot Edition"},{"location":"images_buildroot/#auto-detection-and-configuration-of-hats","text":"The buildroot OpenVoiceOS editions is considered to be consumer friendly type of device, or as Mycroft A.I. would like to call, a retail version. However as we so not target a specific hardware platform and would like to support custom made systems we are implementing a smart way to detect and configure different type of Raspberry Pi HAT's. At boot the system scan the I2C bus for known and supported HAT's and if found configures the underlying linux sound system. At the moment this is still very much in development, however the below HAT's are or should soon be supported by this system; - ReSpeaker 2-mic HAT - ReSpeaker 4-mic Square HAT - ReSpeaker 4-mic linear / 6-mic HAT - USB devices such as the PS EYE-2 - SJ-201 Dev Kits - SJ-201 Mark2 retail device","title":"Auto detection and configuration of HAT's"},{"location":"images_buildroot/#snapcast-client-server","text":"TODO - write docs","title":"Snapcast Client &amp; Server"},{"location":"images_buildroot/#remote-shared-folder-access-smb-windows","text":"Your OpenVoiceOS device is accessible over the network from your Windows computer. This is still a work in process, but you can open a file explorer and navigate to you OpenVoiceOS device. At the moment the following directories within the user's home directory are shared over the network. - Documents - Music - Pictures These folders are also used by KDE Connect file transfer plugins and for instance the Camera skill (Hey Mycroft, take a selfie) and / or Homescreen Skill (Hey Mycroft, take a screenshot)","title":"Remote shared folder access (SMB - Windows)"},{"location":"images_buildroot/#remote-shared-folder-access-nfs-linux","text":"In the near future the above Windows network shares are also made available over NFS for Linux clients. This is still a Work In Progress / To Do item.","title":"Remote shared folder access (NFS - Linux)"},{"location":"images_buildroot/#development","text":"At this moment development is in very early stages and focussed on the Raspberry Pi 3B & 4. As soon as an initial first workable version is created, other hardware might be added. Source code: https://github.com/OpenVoiceOS/ovos-buildroot","title":"Development."},{"location":"images_buildroot/#build-environment","text":"Only use x86_64 based architecture/ hardware to build the image. The following example Build environment has been tested : Architecture: x86_64 Hardware: Intel Core i5 processor, 8GB RAM, 240GB SSD (you can build on less RAM (2GB) and slower storage but more RAM, faster storage = quicker image building) OS: Ubuntu 22.04 LTS desktop","title":"Build Environment"},{"location":"images_buildroot/#installing-system-build-dependencies","text":"The following system packages are required to build the image: gcc subversion qttools5-dev qttools5-dev-tools python git make g++ curl wget qtdeclarative5-dev","title":"Installing System Build Dependencies"},{"location":"images_buildroot/#the-following-firewall-ports-need-to-be-allowed-to-the-internet","text":"In addition to the usual http/https ports (tcp 80, tcp 443) a couple of other ports need to be allowed to the internet : - tcp 9418 (git). - tcp 21 (ftp PASV) and random ports for DATA channel. This can be optional but better to have this allowed along with the corresponding random data channel ports. (knowledge of firewalls required)","title":"The following firewall ports need to be allowed to the internet."},{"location":"images_buildroot/#getting-the-code","text":"First, get the code on your system! The simplest method is via git. - cd ~/ - git clone --recurse-submodules https://github.com/OpenVoiceOS/OpenVoiceOS.git - cd OpenVoiceOS","title":"Getting the code."},{"location":"images_buildroot/#patching-buildroot","text":"(ONLY at the first clean checkout/clone) If this is the very first time you are going to build an image, you need to execute the following command once; - ./scripts/br-patches.sh This will patch the Buildroot packages.","title":"Patching Buildroot."},{"location":"images_buildroot/#building-the-image","text":"Building the image(s) can be done by utilizing a proper Makefile; To see the available commands, just run: 'make help' As example to build the rpi4 version; - make clean - make rpi4_64-gui-config - make rpi4_64-gui Now grab a cup of coffee, go for a walk, sleep and repeat as the build process takes up a long time pulling everything from source and cross compiling everything for the device. Especially the qtwebengine package is taking a LONG time. (At the moment there is an outstanding issue which prevents the build to run completely to the end. The plasma-workspace package will error out, not finding the libGLESv4 properly linked within QT5GUI. When the build stopped because of this error, edit the following file; buildroot/output/host/aarch64-buildroot-linux-gnu/sysroot/usr/lib/cmake/Qt5Gui/Qt5GuiConfigExtras.cmake at the bottom of the file replace this line; _qt5gui_find_extra_libs(OPENGL \"GLESv2\" \"\" \"\") And replace it bit this line; _qt5gui_find_extra_libs(OPENGL \"${CMAKE_SYSROOT}/usr/lib/libGLESv2.so\" \"\" \"${CMAKE_SYSROOT}/usr/include/libdrm\") Then you can continue the build process by re-running the \"make rpi4_64-gui\" command. (DO NOT, run \"make clean\" and/or \"make rpi4_64-gui-config\" again, or you will start from scratch again !!!) When everything goes fine the xz compressed image will be available within the release directory.","title":"Building the image."},{"location":"images_buildroot/#booting-image-from-sd-card-for-the-first-time-setting-up-wi-fi-and-backend","text":"1.Ensure all required peripherals (mic, speakers, HDMI, usb mouse etc) are plugged in before powering on your RPI4 for the first time. 2. Skip this step if RPI4 is using an ethernet cable. Once powered on, the screen will present the Wifi setup screen ( a Wifi HotSpot is created). Connect to the Wifi HotSpot (ssid OVOS) from another device and follow the on-screen instructions to set up Wifi. 3.Once Wifi is setup a choice of Mycroft backend and Local backend is presented. Choose the Mycroft backend for now and follow the on-screen instructions, Local backend is not ready to use yet. After the pairing process has completed and skills downloaded it's time to test/ use it.","title":"Booting image from sd card for the first time (setting up Wi-Fi and backend)."},{"location":"images_buildroot/#accessing-the-cli","text":"SSH to ip address of RPI4 default credentials 'mycroft/mycroft'","title":"Accessing the CLI."},{"location":"images_intro/","text":"OpenVoiceOS Image Options Coming soon","title":"Introduction"},{"location":"images_intro/#openvoiceos-image-options","text":"Coming soon","title":"OpenVoiceOS Image Options"},{"location":"images_manjaro/","text":"ovos-image-arch-recipe Make a manjaro based OpenVoiceOS image source code: https://github.com/OpenVoiceOS/ovos-image-arch-recipe/ Building Docker Automated Image Building The included Dockerfile can be used to build a default image in a Docker environment. The following dependencies must be installed on the build system before running the container: chroot qemu-user-static First, create the Docker container: docker build . -t ovos-image-builder Then, run the container to create a OVOS Image. Set CORE_REF to the branch of ovos-core that you want to build and RECIPE_REF to the branch of ovos-image-recipe you want to use. Set MAKE_THREADS to the number of threads to use for make processes. docker run \\ -v /home/${USER}/output:/output:rw \\ -v /run/systemd/resolve:/run/systemd/resolve \\ -e CORE_REF=${CORE_REF:-dev} \\ -e RECIPE_REF=${RECIPE_REF:-master} \\ -e MAKE_THREADS=${MAKE_THREADS:-4} \\ --privileged \\ --network=host \\ --name=ovos-image-builder \\ ovos-image-builder The entire build process will generally take several hours; it takes 1-2 hours on a build server with 2x Xeon Gold 5118 CPUs (48T Total). Interactive Image Building The scripts in the automation directory are available to help automate building a default image. For building an image interactively: bash automation/prepare.sh bash /tmp/run_scripts.sh The below documentation describes how to manually build an image using the individual scripts in this repository. Steps Getting Started The scripts and overlay files in this repository are designed to be applied to a base image as the root user. It is recommended to apply these scripts to a clean base image. Instructions are available at opensource.com . Note : The GUI shell is not installable under some base images For each step except boot_overlay , the directory corresponding to the step should be copied to the mounted image and the script run from a terminal chroot -ed to the image. If running scripts from a booted image, they should be run as root . Preparation From the host system where this repository is cloned, running prepare.sh <base_image> will copy boot overlay files, mount the image, mount DNS resolver config from the host system, copy all other image overlay files to /tmp , and chroot into the image. From here, you can run any/all of the following scripts to prepare the image before cleaning up core_configuration Configures user accounts and base functionality for RPi. ovos user is created with proper permissions here. At this stage, a booted image should resize its file system to fill the drive it is flashed to. Local login and ssh connections should use ovos / ovos to authenticate and be prompted to change password on login. network_manager Adds Balena wifi-connect to enable a portal for connecting the Pi device to a Wi-Fi network. A booted image will now be ready to connect to a network via SSID OVOS . sj201 For SJ201 board support, the included script will build/install drivers, add required overlays, install required system packages, and add a systemd service to flash the SJ201 chip on boot. This will modify pulseaudio and potentially overwrite any previous settings. Note: Running this scripts grants GPIO permissions to the gpio group. Any user that interfaces with the SJ201 board should be a member of the gpio group. Group permissions are not modified by this script Audio devices should now show up with pactl list . Audio devices can be tested in the image by recording a short audio clip and playing it back. parecord test.wav paplay test.wav embedded_shell Installs ovos-shell and mycroft-gui-app. Adds and enables ovos-gui.service to start the shell on system boot. The image should now boot to the GUI shell. ovos_core Installs ovos-core and dependencies. Configures services for core modules. At this stage, the image is complete and when booted should start OVOS. dashboard Installs the OVOS Dashboard and service to start the dashboard from the GUI. From the GUI Settings -> Developer Settings menu, Enable Dashboard will now start the dashboard for remote access to device diagnostics. camera Installs libcamera and other dependencies for using a CSI camera. The default camera skill can be used to take a photo; libcamera-apps are also installed for testing via CLI. splash_screen Enables a custom splash screen and disables on-device TTY at boot. On boot, a static image should be shown until the GUI Shell starts. Clean Up cleanup.sh removes any temporary files from the mounted image before unmounting it. After running cleanup.sh , the image is ready to burn to a drive and boot.","title":"Manjaro"},{"location":"images_manjaro/#ovos-image-arch-recipe","text":"Make a manjaro based OpenVoiceOS image source code: https://github.com/OpenVoiceOS/ovos-image-arch-recipe/","title":"ovos-image-arch-recipe"},{"location":"images_manjaro/#building","text":"","title":"Building"},{"location":"images_manjaro/#docker-automated-image-building","text":"The included Dockerfile can be used to build a default image in a Docker environment. The following dependencies must be installed on the build system before running the container: chroot qemu-user-static First, create the Docker container: docker build . -t ovos-image-builder Then, run the container to create a OVOS Image. Set CORE_REF to the branch of ovos-core that you want to build and RECIPE_REF to the branch of ovos-image-recipe you want to use. Set MAKE_THREADS to the number of threads to use for make processes. docker run \\ -v /home/${USER}/output:/output:rw \\ -v /run/systemd/resolve:/run/systemd/resolve \\ -e CORE_REF=${CORE_REF:-dev} \\ -e RECIPE_REF=${RECIPE_REF:-master} \\ -e MAKE_THREADS=${MAKE_THREADS:-4} \\ --privileged \\ --network=host \\ --name=ovos-image-builder \\ ovos-image-builder The entire build process will generally take several hours; it takes 1-2 hours on a build server with 2x Xeon Gold 5118 CPUs (48T Total).","title":"Docker Automated Image Building"},{"location":"images_manjaro/#interactive-image-building","text":"The scripts in the automation directory are available to help automate building a default image. For building an image interactively: bash automation/prepare.sh bash /tmp/run_scripts.sh The below documentation describes how to manually build an image using the individual scripts in this repository.","title":"Interactive Image Building"},{"location":"images_manjaro/#steps","text":"","title":"Steps"},{"location":"images_manjaro/#getting-started","text":"The scripts and overlay files in this repository are designed to be applied to a base image as the root user. It is recommended to apply these scripts to a clean base image. Instructions are available at opensource.com . Note : The GUI shell is not installable under some base images For each step except boot_overlay , the directory corresponding to the step should be copied to the mounted image and the script run from a terminal chroot -ed to the image. If running scripts from a booted image, they should be run as root .","title":"Getting Started"},{"location":"images_manjaro/#preparation","text":"From the host system where this repository is cloned, running prepare.sh <base_image> will copy boot overlay files, mount the image, mount DNS resolver config from the host system, copy all other image overlay files to /tmp , and chroot into the image. From here, you can run any/all of the following scripts to prepare the image before cleaning up","title":"Preparation"},{"location":"images_manjaro/#core_configuration","text":"Configures user accounts and base functionality for RPi. ovos user is created with proper permissions here. At this stage, a booted image should resize its file system to fill the drive it is flashed to. Local login and ssh connections should use ovos / ovos to authenticate and be prompted to change password on login.","title":"core_configuration"},{"location":"images_manjaro/#network_manager","text":"Adds Balena wifi-connect to enable a portal for connecting the Pi device to a Wi-Fi network. A booted image will now be ready to connect to a network via SSID OVOS .","title":"network_manager"},{"location":"images_manjaro/#sj201","text":"For SJ201 board support, the included script will build/install drivers, add required overlays, install required system packages, and add a systemd service to flash the SJ201 chip on boot. This will modify pulseaudio and potentially overwrite any previous settings. Note: Running this scripts grants GPIO permissions to the gpio group. Any user that interfaces with the SJ201 board should be a member of the gpio group. Group permissions are not modified by this script Audio devices should now show up with pactl list . Audio devices can be tested in the image by recording a short audio clip and playing it back. parecord test.wav paplay test.wav","title":"sj201"},{"location":"images_manjaro/#embedded_shell","text":"Installs ovos-shell and mycroft-gui-app. Adds and enables ovos-gui.service to start the shell on system boot. The image should now boot to the GUI shell.","title":"embedded_shell"},{"location":"images_manjaro/#ovos_core","text":"Installs ovos-core and dependencies. Configures services for core modules. At this stage, the image is complete and when booted should start OVOS.","title":"ovos_core"},{"location":"images_manjaro/#dashboard","text":"Installs the OVOS Dashboard and service to start the dashboard from the GUI. From the GUI Settings -> Developer Settings menu, Enable Dashboard will now start the dashboard for remote access to device diagnostics.","title":"dashboard"},{"location":"images_manjaro/#camera","text":"Installs libcamera and other dependencies for using a CSI camera. The default camera skill can be used to take a photo; libcamera-apps are also installed for testing via CLI.","title":"camera"},{"location":"images_manjaro/#splash_screen","text":"Enables a custom splash screen and disables on-device TTY at boot. On boot, a static image should be shown until the GUI Shell starts.","title":"splash_screen"},{"location":"images_manjaro/#clean-up","text":"cleanup.sh removes any temporary files from the mounted image before unmounting it. After running cleanup.sh , the image is ready to burn to a drive and boot.","title":"Clean Up"},{"location":"install_image/","text":"OpenVoiceOS Architetcture Work in Progress","title":"OpenVoiceOS Architetcture"},{"location":"install_image/#openvoiceos-architetcture","text":"Work in Progress","title":"OpenVoiceOS Architetcture"},{"location":"install_ovos_core/","text":"OpenVoiceOS Core (ovos-core) OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OVOS-core is a backwards-compatible descendant of Mycroft-core , the central component of Mycroft. It contains extensions and features not present upstream. All Mycroft Skills and Plugins should work normally with OVOS-core. OVOS-core is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library! Getting Started ovos-core is very modular, depending on where you are running ovos-core you may want to run only a subset of the services by default ovos-core only installs the minimum components common to all services, for the purposes of this document we will assume you want a full install if you want to finetune the components please replace [all] in commands below with the subset of desired extras, eg [skills,bus] Installing ovos-core ovos-core can be installed from pypi or from source if install fails you may need to install some system dependencies, how to do this will depend on your distro sudo apt install build-essential python3-dev swig libssl-dev libfann-dev portaudio19-dev libpulse-dev Note : MycroftAI's dev_setup.sh does not exist in OVOS-core. from source We suggest you do this in a virtualenv: pip install git+https://github.com/OpenVoiceOS/ovos-core[all] from pypi pip install ovos-core[all] Running ovos-core Developer launcher script start-mycroft.sh is available to perform common tasks. Assuming you installed ovos-core in your home directory, run: cd ~/ovos-core ./start-mycroft.sh debug The \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface. Later you can bring up the CLI using ./start-mycroft.sh cli . The background services can be stopped as a group with: ./stop-mycroft.sh Automatically on boot We recommend you create system services to manage ovos instead of depending on the launcher script above A good explanation can be found here https://github.com/j1nx/mycroft-systemd A reference implementation can be found in ovos-buildroot","title":"Install OVOS Core"},{"location":"install_ovos_core/#openvoiceos-core-ovos-core","text":"OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OVOS-core is a backwards-compatible descendant of Mycroft-core , the central component of Mycroft. It contains extensions and features not present upstream. All Mycroft Skills and Plugins should work normally with OVOS-core. OVOS-core is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library!","title":"OpenVoiceOS Core (ovos-core)"},{"location":"install_ovos_core/#getting-started","text":"ovos-core is very modular, depending on where you are running ovos-core you may want to run only a subset of the services by default ovos-core only installs the minimum components common to all services, for the purposes of this document we will assume you want a full install if you want to finetune the components please replace [all] in commands below with the subset of desired extras, eg [skills,bus]","title":"Getting Started"},{"location":"install_ovos_core/#installing-ovos-core","text":"ovos-core can be installed from pypi or from source if install fails you may need to install some system dependencies, how to do this will depend on your distro sudo apt install build-essential python3-dev swig libssl-dev libfann-dev portaudio19-dev libpulse-dev Note : MycroftAI's dev_setup.sh does not exist in OVOS-core.","title":"Installing ovos-core"},{"location":"install_ovos_core/#from-source","text":"We suggest you do this in a virtualenv: pip install git+https://github.com/OpenVoiceOS/ovos-core[all]","title":"from source"},{"location":"install_ovos_core/#from-pypi","text":"pip install ovos-core[all]","title":"from pypi"},{"location":"install_ovos_core/#running-ovos-core","text":"","title":"Running ovos-core"},{"location":"install_ovos_core/#developer-launcher-script","text":"start-mycroft.sh is available to perform common tasks. Assuming you installed ovos-core in your home directory, run: cd ~/ovos-core ./start-mycroft.sh debug The \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface. Later you can bring up the CLI using ./start-mycroft.sh cli . The background services can be stopped as a group with: ./stop-mycroft.sh","title":"Developer launcher script"},{"location":"install_ovos_core/#automatically-on-boot","text":"We recommend you create system services to manage ovos instead of depending on the launcher script above A good explanation can be found here https://github.com/j1nx/mycroft-systemd A reference implementation can be found in ovos-buildroot","title":"Automatically on boot"},{"location":"install_raspbian/","text":"Raspbian OpenVoice OS coming soon","title":"Raspbian"},{"location":"install_raspbian/#raspbian-openvoice-os","text":"coming soon","title":"Raspbian OpenVoice OS"},{"location":"install_skills/","text":"Installing New Skills There are a few ways to install skills in ovos. The official way is to use osm OVOS skills manager Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi Supported stores OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills Install pip install ovos-skills-manager Usage osm provides a few command line utilities, explained below Install Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit. Enable Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit. Disable Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit. Sync Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit. Priority Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit. Print config print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit. Search Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit. Manual Install Finding Skills Most skills are found throughout github. The official skills can be found at the OpenVoiceOS github page. Search the repositories for \"skills\". There are a few other places they can be found. Neon AI has several skills, and a search through github will for sure find more. Installing a found skill pip install The preferred method is with pip . If a skill has a setup.py file, it can be installed this way. The syntax is pip install git+<github/repository.git> . ex. pip install git+https://github.com/OpenVoiceOS/skill-ovos-date-time.git should install the ovos-date-time skill After installing skills this way, ovos skills service needs to be restarted buildroot / ovos-picroft systemctl --user restart mycroft-skills manjaro / other system installed services systemctl restart mycroft-skills git install Skills can also be directly cloned to the skill directory, usually located at ~/.local/share/mycroft/skills/ enter the skill directory cd ~/.local/share/mycroft/skills and clone the found skill here with git git clone <github/repository.git> ex. git clone https://github.com/OpenVoiceOS/skill-ovos-date-time.git will install the ovos-date-time skill. A restart of the ovos-skills service is not required when installing this way. Skill Libraries todo","title":"Installing Skills"},{"location":"install_skills/#installing-new-skills","text":"There are a few ways to install skills in ovos. The official way is to use osm","title":"Installing New Skills"},{"location":"install_skills/#ovos-skills-manager","text":"Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi","title":"OVOS skills manager"},{"location":"install_skills/#supported-stores","text":"OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills","title":"Supported stores"},{"location":"install_skills/#install","text":"pip install ovos-skills-manager","title":"Install"},{"location":"install_skills/#usage","text":"osm provides a few command line utilities, explained below","title":"Usage"},{"location":"install_skills/#install_1","text":"Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit.","title":"Install"},{"location":"install_skills/#enable","text":"Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit.","title":"Enable"},{"location":"install_skills/#disable","text":"Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit.","title":"Disable"},{"location":"install_skills/#sync","text":"Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit.","title":"Sync"},{"location":"install_skills/#priority","text":"Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit.","title":"Priority"},{"location":"install_skills/#print-config","text":"print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit.","title":"Print config"},{"location":"install_skills/#search","text":"Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit.","title":"Search"},{"location":"install_skills/#manual-install","text":"","title":"Manual Install"},{"location":"install_skills/#finding-skills","text":"Most skills are found throughout github. The official skills can be found at the OpenVoiceOS github page. Search the repositories for \"skills\". There are a few other places they can be found. Neon AI has several skills, and a search through github will for sure find more.","title":"Finding Skills"},{"location":"install_skills/#installing-a-found-skill","text":"","title":"Installing a found skill"},{"location":"install_skills/#pip-install","text":"The preferred method is with pip . If a skill has a setup.py file, it can be installed this way. The syntax is pip install git+<github/repository.git> . ex. pip install git+https://github.com/OpenVoiceOS/skill-ovos-date-time.git should install the ovos-date-time skill After installing skills this way, ovos skills service needs to be restarted buildroot / ovos-picroft systemctl --user restart mycroft-skills manjaro / other system installed services systemctl restart mycroft-skills","title":"pip install"},{"location":"install_skills/#git-install","text":"Skills can also be directly cloned to the skill directory, usually located at ~/.local/share/mycroft/skills/ enter the skill directory cd ~/.local/share/mycroft/skills and clone the found skill here with git git clone <github/repository.git> ex. git clone https://github.com/OpenVoiceOS/skill-ovos-date-time.git will install the ovos-date-time skill. A restart of the ovos-skills service is not required when installing this way.","title":"git install"},{"location":"install_skills/#skill-libraries","text":"","title":"Skill Libraries"},{"location":"install_skills/#todo","text":"","title":"todo"},{"location":"intents/","text":"Intents A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. Keyword / Rule based these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here","title":"Intents"},{"location":"intents/#intents","text":"A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. Keyword / Rule based these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here","title":"Intents"},{"location":"kdeconnect/","text":"KDE Connect KDE Connect is a multi-platform application developed by KDE, which facilitates wireless communications and data transfer between devices over local networks and is installed and configured by default on the Buildroot based image. A couple of features of KDE Connect are: Shared clipboard: copy and paste between your phone, computer and/or OpenVoiceOS device. Share files and URLs instantly from one device to another including your OpenVoiceOS device. Multimedia remote control: Use your phone, tablet or computer as a remote for what is playing on your OpenVoiceOS device. Auto mute your OpenVoiceOS device when your mobile phone rings. Virtual touchpad / keyboard: Use your phone/tablet screen as your OpenVoiceOS device its mouse and keyboard. For the sake of simplicity the below screenshots are made using the iPhone KDE Connect client, however as it is not yet fully feature complete and / or stable, it is recommended to use the Android and / or Linux client. Especially if you would like to have full MPRIS control of your OpenVoiceOS device. On your mobile device, open the KDE Connect app and it will see the advertised OpenVoiceOS KDE Connect device automatically. { width=50% } Click / Tap on the \"OpenVoiceOS-*\" to start the pairing process. By clicking / tapping the pair button a similar pop-up will appear on the screen of the OpenVoiceOS device. Also click / tap on the pair button finalises the pairing proces allowing your Mobile device to automatically connect with your OpenVoiceOS device and make use of all the extra functionality of what KDE Connect brings.","title":"KDE Connect"},{"location":"kdeconnect/#kde-connect","text":"KDE Connect is a multi-platform application developed by KDE, which facilitates wireless communications and data transfer between devices over local networks and is installed and configured by default on the Buildroot based image. A couple of features of KDE Connect are: Shared clipboard: copy and paste between your phone, computer and/or OpenVoiceOS device. Share files and URLs instantly from one device to another including your OpenVoiceOS device. Multimedia remote control: Use your phone, tablet or computer as a remote for what is playing on your OpenVoiceOS device. Auto mute your OpenVoiceOS device when your mobile phone rings. Virtual touchpad / keyboard: Use your phone/tablet screen as your OpenVoiceOS device its mouse and keyboard. For the sake of simplicity the below screenshots are made using the iPhone KDE Connect client, however as it is not yet fully feature complete and / or stable, it is recommended to use the Android and / or Linux client. Especially if you would like to have full MPRIS control of your OpenVoiceOS device. On your mobile device, open the KDE Connect app and it will see the advertised OpenVoiceOS KDE Connect device automatically. { width=50% } Click / Tap on the \"OpenVoiceOS-*\" to start the pairing process. By clicking / tapping the pair button a similar pop-up will appear on the screen of the OpenVoiceOS device. Also click / tap on the pair button finalises the pairing proces allowing your Mobile device to automatically connect with your OpenVoiceOS device and make use of all the extra functionality of what KDE Connect brings.","title":"KDE Connect"},{"location":"lang_plugins/","text":"Language Detection/Translation Plugins These plugins can be used to detect the language of text and to translate it They are not used internally by ovos-core but are integrated with external tools neon-core also makes heavy use of OPM language plugins List of Language plugins Plugin Detect Tx Offline Type neon-lang-plugin-cld2 yes no yes FOSS neon-lang-plugin-cld3 yes no yes FOSS neon-lang-plugin-langdetect yes no yes FOSS neon-lang-plugin-fastlang yes no yes FOSS neon-lang-plugin-lingua_podre yes no yes FOSS neon-lang-plugin-libretranslate yes yes no API (self hosted) neon-lang-plugin-apertium no yes no API (self hosted) neon-lang-plugin-amazon_translate yes yes no API (key) neon-lang-plugin-google_translate yes yes no API (key) Open Linguistika Open Linguistika is a tool to allow Mycroft Skill developers working on GUI\u2019s to easily translate their GUI\u2019s to other languages. For Mycroft\u2019s GUI, the UI interface used currently by Mycroft for which you can find QML files under the UI directory of skills, is based on Qt. Mycroft GUI uses Qt\u2019s translation mechanism to translate GUI\u2019s to other languages. To get your skills GUI translated and ready for other languages involves several manual steps from running Qt tools like lupdate against each QML UI file for each translatable language to running Qt\u2019s tool lrelease for specific language targets to compile a language for the QT environment to understand. To make your developer experience smarter and easier the OpenVoiceOS team is introducing an all-in-one toolkit for GUI language translations. The Open Linguistika toolkit allows developers to use auto-translate from various supported translator providers, and additionally support more languages, with the possibility for manual translations without having to go through the different Qt tools and command chain required to manually support a skill GUI for a different language. As a GUI skill Developer, the only know-how you need is to add the translation calls to your skill QML files, Developers can get more information about how to add them here Internationalization and Localization with Qt Quick | Qt 6.3. The \u201cTLDR\u201d version is that for every hard-coded string in your QML UI skill file you need to decorate your strings with the qsTr() decorator and your model list elements with the QT_TR_NOOP() decorator. Open Linguistika when installed by default on your distribution by choice, currently supports 6 European languages and 2 auto-translation providers. The tool provides extensibility through its JSON configuration interface to add more language support, where using a simple JSON language addition mechanism you can extend the tool to support a number of additional languages you would like to support for your skills UI. You can read more about adding additional languages on the tool\u2019s GitHub repository. How-To-Use Demo:","title":"Language Detection/Translation Plugins"},{"location":"lang_plugins/#language-detectiontranslation-plugins","text":"These plugins can be used to detect the language of text and to translate it They are not used internally by ovos-core but are integrated with external tools neon-core also makes heavy use of OPM language plugins","title":"Language Detection/Translation Plugins"},{"location":"lang_plugins/#list-of-language-plugins","text":"Plugin Detect Tx Offline Type neon-lang-plugin-cld2 yes no yes FOSS neon-lang-plugin-cld3 yes no yes FOSS neon-lang-plugin-langdetect yes no yes FOSS neon-lang-plugin-fastlang yes no yes FOSS neon-lang-plugin-lingua_podre yes no yes FOSS neon-lang-plugin-libretranslate yes yes no API (self hosted) neon-lang-plugin-apertium no yes no API (self hosted) neon-lang-plugin-amazon_translate yes yes no API (key) neon-lang-plugin-google_translate yes yes no API (key)","title":"List of Language plugins"},{"location":"lang_plugins/#open-linguistika","text":"Open Linguistika is a tool to allow Mycroft Skill developers working on GUI\u2019s to easily translate their GUI\u2019s to other languages. For Mycroft\u2019s GUI, the UI interface used currently by Mycroft for which you can find QML files under the UI directory of skills, is based on Qt. Mycroft GUI uses Qt\u2019s translation mechanism to translate GUI\u2019s to other languages. To get your skills GUI translated and ready for other languages involves several manual steps from running Qt tools like lupdate against each QML UI file for each translatable language to running Qt\u2019s tool lrelease for specific language targets to compile a language for the QT environment to understand. To make your developer experience smarter and easier the OpenVoiceOS team is introducing an all-in-one toolkit for GUI language translations. The Open Linguistika toolkit allows developers to use auto-translate from various supported translator providers, and additionally support more languages, with the possibility for manual translations without having to go through the different Qt tools and command chain required to manually support a skill GUI for a different language. As a GUI skill Developer, the only know-how you need is to add the translation calls to your skill QML files, Developers can get more information about how to add them here Internationalization and Localization with Qt Quick | Qt 6.3. The \u201cTLDR\u201d version is that for every hard-coded string in your QML UI skill file you need to decorate your strings with the qsTr() decorator and your model list elements with the QT_TR_NOOP() decorator. Open Linguistika when installed by default on your distribution by choice, currently supports 6 European languages and 2 auto-translation providers. The tool provides extensibility through its JSON configuration interface to add more language support, where using a simple JSON language addition mechanism you can extend the tool to support a number of additional languages you would like to support for your skills UI. You can read more about adding additional languages on the tool\u2019s GitHub repository. How-To-Use Demo:","title":"Open Linguistika"},{"location":"license/","text":"License We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects. Notable licensing exceptions The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"License"},{"location":"license/#license","text":"We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects.","title":"License"},{"location":"license/#notable-licensing-exceptions","text":"The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"Notable licensing exceptions"},{"location":"mycroft/","text":"ovos-core vs mycroft-core Speech Client Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module Audio Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify Skills Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching Hardware Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core Misc Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat Dinkum What is Dinkum Mycroft Mark2 shipped with a new version of mycroft called \"dinkum\", this is a total overhaul of mycroft-core and incompatible mycroft-core is now referred to as \"Classic Core\" by MycroftAI MycroftAI now provides what they call sandbox images, to add to the confusion those only work in the mark 2 and \"Classic Core\" means the mark-ii/latest branch of mycroft-core, this is a derivative version of the branch that was used in the dev kits ( mark-ii/qa ) and is also backwards incompatible, changes in this branch were not done via PRs and had no review or community input Mark2 useful links: mark 2 docs dinkum source code sandbox images mark2/latest (core for sandbox images) mark2/qa (core for dev kit image) Dinkum vs ovos-core you can find mycroft's guide to porting skills to dinkum here https://mycroft-ai.gitbook.io/mark-ii/differences-to-classic-core/porting-classic-core-skills mark2/qa brought some changes to mycroft-core, not all of them backwards compatible and some that were contentious within the community. VAD - VAD has been added to the mark-ii, but it is hardcoded to silero, this feature has been adopted via OPM, it is an important part of ovos-core listening modes introduced in version 0.0.5 self.resources - resource file loading was overhauled, this feature has been improved ( ovos-core/pull/130 + ovos-core/pull/131 + ovos-core/pull/135 + ovos-core/pull/170 ) and ported to OVOS and is also available in OVOSkill class ( OVOS-workshop/pull/30 ) for usage in classic core audio hal - audio playback was rewritten from scratch, audio plugin support has been removed, OVOS will not adopt this new approach but keep improving the previous one skill states - converse method introduced skill states, this changed some core assumptions behind converse method and active skills, OVOS will not adopt skill states, see community discussion here mycroft-core/pull/2901 + mycroft-core/pull/2906 pure regex intents - pure regex intents have been introduced, we strongly recommend you use padatious instead if this is desired, regex makes language support really hard, let the intent engines do their jobs adapt fork - a fork of adapt is used in the mark2, it introduces the exactly and excludes methods. excludes will be added upstream in adapt/pull/156 . Any skill using these new methods will be incompatible with most core versions activities - an activity is just a set of bus messages to indicate something started and ended, it is a reimplementation of an already existing feature, in ovos we use the native events from the self.add_event skill method dinkum contains all changes above and also brought further changes to the table sessions - in dinkum session handling is done by skills, it completely ignores the message.context mechanism and existing session_id, in ovos we believe session should come in the message and handled by the clients (eg, a chat user or a hivemind client....), in ovos we are expanding the original session concept ovos-core/pull/160 dbus-hal - a dbus service specific to the mk2 has been introduced, in ovos we have a generic PHAL service and companion plugins to interface with mk2 hardware instead, this component is mark2 specific and should be ignored in the ovos ecosystem Any skills using these new \"features\" will not work outside the mark2 FAQ Do OVOS skills run in dinkum? No, not even classic core skills run in dinkum. We have no plans to support this Do Dinkum skills run in ovos? No, dinkum is designed in a very incompatible way, the mycroft module is not always mycroft-core and the MycroftSkill class is not always a MycroftSkill, we have no intention of transparently loading dinkum skills in ovos-core We have a small proof of concept tool to convert a dinkum skill into an ovos/classic core compatible skill, see https://github.com/OpenVoiceOS/undinkumfier Does OCP work in dinkum? No, Audio plugin support has been removed, you can run OCP standalone but will be missing the compatibility layers and can't load OCP skills anyway It could be made to work but this is not in the roadmap, PRs will be accepted and reviewed Does PHAL work in dinkum? It should! We don't explicitly target or test it with dinkum, but it is a fairly standalone component Does OPM work in dinkum? STT , TTS and WW plugins should work, We don't explicitly target or test compatibility, PRs will be accepted and reviewed","title":"Mycroft"},{"location":"mycroft/#ovos-core-vs-mycroft-core","text":"","title":"ovos-core vs mycroft-core"},{"location":"mycroft/#speech-client","text":"Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module","title":"Speech Client"},{"location":"mycroft/#audio","text":"Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify","title":"Audio"},{"location":"mycroft/#skills","text":"Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching","title":"Skills"},{"location":"mycroft/#hardware","text":"Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core","title":"Hardware"},{"location":"mycroft/#misc","text":"Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat","title":"Misc"},{"location":"mycroft/#dinkum","text":"","title":"Dinkum"},{"location":"mycroft/#what-is-dinkum","text":"Mycroft Mark2 shipped with a new version of mycroft called \"dinkum\", this is a total overhaul of mycroft-core and incompatible mycroft-core is now referred to as \"Classic Core\" by MycroftAI MycroftAI now provides what they call sandbox images, to add to the confusion those only work in the mark 2 and \"Classic Core\" means the mark-ii/latest branch of mycroft-core, this is a derivative version of the branch that was used in the dev kits ( mark-ii/qa ) and is also backwards incompatible, changes in this branch were not done via PRs and had no review or community input Mark2 useful links: mark 2 docs dinkum source code sandbox images mark2/latest (core for sandbox images) mark2/qa (core for dev kit image)","title":"What is Dinkum"},{"location":"mycroft/#dinkum-vs-ovos-core","text":"you can find mycroft's guide to porting skills to dinkum here https://mycroft-ai.gitbook.io/mark-ii/differences-to-classic-core/porting-classic-core-skills mark2/qa brought some changes to mycroft-core, not all of them backwards compatible and some that were contentious within the community. VAD - VAD has been added to the mark-ii, but it is hardcoded to silero, this feature has been adopted via OPM, it is an important part of ovos-core listening modes introduced in version 0.0.5 self.resources - resource file loading was overhauled, this feature has been improved ( ovos-core/pull/130 + ovos-core/pull/131 + ovos-core/pull/135 + ovos-core/pull/170 ) and ported to OVOS and is also available in OVOSkill class ( OVOS-workshop/pull/30 ) for usage in classic core audio hal - audio playback was rewritten from scratch, audio plugin support has been removed, OVOS will not adopt this new approach but keep improving the previous one skill states - converse method introduced skill states, this changed some core assumptions behind converse method and active skills, OVOS will not adopt skill states, see community discussion here mycroft-core/pull/2901 + mycroft-core/pull/2906 pure regex intents - pure regex intents have been introduced, we strongly recommend you use padatious instead if this is desired, regex makes language support really hard, let the intent engines do their jobs adapt fork - a fork of adapt is used in the mark2, it introduces the exactly and excludes methods. excludes will be added upstream in adapt/pull/156 . Any skill using these new methods will be incompatible with most core versions activities - an activity is just a set of bus messages to indicate something started and ended, it is a reimplementation of an already existing feature, in ovos we use the native events from the self.add_event skill method dinkum contains all changes above and also brought further changes to the table sessions - in dinkum session handling is done by skills, it completely ignores the message.context mechanism and existing session_id, in ovos we believe session should come in the message and handled by the clients (eg, a chat user or a hivemind client....), in ovos we are expanding the original session concept ovos-core/pull/160 dbus-hal - a dbus service specific to the mk2 has been introduced, in ovos we have a generic PHAL service and companion plugins to interface with mk2 hardware instead, this component is mark2 specific and should be ignored in the ovos ecosystem Any skills using these new \"features\" will not work outside the mark2","title":"Dinkum vs ovos-core"},{"location":"mycroft/#faq","text":"","title":"FAQ"},{"location":"mycroft/#do-ovos-skills-run-in-dinkum","text":"No, not even classic core skills run in dinkum. We have no plans to support this","title":"Do OVOS skills run in dinkum?"},{"location":"mycroft/#do-dinkum-skills-run-in-ovos","text":"No, dinkum is designed in a very incompatible way, the mycroft module is not always mycroft-core and the MycroftSkill class is not always a MycroftSkill, we have no intention of transparently loading dinkum skills in ovos-core We have a small proof of concept tool to convert a dinkum skill into an ovos/classic core compatible skill, see https://github.com/OpenVoiceOS/undinkumfier","title":"Do Dinkum skills run in ovos?"},{"location":"mycroft/#does-ocp-work-in-dinkum","text":"No, Audio plugin support has been removed, you can run OCP standalone but will be missing the compatibility layers and can't load OCP skills anyway It could be made to work but this is not in the roadmap, PRs will be accepted and reviewed","title":"Does OCP work in dinkum?"},{"location":"mycroft/#does-phal-work-in-dinkum","text":"It should! We don't explicitly target or test it with dinkum, but it is a fairly standalone component","title":"Does PHAL work in dinkum?"},{"location":"mycroft/#does-opm-work-in-dinkum","text":"STT , TTS and WW plugins should work, We don't explicitly target or test compatibility, PRs will be accepted and reviewed","title":"Does OPM work in dinkum?"},{"location":"neonos/","text":"NeonOS Coming soon","title":"NeonOS"},{"location":"neonos/#neonos","text":"Coming soon","title":"NeonOS"},{"location":"ocp/","text":"OCP OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player Introduction OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from a typical mycroft-core audio service in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill OCP skills have a dedicated MycroftSkill subclass and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP mycroft-core CommonPlay skill framework is disabled when OCP loads WARNING : this will be removed in ovos-core 0.1.0 or earlier mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills WARNING : this will be removed in ovos-core 0.1.0 or earlier OCP Skills Skills provide search results, think about them as media providers/catalogs for OCP You can find OCP skills in the awesome-ocp-skills list Skills Menu Some skills provide featured_media, you can access these from the OCP menu Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework. File Browser integration selected files will be played in OCP folders are considered playlists MPRIS integration Sync with external players Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify Manage multiple players If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal) Configuration GUI Some OCP settings are exposed via the GUI Visualization can be either a wave or bars, just tap on it to change Timeout settings, return to homescreen widget after 30 seconds Advanced OCP contains an audio service plugin component that acts as a compatibility layer with MycroftAI CommonPlay skills framework. in mycroft-core you can set OCP as default-backend , ovos-core is not required for OCP For compatibility and historical reasons OCP reads its configuration from the \"Audio\" section \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true // all values below are optional // plugin config // operational mode refers to the OCP integration // \"external\" - OCP is already running elsewhere, connect by bus only // \"native\" - launch OCP service from the plugin // \"auto\" - if OCP is already running connect to it, else launch it // you should only change this if you want to run OCP as a standalone system service \"mode\": \"auto\", // MPRIS integrations // integration is enabled by default, but can be disabled \"disable_mpris\": False, // dbus type for MPRIS, \"session\" or \"system\" \"dbus_type\": \"session\", // allow OCP to control MPRIS enabled 3rd party applications // voice enable them (next/prev/stop/resume..) // and stop them when OCP starts it's own playback // NOTE: OCP can be controlled itself via MPRIS independentely of this setting \"manage_external_players\": False, // Playback settings // AUTO = 0 - play each entry as considered appropriate, // ie, make it happen the best way possible // AUDIO_ONLY = 10 - only consider audio entries // VIDEO_ONLY = 20 - only consider video entries // FORCE_AUDIO = 30 - cast video to audio unconditionally // (audio can still play in mycroft-gui) // FORCE_AUDIOSERVICE = 40 - cast everything to audio service backend, // mycroft-gui will not be used // EVENTS_ONLY = 50 - only emit ocp events, // do not display or play anything. // allows integration with external interfaces \"playback_mode\": 0, // ordered list of audio backend preferences, // when OCP selects a audio service for playback // this list is checked in order until a available backend is found \"preferred_audio_services\": [\"vlc\", \"mplayer\", \"simple\"], // when media playback ends \"click next\" \"autoplay\": True, // if True behaves as if the search results are part of the playlist // eg: // - click next in last track -> play next search result // - end of playlist + autoplay -> play next search result \"merge_search\": True, // search params // minimum time to wait for skill replies, // after this time, if at least 1 result was // found, selection is triggered \"min_timeout\": 5, // maximum time to wait for skill replies, // after this time, regardless of number of // results, selection is triggered \"max_timeout\": 15, // ignore results below min_Score \"min_score\": 50, // stop collecting results if we get a // match with confidence >= early_stop_thresh \"early_stop_thresh\": 85, // sleep this amount before early stop, // allows skills that \"just miss\" to also be taken into account \"early_stop_grace_period\": 0.5, // if True emits the regular mycroft-core // bus messages to get results from \"old style\" skills \"backwards_compatibility\": True, // allow skills to request more time, // extends min_timeout for individual queries (up to max_timeout) \"allow_extensions\": True, // if no results for a MediaType, perform a second query with MediaType.GENERIC \"search_fallback\": True, // stream extractor settings // how to handle bandcamp streams // \"pybandcamp\", \"youtube-dl\" \"bandcamp_backend\": \"pybandcamp\", // how to handle youtube streams // \"youtube-dl\", \"pytube\", \"pafy\", \"invidious\" \"youtube_backend\": \"invidious\", // the url to the invidious instance to be used\" // by default uses a random instance \"invidious_host\": None, // get final stream locally or from where invidious is hosted // This partially allows bypassing geoblocked content, // but it is a global flag, not per entry. \"invidious_proxy\": False, // different forks of youtube-dl are supported // \"yt-dlp\", \"youtube-dl\", \"youtube-dlc\" \"ydl_backend\": \"yt-dlp\", // how to extract live streams from a youtube channel // \"pytube\", \"youtube_searcher\", \"redirect\", \"youtube-dl\" // uses youtube auto redirect https://www.youtube.com/{channel_name}/live \"youtube_live_backend\": \"redirect\" } }","title":"OCP"},{"location":"ocp/#ocp","text":"OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player","title":"OCP"},{"location":"ocp/#introduction","text":"OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from a typical mycroft-core audio service in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill OCP skills have a dedicated MycroftSkill subclass and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP mycroft-core CommonPlay skill framework is disabled when OCP loads WARNING : this will be removed in ovos-core 0.1.0 or earlier mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills WARNING : this will be removed in ovos-core 0.1.0 or earlier","title":"Introduction"},{"location":"ocp/#ocp-skills","text":"Skills provide search results, think about them as media providers/catalogs for OCP You can find OCP skills in the awesome-ocp-skills list","title":"OCP Skills"},{"location":"ocp/#skills-menu","text":"Some skills provide featured_media, you can access these from the OCP menu","title":"Skills Menu"},{"location":"ocp/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"ocp/#file-browser-integration","text":"selected files will be played in OCP folders are considered playlists","title":"File Browser integration"},{"location":"ocp/#mpris-integration","text":"","title":"MPRIS integration"},{"location":"ocp/#sync-with-external-players","text":"Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify","title":"Sync with external players"},{"location":"ocp/#manage-multiple-players","text":"If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal)","title":"Manage multiple players"},{"location":"ocp/#configuration","text":"","title":"Configuration"},{"location":"ocp/#gui","text":"Some OCP settings are exposed via the GUI Visualization can be either a wave or bars, just tap on it to change Timeout settings, return to homescreen widget after 30 seconds","title":"GUI"},{"location":"ocp/#advanced","text":"OCP contains an audio service plugin component that acts as a compatibility layer with MycroftAI CommonPlay skills framework. in mycroft-core you can set OCP as default-backend , ovos-core is not required for OCP For compatibility and historical reasons OCP reads its configuration from the \"Audio\" section \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true // all values below are optional // plugin config // operational mode refers to the OCP integration // \"external\" - OCP is already running elsewhere, connect by bus only // \"native\" - launch OCP service from the plugin // \"auto\" - if OCP is already running connect to it, else launch it // you should only change this if you want to run OCP as a standalone system service \"mode\": \"auto\", // MPRIS integrations // integration is enabled by default, but can be disabled \"disable_mpris\": False, // dbus type for MPRIS, \"session\" or \"system\" \"dbus_type\": \"session\", // allow OCP to control MPRIS enabled 3rd party applications // voice enable them (next/prev/stop/resume..) // and stop them when OCP starts it's own playback // NOTE: OCP can be controlled itself via MPRIS independentely of this setting \"manage_external_players\": False, // Playback settings // AUTO = 0 - play each entry as considered appropriate, // ie, make it happen the best way possible // AUDIO_ONLY = 10 - only consider audio entries // VIDEO_ONLY = 20 - only consider video entries // FORCE_AUDIO = 30 - cast video to audio unconditionally // (audio can still play in mycroft-gui) // FORCE_AUDIOSERVICE = 40 - cast everything to audio service backend, // mycroft-gui will not be used // EVENTS_ONLY = 50 - only emit ocp events, // do not display or play anything. // allows integration with external interfaces \"playback_mode\": 0, // ordered list of audio backend preferences, // when OCP selects a audio service for playback // this list is checked in order until a available backend is found \"preferred_audio_services\": [\"vlc\", \"mplayer\", \"simple\"], // when media playback ends \"click next\" \"autoplay\": True, // if True behaves as if the search results are part of the playlist // eg: // - click next in last track -> play next search result // - end of playlist + autoplay -> play next search result \"merge_search\": True, // search params // minimum time to wait for skill replies, // after this time, if at least 1 result was // found, selection is triggered \"min_timeout\": 5, // maximum time to wait for skill replies, // after this time, regardless of number of // results, selection is triggered \"max_timeout\": 15, // ignore results below min_Score \"min_score\": 50, // stop collecting results if we get a // match with confidence >= early_stop_thresh \"early_stop_thresh\": 85, // sleep this amount before early stop, // allows skills that \"just miss\" to also be taken into account \"early_stop_grace_period\": 0.5, // if True emits the regular mycroft-core // bus messages to get results from \"old style\" skills \"backwards_compatibility\": True, // allow skills to request more time, // extends min_timeout for individual queries (up to max_timeout) \"allow_extensions\": True, // if no results for a MediaType, perform a second query with MediaType.GENERIC \"search_fallback\": True, // stream extractor settings // how to handle bandcamp streams // \"pybandcamp\", \"youtube-dl\" \"bandcamp_backend\": \"pybandcamp\", // how to handle youtube streams // \"youtube-dl\", \"pytube\", \"pafy\", \"invidious\" \"youtube_backend\": \"invidious\", // the url to the invidious instance to be used\" // by default uses a random instance \"invidious_host\": None, // get final stream locally or from where invidious is hosted // This partially allows bypassing geoblocked content, // but it is a global flag, not per entry. \"invidious_proxy\": False, // different forks of youtube-dl are supported // \"yt-dlp\", \"youtube-dl\", \"youtube-dlc\" \"ydl_backend\": \"yt-dlp\", // how to extract live streams from a youtube channel // \"pytube\", \"youtube_searcher\", \"redirect\", \"youtube-dl\" // uses youtube auto redirect https://www.youtube.com/{channel_name}/live \"youtube_live_backend\": \"redirect\" } }","title":"Advanced"},{"location":"opm/","text":"OPM OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Plugin Packaging Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} ) Projects using OPM OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"OPM"},{"location":"opm/#opm","text":"OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py","title":"OPM"},{"location":"opm/#plugin-packaging","text":"Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} )","title":"Plugin Packaging"},{"location":"opm/#projects-using-opm","text":"OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"Projects using OPM"},{"location":"osm/","text":"","title":"Osm"},{"location":"ovos_picroft/","text":"ovos-picroft OVOS on top of RaspberryPiOS Lite Purpose of this guide This guide will provide you with a minimal HEADLESS ovos system sutable for running on a Raspberry Pi 3. The RPi3 does not have the processing power to reliably run ovos-shell , the GUI system for OVOS, but has pleanty to run the rest of the stack. By the end of the guide, you should have a running OVOS stack, (messagebus, phal, skills, voice, and speech), along with a \"lite\" version of RaspberryPiOS. Which means you have a package manager, (apt) available to you also. Source files used by this guide can be found at ovos-picroft . Any issues or pull requests should be made in this repository. Step 1: Create the boot medium Download latest lite image and install to SD card or USB. There are lots of guides, but this one is the official guide This is the suggested download Step 2: Setup the system Boot your newly created medium and follow the prompts to create an user. - Create user ovos with password ovos - The system will reboot and ask you to log in. Log in with the above credentals raspi-config RaspberryPiOS comes with a great tool raspi-config . We need to access that to get started setting things up. Run the command sudo raspi-config to enter the utility. We will be running everything as a regular user, so we want to auto login. Enter the System Options page Enter the Boot / Autologin page - Use the second option in the menu. Console Autologin - This enables the ovos user to login to a console terminal on every boot Now we will enable a few interface options. This will allow us to access our device from a ssh shell and prep the PI for other devices that may be used. Some microphone hats require SPI, or I2C (Respeaker, AIY-Voicebonnet, etc) Go back to the main menu and enter the Interface Options page - Enable SSH, SPI, I2C - After SSH is enabled, the rest of the guide can be done from a remote computer Back to the main menu and enter the Localisation Options page - Configure Locale, Timezone, WLAN Country You will need an internet connection to complete the rest of the guide Optional: Setup WIFI Return to the main menu and enter System Options again. Enter the Wireless LAN section and follow the prompts Exit out of the raspi-config tool. And find your IP address. The command is the same if you used the WiFi setup, or have a LAN connected. Enter the command ip addr In the output,if things were configured correctly, there will be one or more lines that are relavant. Find the device that you used to connect, WiFi will start with something like wlan and a LAN connection should begin with eth or enp or something similar. In the device section, there is an inet entry. The number located there is your local IP address. It should be in the format 192.168.x.xxx or something similar. Write this down, or remember it. You will use it to log in with a SSH shell Now the device setup is done. Reboot sudo reboot now From this point on, you should be able to access your device from any SSH terminal. For guide for how to do this, see https://www.raspberrypi.com/documentation/computers/remote-access.html . From a linux machine, open a terminal and enter the command ssh ovos@<your-remembered-IP-address> . There will be a warning making sure you want to connect to this device. Enter yes, and when asked, enter the password for ovos that you made earlier in the setup. ovos First you need to make sure your system is up to date. It should be close as you just installed a new image. sudo apt -y update && sudo apt -y upgrade We should be done with the basic setup now. You should have a running RaspberryPiOS device with the user ovos Step 3: Install OVOS-CORE There are some recommendations to use a venv for ovos. This guide DOES NOT do that. The ovos headless stack on a RPi3 is about all it can handle. It is assumed that this is a dedicated ovos device, therefore no venv is required. There are a few packages required for ovos, so we will install those first sudo apt install build-essential python3-dev python3-pip swig libssl-dev libfann-dev portaudio19-dev libpulse-dev cmake git libncurses-dev pulseaudio-utils We will be using pip to install ovos-core and other related software. We will also be installing everything to the user environment instead of system wide. As ovos is the only user, this should be fine. We will assume that everything from here will be done in the home directory of ovos. cd ~ We will use pip to install ovos-core, but we will be using the latest version directly from github. git clone https://github.com/OpenVoiceOS/ovos-core pip install ./ovos-core[audio,PHAL,stt,tts,skills_lgpl,skills,bus,skills-essential] The rest of the setup requires a few more files. git clone https://github.com/OpenVoiceOS/ovos-picroft.git Step 4: Install the systemd files We will be installing the systemd files as a regular user instead of system wide. The official ovos buildroot images installs these files in /usr/lib/systemd/user/ . There are guides that say user systemd files can also be placed in /etc/systemd/user. or $HOME/.config/systemd/user/ . We will be using the users home directory to avoid any permission issues. Enter the cloned repo ovos-picroft assuming you cloned this to your home directory cd ~/ovos-picroft/systemd/ Copy the files from there cp * ~/.config/systemd/user/ Reload the systemd daemon systemctl --user daemon-reload Enable the system files systemctl --user enable mycroft* Step 5: Install the executables These are the files that systemd uses to start ovos. These include hooks for restarting and stopping the services. cd ~/ovos-picroft/exec/ Here we need to copy the files to the right location. cp * ~/.local/bin/ And make them executable chmod a+x ~/.local/bin/mycroft* These executables require sdnotify pip install sdnotify Do a reboot sudo reboot now This takes a while, especially when you are used to a Rpi4 or x86 install. Loading everything is about as much as a Rpi3 can handle I think. You should now have a running ovos device!! Check with this systemctl --user status mycroft* It takes a while to load, but they should all eventually say active (running) , except for mycroft.service which should say active (exited) Step 6: Final thoughts A generic install of ovos-core does not have any default skills shipped with it. Check this page for more information on skills. Audio is also not covered here. Pulseaudio should be running, check with systemctl --user status pulseaudio , but each piece of hardware is different to setup. I am sure there is a guide somewhere for your hardware. One thing to mention, this is a full raspbian install, so installing drivers should work also.","title":"ovos-picroft"},{"location":"ovos_picroft/#ovos-picroft","text":"OVOS on top of RaspberryPiOS Lite","title":"ovos-picroft"},{"location":"ovos_picroft/#purpose-of-this-guide","text":"This guide will provide you with a minimal HEADLESS ovos system sutable for running on a Raspberry Pi 3. The RPi3 does not have the processing power to reliably run ovos-shell , the GUI system for OVOS, but has pleanty to run the rest of the stack. By the end of the guide, you should have a running OVOS stack, (messagebus, phal, skills, voice, and speech), along with a \"lite\" version of RaspberryPiOS. Which means you have a package manager, (apt) available to you also. Source files used by this guide can be found at ovos-picroft . Any issues or pull requests should be made in this repository.","title":"Purpose of this guide"},{"location":"ovos_picroft/#step-1-create-the-boot-medium","text":"Download latest lite image and install to SD card or USB. There are lots of guides, but this one is the official guide This is the suggested download","title":"Step 1: Create the boot medium"},{"location":"ovos_picroft/#step-2-setup-the-system","text":"Boot your newly created medium and follow the prompts to create an user. - Create user ovos with password ovos - The system will reboot and ask you to log in. Log in with the above credentals","title":"Step 2: Setup the system"},{"location":"ovos_picroft/#raspi-config","text":"RaspberryPiOS comes with a great tool raspi-config . We need to access that to get started setting things up. Run the command sudo raspi-config to enter the utility. We will be running everything as a regular user, so we want to auto login. Enter the System Options page Enter the Boot / Autologin page - Use the second option in the menu. Console Autologin - This enables the ovos user to login to a console terminal on every boot Now we will enable a few interface options. This will allow us to access our device from a ssh shell and prep the PI for other devices that may be used. Some microphone hats require SPI, or I2C (Respeaker, AIY-Voicebonnet, etc) Go back to the main menu and enter the Interface Options page - Enable SSH, SPI, I2C - After SSH is enabled, the rest of the guide can be done from a remote computer Back to the main menu and enter the Localisation Options page - Configure Locale, Timezone, WLAN Country You will need an internet connection to complete the rest of the guide","title":"raspi-config"},{"location":"ovos_picroft/#optional-setup-wifi","text":"Return to the main menu and enter System Options again. Enter the Wireless LAN section and follow the prompts Exit out of the raspi-config tool. And find your IP address. The command is the same if you used the WiFi setup, or have a LAN connected. Enter the command ip addr In the output,if things were configured correctly, there will be one or more lines that are relavant. Find the device that you used to connect, WiFi will start with something like wlan and a LAN connection should begin with eth or enp or something similar. In the device section, there is an inet entry. The number located there is your local IP address. It should be in the format 192.168.x.xxx or something similar. Write this down, or remember it. You will use it to log in with a SSH shell Now the device setup is done. Reboot sudo reboot now From this point on, you should be able to access your device from any SSH terminal. For guide for how to do this, see https://www.raspberrypi.com/documentation/computers/remote-access.html . From a linux machine, open a terminal and enter the command ssh ovos@<your-remembered-IP-address> . There will be a warning making sure you want to connect to this device. Enter yes, and when asked, enter the password for ovos that you made earlier in the setup. ovos First you need to make sure your system is up to date. It should be close as you just installed a new image. sudo apt -y update && sudo apt -y upgrade We should be done with the basic setup now. You should have a running RaspberryPiOS device with the user ovos","title":"Optional: Setup WIFI"},{"location":"ovos_picroft/#step-3-install-ovos-core","text":"There are some recommendations to use a venv for ovos. This guide DOES NOT do that. The ovos headless stack on a RPi3 is about all it can handle. It is assumed that this is a dedicated ovos device, therefore no venv is required. There are a few packages required for ovos, so we will install those first sudo apt install build-essential python3-dev python3-pip swig libssl-dev libfann-dev portaudio19-dev libpulse-dev cmake git libncurses-dev pulseaudio-utils We will be using pip to install ovos-core and other related software. We will also be installing everything to the user environment instead of system wide. As ovos is the only user, this should be fine. We will assume that everything from here will be done in the home directory of ovos. cd ~ We will use pip to install ovos-core, but we will be using the latest version directly from github. git clone https://github.com/OpenVoiceOS/ovos-core pip install ./ovos-core[audio,PHAL,stt,tts,skills_lgpl,skills,bus,skills-essential] The rest of the setup requires a few more files. git clone https://github.com/OpenVoiceOS/ovos-picroft.git","title":"Step 3: Install OVOS-CORE"},{"location":"ovos_picroft/#step-4-install-the-systemd-files","text":"We will be installing the systemd files as a regular user instead of system wide. The official ovos buildroot images installs these files in /usr/lib/systemd/user/ . There are guides that say user systemd files can also be placed in /etc/systemd/user. or $HOME/.config/systemd/user/ . We will be using the users home directory to avoid any permission issues. Enter the cloned repo ovos-picroft assuming you cloned this to your home directory cd ~/ovos-picroft/systemd/ Copy the files from there cp * ~/.config/systemd/user/ Reload the systemd daemon systemctl --user daemon-reload Enable the system files systemctl --user enable mycroft*","title":"Step 4: Install the systemd files"},{"location":"ovos_picroft/#step-5-install-the-executables","text":"These are the files that systemd uses to start ovos. These include hooks for restarting and stopping the services. cd ~/ovos-picroft/exec/ Here we need to copy the files to the right location. cp * ~/.local/bin/ And make them executable chmod a+x ~/.local/bin/mycroft* These executables require sdnotify pip install sdnotify Do a reboot sudo reboot now This takes a while, especially when you are used to a Rpi4 or x86 install. Loading everything is about as much as a Rpi3 can handle I think. You should now have a running ovos device!! Check with this systemctl --user status mycroft* It takes a while to load, but they should all eventually say active (running) , except for mycroft.service which should say active (exited)","title":"Step 5: Install the executables"},{"location":"ovos_picroft/#step-6-final-thoughts","text":"A generic install of ovos-core does not have any default skills shipped with it. Check this page for more information on skills. Audio is also not covered here. Pulseaudio should be running, check with systemctl --user status pulseaudio , but each piece of hardware is different to setup. I am sure there is a guide somewhere for your hardware. One thing to mention, this is a full raspbian install, so installing drivers should work also.","title":"Step 6: Final thoughts"},{"location":"personal_backend/","text":"OVOS Personal Backend Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers :warning: there are no user accounts :warning: This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network For a full backend experience, the official mycroft backend has been open sourced, read the blog post NOTE: There is no pairing, devices will just activate themselves and work Install from pip pip install ovos-local-backend Mycroft Setup There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } } Companion projects ovos-backend-client - reference python library to interact with selene/local backend ovos-backend-manager - graphical interface to manage all things backend ovos-stt-plugin-selene - stt plugin for selene/local backend Usage start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host Docker There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux) How it works Configuration configure backend by editing/creating ~/.config/json_database/ovos_backend.json see default values here { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances Databases Since the local backend is not meant to provision hundreds of devices or manage user accounts it works only with json databases metadata about uploaded wakewords can be found at ~/.local/share/json_database/ovos_wakewords.jsondb metadata about uploaded utterances can be found at ~/.local/share/json_database/ovos_utterances.jsondb database of uploaded metrics can be found at ~/.local/share/json_database/ovos_metrics.jsondb paired devices database can be found at ~/.local/share/json_database/ovos_devices.json per device skill settings database can be found at ~/.local/share/json_database/ovos_skill_settings.json shared skill settings database can be found at ~/.local/share/json_database/ovos_shared_skill_settings.json metrics, wake words and utterances respect the individual devices opt_in flag, nothing will be saved unless devices opt_in (default True) Device Settings Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api (./ovos_local_backend/backend/admin.py) you can also change these settings per device by manually editing paired devices database Location Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database Device Preferences Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api (./ovos_local_backend/backend/admin.py) you can also set these values per device by manually editing paired devices database Skill settings in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api (./ovos_local_backend/backend/admin.py) you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database Email Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api (./ovos_local_backend/backend/admin.py) you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps Selene Proxy You can integrate local backend with selene, the backend will show up as a device you can manage in mycroft.home wait... what? isn't the point of local backend to disable selene? Open Dataset, You do not want to use selene, but you want to opt_in to the open dataset (share recordings with mycroft) Privacy, you want to use selene, but you do not want to give away your personal data (email, location, ip address...) Control, you want to use only a subset of selene features Convenience, pair once, manage all your devices Functionality, extra features such as isolated skill settings and forced 2 way sync Esoteric Setups, isolated mycroft services that can not share an identity file, such as ovos-qubes Pairing To pair the local backend with selene you have 2 options 1 - pair a mycroft-core instance, then copy the identity file 2 - enable proxy_pairing, whenever a device pairs with local backend the code it speaks is also valid for selene, use that code to pair local backend with selene If a device tries to use a selene enabled endpoint without the backend being paired a 401 authentication error will be returned, if the endpoint does not use selene (e.g. disabled in config) this check is skipped Selene Config In your backend config add the following section \"selene\": { \"enabled\": False, # needs to be explicitly enabled by user \"url\": \"https://api.mycroft.ai\", # change if you are self-hosting selene \"version\": \"v1\", # pairing settings # NOTE: the file should be used exclusively by backend, do not share with a mycroft-core instance \"identity_file\": BACKEND_IDENTITY, # path to identity2.json file # send the pairing from selene to any device that attempts to pair with local backend # this will provide voice/gui prompts to the user and avoid the need to copy an identity file # only happens if backend is not paired with selene (hopefully exactly once) # if False you need to pair an existing mycroft-core as usual and move the file for backend usage \"proxy_pairing\": False, # micro service settings # NOTE: STT is handled at plugin level, configure ovos-stt-plugin-selene \"proxy_weather\": True, # use selene for weather api calls \"proxy_wolfram\": True, # use selene for wolfram alpha api calls \"proxy_geolocation\": True, # use selene for geolocation api calls \"proxy_email\": False, # use selene for sending email (only for email registered in selene) # device settings - if you want to spoof data in selene set these to False \"download_location\": True, # set default location from selene \"download_prefs\": True, # set default device preferences from selene \"download_settings\": True, # download shared skill settings from selene \"upload_settings\": True, # upload shared skill settings to selene \"force2way\": False, # this forcefully re-enables 2way settings sync with selene # this functionality was removed from core, we hijack the settingsmeta endpoint to upload settings # upload will happen when mycroft-core boots and overwrite any values in selene (no checks for settings changed) # the assumption is that selene changes are downloaded instantaneously # if a device is offline when selene changes those changes will be discarded on next device boot # opt-in settings - what data to share with selene # NOTE: these also depend on opt_in being set in selene \"opt_in\": False, # share data from all devices with selene (as if from a single device) \"opt_in_blacklist\": [], # list of uuids that should ignore opt_in flag (never share data) \"upload_metrics\": True, # upload device metrics to selene \"upload_wakewords\": True, # upload wake word samples to selene \"upload_utterances\": True # upload utterance samples to selene }","title":"Personal Backend"},{"location":"personal_backend/#ovos-personal-backend","text":"Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers :warning: there are no user accounts :warning: This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network For a full backend experience, the official mycroft backend has been open sourced, read the blog post NOTE: There is no pairing, devices will just activate themselves and work","title":"OVOS Personal Backend"},{"location":"personal_backend/#install","text":"from pip pip install ovos-local-backend","title":"Install"},{"location":"personal_backend/#mycroft-setup","text":"There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } }","title":"Mycroft Setup"},{"location":"personal_backend/#companion-projects","text":"ovos-backend-client - reference python library to interact with selene/local backend ovos-backend-manager - graphical interface to manage all things backend ovos-stt-plugin-selene - stt plugin for selene/local backend","title":"Companion projects"},{"location":"personal_backend/#usage","text":"start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host","title":"Usage"},{"location":"personal_backend/#docker","text":"There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux)","title":"Docker"},{"location":"personal_backend/#how-it-works","text":"","title":"How it works"},{"location":"personal_backend/#configuration","text":"configure backend by editing/creating ~/.config/json_database/ovos_backend.json see default values here { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances","title":"Configuration"},{"location":"personal_backend/#databases","text":"Since the local backend is not meant to provision hundreds of devices or manage user accounts it works only with json databases metadata about uploaded wakewords can be found at ~/.local/share/json_database/ovos_wakewords.jsondb metadata about uploaded utterances can be found at ~/.local/share/json_database/ovos_utterances.jsondb database of uploaded metrics can be found at ~/.local/share/json_database/ovos_metrics.jsondb paired devices database can be found at ~/.local/share/json_database/ovos_devices.json per device skill settings database can be found at ~/.local/share/json_database/ovos_skill_settings.json shared skill settings database can be found at ~/.local/share/json_database/ovos_shared_skill_settings.json metrics, wake words and utterances respect the individual devices opt_in flag, nothing will be saved unless devices opt_in (default True)","title":"Databases"},{"location":"personal_backend/#device-settings","text":"Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api (./ovos_local_backend/backend/admin.py) you can also change these settings per device by manually editing paired devices database","title":"Device Settings"},{"location":"personal_backend/#location","text":"Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database","title":"Location"},{"location":"personal_backend/#device-preferences","text":"Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api (./ovos_local_backend/backend/admin.py) you can also set these values per device by manually editing paired devices database","title":"Device Preferences"},{"location":"personal_backend/#skill-settings","text":"in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api (./ovos_local_backend/backend/admin.py) you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database","title":"Skill settings"},{"location":"personal_backend/#email","text":"Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api (./ovos_local_backend/backend/admin.py) you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps","title":"Email"},{"location":"personal_backend/#selene-proxy","text":"You can integrate local backend with selene, the backend will show up as a device you can manage in mycroft.home wait... what? isn't the point of local backend to disable selene? Open Dataset, You do not want to use selene, but you want to opt_in to the open dataset (share recordings with mycroft) Privacy, you want to use selene, but you do not want to give away your personal data (email, location, ip address...) Control, you want to use only a subset of selene features Convenience, pair once, manage all your devices Functionality, extra features such as isolated skill settings and forced 2 way sync Esoteric Setups, isolated mycroft services that can not share an identity file, such as ovos-qubes","title":"Selene Proxy"},{"location":"personal_backend/#pairing","text":"To pair the local backend with selene you have 2 options 1 - pair a mycroft-core instance, then copy the identity file 2 - enable proxy_pairing, whenever a device pairs with local backend the code it speaks is also valid for selene, use that code to pair local backend with selene If a device tries to use a selene enabled endpoint without the backend being paired a 401 authentication error will be returned, if the endpoint does not use selene (e.g. disabled in config) this check is skipped","title":"Pairing"},{"location":"personal_backend/#selene-config","text":"In your backend config add the following section \"selene\": { \"enabled\": False, # needs to be explicitly enabled by user \"url\": \"https://api.mycroft.ai\", # change if you are self-hosting selene \"version\": \"v1\", # pairing settings # NOTE: the file should be used exclusively by backend, do not share with a mycroft-core instance \"identity_file\": BACKEND_IDENTITY, # path to identity2.json file # send the pairing from selene to any device that attempts to pair with local backend # this will provide voice/gui prompts to the user and avoid the need to copy an identity file # only happens if backend is not paired with selene (hopefully exactly once) # if False you need to pair an existing mycroft-core as usual and move the file for backend usage \"proxy_pairing\": False, # micro service settings # NOTE: STT is handled at plugin level, configure ovos-stt-plugin-selene \"proxy_weather\": True, # use selene for weather api calls \"proxy_wolfram\": True, # use selene for wolfram alpha api calls \"proxy_geolocation\": True, # use selene for geolocation api calls \"proxy_email\": False, # use selene for sending email (only for email registered in selene) # device settings - if you want to spoof data in selene set these to False \"download_location\": True, # set default location from selene \"download_prefs\": True, # set default device preferences from selene \"download_settings\": True, # download shared skill settings from selene \"upload_settings\": True, # upload shared skill settings to selene \"force2way\": False, # this forcefully re-enables 2way settings sync with selene # this functionality was removed from core, we hijack the settingsmeta endpoint to upload settings # upload will happen when mycroft-core boots and overwrite any values in selene (no checks for settings changed) # the assumption is that selene changes are downloaded instantaneously # if a device is offline when selene changes those changes will be discarded on next device boot # opt-in settings - what data to share with selene # NOTE: these also depend on opt_in being set in selene \"opt_in\": False, # share data from all devices with selene (as if from a single device) \"opt_in_blacklist\": [], # list of uuids that should ignore opt_in flag (never share data) \"upload_metrics\": True, # upload device metrics to selene \"upload_wakewords\": True, # upload wake word samples to selene \"upload_utterances\": True # upload utterance samples to selene }","title":"Selene Config"},{"location":"playing_music/","text":"Playing music For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list Youtube Music A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter. Play the news Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news Some more features that come out of the box The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing. Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Playing music"},{"location":"playing_music/#playing-music","text":"For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list","title":"Playing music"},{"location":"playing_music/#youtube-music","text":"A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter.","title":"Youtube Music"},{"location":"playing_music/#play-the-news","text":"Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news","title":"Play the news"},{"location":"playing_music/#some-more-features-that-come-out-of-the-box","text":"The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing.","title":"Some more features that come out of the box"},{"location":"playing_music/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"playing_video/","text":"Playing video Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"playing_video/#playing-video","text":"Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"plugin_intro/","text":"","title":"Plugin intro"},{"location":"scraps/","text":"from downloading image Buildroot SSH Details: Username: mycroft | password: mycroft Manjaro SSH Details for Respeaker Image: Username: mycroft | password: 12345 Manjaro SSH Details for Mark-2/DevKit Image: Username: ovos | password: ovos buildroot manjaro From Backend Admin Api (personal backend only!) Since local backend does not provide a web ui a admin api can be used to manage your devices A backend is a service that provides your device with additional tools to function, these could range from managing your skill settings to configuring certain aspects of your device OpenVoiceOS is all about choice, We currently support 3 backend types Selene Backend The mycroft backend connects your device to mycroft servers and allows you to use their web interface to manage your device, this requires paring and all your Speech to text queries are processed via this backend Personal Backend The personal backend is a choice for users who would like to self-host their own backend on the device or in their personal home network, this backend requires additional setup but also provides a cool web interface to configure your device and manage your settings No Backend Open Voice OS by default comes with no backend, we do not really believe you need a backend to do anything, this is the best choice for your device if you wish to run completely locally, we provide you with a whole list of Speech to text and Text to speech online and offline options to choose from, All communication to the outside world happens from your own device without data sharing From First Run Setting up your device at first run. At first run of your OpenVoiceOS device a first run setup wizard is started that guides you through the process of setting up your device.","title":"Scraps"},{"location":"scraps/#from-downloading-image","text":"Buildroot SSH Details: Username: mycroft | password: mycroft Manjaro SSH Details for Respeaker Image: Username: mycroft | password: 12345 Manjaro SSH Details for Mark-2/DevKit Image: Username: ovos | password: ovos buildroot manjaro From Backend","title":"from downloading image"},{"location":"scraps/#admin-api-personal-backend-only","text":"Since local backend does not provide a web ui a admin api can be used to manage your devices A backend is a service that provides your device with additional tools to function, these could range from managing your skill settings to configuring certain aspects of your device OpenVoiceOS is all about choice, We currently support 3 backend types","title":"Admin Api (personal backend only!)"},{"location":"scraps/#selene-backend","text":"The mycroft backend connects your device to mycroft servers and allows you to use their web interface to manage your device, this requires paring and all your Speech to text queries are processed via this backend","title":"Selene Backend"},{"location":"scraps/#personal-backend","text":"The personal backend is a choice for users who would like to self-host their own backend on the device or in their personal home network, this backend requires additional setup but also provides a cool web interface to configure your device and manage your settings","title":"Personal Backend"},{"location":"scraps/#no-backend","text":"Open Voice OS by default comes with no backend, we do not really believe you need a backend to do anything, this is the best choice for your device if you wish to run completely locally, we provide you with a whole list of Speech to text and Text to speech online and offline options to choose from, All communication to the outside world happens from your own device without data sharing","title":"No Backend"},{"location":"scraps/#from-first-run","text":"","title":"From First Run"},{"location":"scraps/#setting-up-your-device-at-first-run","text":"At first run of your OpenVoiceOS device a first run setup wizard is started that guides you through the process of setting up your device.","title":"Setting up your device at first run."},{"location":"selecting_backend/","text":"","title":"Selecting backend"},{"location":"selene_backend/","text":"Selene Backend coming soon Installing your own Selene backend","title":"Selene Backend"},{"location":"selene_backend/#selene-backend","text":"coming soon Installing your own Selene backend","title":"Selene Backend"},{"location":"shell/","text":"OVOS Shell OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images Plugins OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider Alternative Clients Other distributions may offer alternative implementations such as: mycroft-gui also hosts a client for developers on the desktop. plasma-bigscreen mycroft mark2 Configuration The Shell can be configured in a few ways. GUI Display settings Color Theme editor Shell Options ~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true Themes Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"OVOS Shell"},{"location":"shell/#ovos-shell","text":"OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images","title":"OVOS Shell"},{"location":"shell/#plugins","text":"OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider","title":"Plugins"},{"location":"shell/#alternative-clients","text":"Other distributions may offer alternative implementations such as: mycroft-gui also hosts a client for developers on the desktop. plasma-bigscreen mycroft mark2","title":"Alternative Clients"},{"location":"shell/#configuration","text":"The Shell can be configured in a few ways.","title":"Configuration"},{"location":"shell/#gui","text":"Display settings Color Theme editor","title":"GUI"},{"location":"shell/#shell-options","text":"~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true","title":"Shell Options"},{"location":"shell/#themes","text":"Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"Themes"},{"location":"spotifyd/","text":"Spotifyd Spotifyd is able to advertise itself on the network without credentials and using zeroconf authentication from Spotify Connect on your mobile device. This is the default configuration shipped with the buildroot image. If for whatever reason zeroconf is not properly working on your network, or you want spotifyd to log in itself you can configure your username and password combination within it's configuration file by uncommenting and configuring the username and password variables within ~/.config/spotifyd/spotifyd.conf and reboot the device or run systemctl --user restart spotifyd . Open spotify on you mobile device and go to the Devices menu within the Settings or tap the devices menu icon on the left bottom of the now playing screen. An OpenVoiceOS \"speaker\" device will be present which you can select as output device. When you play something on Spotify the music will come from your OpenVoiceOS device which will be indicated by the \"OPENVOICEOS\" indicator on the device menu icon on the top bottom of the now playing screen on your mobile device. As Spotifyd has full MPRIS support including audio player controls, the full OCP now playing screen will be shown on your OpenVoiceOS device as shown below, just like playing something from YouTube as shown above.","title":"Spotifyd"},{"location":"spotifyd/#spotifyd","text":"Spotifyd is able to advertise itself on the network without credentials and using zeroconf authentication from Spotify Connect on your mobile device. This is the default configuration shipped with the buildroot image. If for whatever reason zeroconf is not properly working on your network, or you want spotifyd to log in itself you can configure your username and password combination within it's configuration file by uncommenting and configuring the username and password variables within ~/.config/spotifyd/spotifyd.conf and reboot the device or run systemctl --user restart spotifyd . Open spotify on you mobile device and go to the Devices menu within the Settings or tap the devices menu icon on the left bottom of the now playing screen. An OpenVoiceOS \"speaker\" device will be present which you can select as output device. When you play something on Spotify the music will come from your OpenVoiceOS device which will be indicated by the \"OPENVOICEOS\" indicator on the device menu icon on the top bottom of the now playing screen on your mobile device. As Spotifyd has full MPRIS support including audio player controls, the full OCP now playing screen will be shown on your OpenVoiceOS device as shown below, just like playing something from YouTube as shown above.","title":"Spotifyd"},{"location":"statements/","text":"Statements Editors Note This will probably move Speaking a statement One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism. Multilingualism To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\". The Tomato Skill Revisited To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement. Simple statement The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file. File locations The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us Statements with variables The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\". Waiting for speech By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this. Wait Parameter We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\") wait_while_speaking The mycroft.audio.wait_while_speaking() method allows us to execute some code, then wait for OVOS to finish speaking. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Returns from speak_dialog() before the speaking has completed, and logs some info. Then it, waits for the speech to complete. \"\"\" self.speak_dialog('tomato.description') self.log.info(\"I am executed immediately\") wait_while_speaking() self.log.info(\"But I waited for you\") Here we have executed one line of code immediately. Our Skill will then wait for the statement from i.do.like.dialog to be spoken before executing the final line of code. Using translatable resources There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The MycroftSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Statements"},{"location":"statements/#statements","text":"Editors Note This will probably move","title":"Statements"},{"location":"statements/#speaking-a-statement","text":"One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism.","title":"Speaking a statement"},{"location":"statements/#multilingualism","text":"To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\".","title":"Multilingualism"},{"location":"statements/#the-tomato-skill-revisited","text":"To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement.","title":"The Tomato Skill Revisited"},{"location":"statements/#simple-statement","text":"The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file.","title":"Simple statement"},{"location":"statements/#file-locations","text":"The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us","title":"File locations"},{"location":"statements/#statements-with-variables","text":"The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\".","title":"Statements with variables"},{"location":"statements/#waiting-for-speech","text":"By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this.","title":"Waiting for speech"},{"location":"statements/#wait-parameter","text":"We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\")","title":"Wait Parameter"},{"location":"statements/#wait_while_speaking","text":"The mycroft.audio.wait_while_speaking() method allows us to execute some code, then wait for OVOS to finish speaking. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Returns from speak_dialog() before the speaking has completed, and logs some info. Then it, waits for the speech to complete. \"\"\" self.speak_dialog('tomato.description') self.log.info(\"I am executed immediately\") wait_while_speaking() self.log.info(\"But I waited for you\") Here we have executed one line of code immediately. Our Skill will then wait for the statement from i.do.like.dialog to be spoken before executing the final line of code.","title":"wait_while_speaking"},{"location":"statements/#using-translatable-resources","text":"There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The MycroftSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Using translatable resources"},{"location":"stt_plugins/","text":"STT Plugins STT plugins are responsible for converting spoken audio into text List of STT plugins Plugin Offline Type ovos-stt-plugin-vosk yes FOSS ovos-stt-plugin-chromium no API (free) neon-stt-plugin-google_cloud_streaming no API (key) neon-stt-plugin-scribosermo yes FOSS neon-stt-plugin-silero yes FOSS neon-stt-plugin-polyglot yes FOSS neon-stt-plugin-deepspeech_stream_local yes FOSS ovos-stt-plugin-selene no API (free) ovos-stt-plugin-http-server no API (self hosted) ovos-stt-plugin-pocketsphinx yes FOSS Standalone Usage STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang) Plugin Template from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"STT Plugins"},{"location":"stt_plugins/#stt-plugins","text":"STT plugins are responsible for converting spoken audio into text","title":"STT Plugins"},{"location":"stt_plugins/#list-of-stt-plugins","text":"Plugin Offline Type ovos-stt-plugin-vosk yes FOSS ovos-stt-plugin-chromium no API (free) neon-stt-plugin-google_cloud_streaming no API (key) neon-stt-plugin-scribosermo yes FOSS neon-stt-plugin-silero yes FOSS neon-stt-plugin-polyglot yes FOSS neon-stt-plugin-deepspeech_stream_local yes FOSS ovos-stt-plugin-selene no API (free) ovos-stt-plugin-http-server no API (self hosted) ovos-stt-plugin-pocketsphinx yes FOSS","title":"List of STT plugins"},{"location":"stt_plugins/#standalone-usage","text":"STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang)","title":"Standalone Usage"},{"location":"stt_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"stt_server/","text":"OpenVoiceOS STT HTTP Server Turn any OVOS STT plugin into a microservice! Install pip install ovos-stt-http-server Usage ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host Companion plugin Use with OpenVoiceOS companion plugin Docker Template you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"OpenVoiceOS STT HTTP Server"},{"location":"stt_server/#openvoiceos-stt-http-server","text":"Turn any OVOS STT plugin into a microservice!","title":"OpenVoiceOS STT HTTP Server"},{"location":"stt_server/#install","text":"pip install ovos-stt-http-server","title":"Install"},{"location":"stt_server/#usage","text":"ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host","title":"Usage"},{"location":"stt_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"stt_server/#docker-template","text":"you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"Docker Template"},{"location":"translate_server/","text":"OpenVoiceOS Translate Server Turn any OVOS Language plugin into a microservice! Use with OpenVoiceOS companion plugin Install pip install ovos-translate-server Usage ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the Google Translate plugin ovos-translate-server --tx-engine googletranslate_plug --detect-engine googletranslate_detection_plug then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro Docker Template you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"OpenVoiceOS Translate Server"},{"location":"translate_server/#openvoiceos-translate-server","text":"Turn any OVOS Language plugin into a microservice! Use with OpenVoiceOS companion plugin","title":"OpenVoiceOS Translate Server"},{"location":"translate_server/#install","text":"pip install ovos-translate-server","title":"Install"},{"location":"translate_server/#usage","text":"ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the Google Translate plugin ovos-translate-server --tx-engine googletranslate_plug --detect-engine googletranslate_detection_plug then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro","title":"Usage"},{"location":"translate_server/#docker-template","text":"you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"Docker Template"},{"location":"troubleshooting_audio/","text":"Troubleshooting operating system audio If audio isn't working for OpenVoiceOS, it's useful to verify that the operating system audio is working. Architecture ALSA ALSA is the kernel level sound mixer, it manages your sound card directly. ALSA is crap (seriously) and it can handle a few (sometimes just one) channel. We don't generally have to deal with ALSA directly. ALSA can be configured to use PulseAudio as it's default device, that way ALSA applications that are not PulseAudio aware will still use PulseAudio via an indirection layer. Pulse Audio PulseAudio is a software mixer, running in user space. When it runs, it uses Alsa's channel and manages everything. mixing, devices, network devices, etc. PulseAudio always uses ALSA as backend, and on startup opens all ALSA devices. Since most ALSA devices can't be opened multiple times, this will cause all ALSA applications that try to use an ALSA device directly when PulseAudio is running to fail. If you have a legacy application that for some reason doesn't work, you can use pasuspender to temporary suspend PulseAudio to run this particular application. Pulse Audio Modules Troubleshooting Commands List hardware cards cat /proc/asound/cards List Playback and capture devices visible to ALSA (note the Card Number) aplay -l arecord -l This will list the cards, which can then be referenced in arecord using -D hw: ,0: arecord -f dat -r 16000 -D hw:4,0 -c 4 -d 10 test2.wav You can then play the file back to test your speakers aplay -D hw:2,0 test2.wav If PulseAudio is installed, Alsa should be configured to use PulseAudio as it's default, and we don't change anything in Alsa, we configure our default sources and sinks in Pulse Audio Verify that pulseaudio is installed apt list pulseaudio Verify that Alsa is using Pulse Audio as the default $ aplay -L | head -n9 null Discard all samples (playback) or generate zero samples (capture) default Playback/recording through the PulseAudio sound server PulseAudio List sinks (speakers) and sources (microphones) visible to PulseAudio pactl list sinks pactl list sources This will list the sources that can be used to set the default source for pulseaudio below. pacmd set-default-source e.g. pacmd set-default-source alsa_input.usb-OmniVision_Technologies__Inc._USB_Camera-B4.09.24.1-01.multichannel-input Test if OVOS is receiving mic input You can test if OVOS is recieving mic input using the ovos-cli-client Install the ovos-cli-client from github to ensure you have the latest version cd ~ git clone https://github.com/openvoiceos/ovos-cli-client pip install ./ovos-cli-client Run the ovos-cli-client ovos-cli-client In the lower left you can observe the microphone levels, when you talk, the levels should increase. If they don't ovos is probably using the wrong microphone. Gather Data Before submitting an issue or asking a question on Element, please gather the following data. For Microphone issues: arecord -l arecord -L | head -n9 pactl list sources pacmd dump For Speaker issues: aplay -l aplay -L | head -n9 pactl list sinks pacmd dump Additional Resources https://wiki.archlinux.org/title/PulseAudio/Troubleshooting Problem/fix ERROR: pulseaudio sink always suspended Try disabling suspend-on-idle in /etc/pulse/default.pa change this: ### Automatically suspend sinks/sources that become idle for too long load-module module-suspend-on-idle to this: ### Automatically suspend sinks/sources that become idle for too long #load-module module-suspend-on-idle and then to restart it. There is quite a lot of variation in how people do this but killall pulseaudio is one option (it gets automatically started again). If you want to be sure, you can restart the system.","title":"Troubleshooting Audio"},{"location":"troubleshooting_audio/#troubleshooting-operating-system-audio","text":"If audio isn't working for OpenVoiceOS, it's useful to verify that the operating system audio is working.","title":"Troubleshooting operating system audio"},{"location":"troubleshooting_audio/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_audio/#alsa","text":"ALSA is the kernel level sound mixer, it manages your sound card directly. ALSA is crap (seriously) and it can handle a few (sometimes just one) channel. We don't generally have to deal with ALSA directly. ALSA can be configured to use PulseAudio as it's default device, that way ALSA applications that are not PulseAudio aware will still use PulseAudio via an indirection layer.","title":"ALSA"},{"location":"troubleshooting_audio/#pulse-audio","text":"PulseAudio is a software mixer, running in user space. When it runs, it uses Alsa's channel and manages everything. mixing, devices, network devices, etc. PulseAudio always uses ALSA as backend, and on startup opens all ALSA devices. Since most ALSA devices can't be opened multiple times, this will cause all ALSA applications that try to use an ALSA device directly when PulseAudio is running to fail. If you have a legacy application that for some reason doesn't work, you can use pasuspender to temporary suspend PulseAudio to run this particular application. Pulse Audio Modules","title":"Pulse Audio"},{"location":"troubleshooting_audio/#troubleshooting-commands","text":"List hardware cards cat /proc/asound/cards List Playback and capture devices visible to ALSA (note the Card Number) aplay -l arecord -l This will list the cards, which can then be referenced in arecord using -D hw: ,0: arecord -f dat -r 16000 -D hw:4,0 -c 4 -d 10 test2.wav You can then play the file back to test your speakers aplay -D hw:2,0 test2.wav If PulseAudio is installed, Alsa should be configured to use PulseAudio as it's default, and we don't change anything in Alsa, we configure our default sources and sinks in Pulse Audio Verify that pulseaudio is installed apt list pulseaudio Verify that Alsa is using Pulse Audio as the default $ aplay -L | head -n9 null Discard all samples (playback) or generate zero samples (capture) default Playback/recording through the PulseAudio sound server","title":"Troubleshooting Commands"},{"location":"troubleshooting_audio/#pulseaudio","text":"List sinks (speakers) and sources (microphones) visible to PulseAudio pactl list sinks pactl list sources This will list the sources that can be used to set the default source for pulseaudio below. pacmd set-default-source e.g. pacmd set-default-source alsa_input.usb-OmniVision_Technologies__Inc._USB_Camera-B4.09.24.1-01.multichannel-input","title":"PulseAudio"},{"location":"troubleshooting_audio/#test-if-ovos-is-receiving-mic-input","text":"You can test if OVOS is recieving mic input using the ovos-cli-client Install the ovos-cli-client from github to ensure you have the latest version cd ~ git clone https://github.com/openvoiceos/ovos-cli-client pip install ./ovos-cli-client Run the ovos-cli-client ovos-cli-client In the lower left you can observe the microphone levels, when you talk, the levels should increase. If they don't ovos is probably using the wrong microphone.","title":"Test if OVOS is receiving mic input"},{"location":"troubleshooting_audio/#gather-data","text":"Before submitting an issue or asking a question on Element, please gather the following data. For Microphone issues: arecord -l arecord -L | head -n9 pactl list sources pacmd dump For Speaker issues: aplay -l aplay -L | head -n9 pactl list sinks pacmd dump","title":"Gather Data"},{"location":"troubleshooting_audio/#additional-resources","text":"https://wiki.archlinux.org/title/PulseAudio/Troubleshooting","title":"Additional Resources"},{"location":"troubleshooting_audio/#problemfix","text":"","title":"Problem/fix"},{"location":"troubleshooting_audio/#error-pulseaudio-sink-always-suspended","text":"","title":"ERROR: pulseaudio sink always suspended"},{"location":"troubleshooting_audio/#try-disabling-suspend-on-idle-in-etcpulsedefaultpa","text":"change this: ### Automatically suspend sinks/sources that become idle for too long load-module module-suspend-on-idle to this: ### Automatically suspend sinks/sources that become idle for too long #load-module module-suspend-on-idle and then to restart it. There is quite a lot of variation in how people do this but killall pulseaudio is one option (it gets automatically started again). If you want to be sure, you can restart the system.","title":"Try disabling suspend-on-idle in /etc/pulse/default.pa"},{"location":"troubleshooting_backend/","text":"Troubleshooting Backend Configuration Architecture Troubleshooting Commands Problem/Fix","title":"Troubleshooting Backend"},{"location":"troubleshooting_backend/#troubleshooting-backend-configuration","text":"","title":"Troubleshooting Backend Configuration"},{"location":"troubleshooting_backend/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_backend/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_backend/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_installation/","text":"Troubleshooting OpenVoiceOS Installation coming soon Architecture Troubleshooting Commands Gather Data Problem/Fix Install fails to create OVOS wifi hotspot There is a known issue with balena (the wifi access point app) when it detects a WPA3 network of any sort, it fails. More Information","title":"Troubleshooting Installation"},{"location":"troubleshooting_installation/#troubleshooting-openvoiceos-installation","text":"coming soon","title":"Troubleshooting OpenVoiceOS Installation"},{"location":"troubleshooting_installation/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_installation/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_installation/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_installation/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_installation/#install-fails-to-create-ovos-wifi-hotspot","text":"There is a known issue with balena (the wifi access point app) when it detects a WPA3 network of any sort, it fails. More Information","title":"Install fails to create OVOS wifi hotspot"},{"location":"troubleshooting_intro/","text":"Introduction to Troubleshooting OpenVoiceOS Coming soon Architecture Troubleshooting Commands Problem/Fix locale.Error: unsupported locale setting This error could from the ovos-cli-client, or other sources. To resolve, ensure that your locale is set correctly, try running raspi-config to set it if your on Raspberry Pi OS (Raspbian). Manually update /etc/default/locale use locale to verify your current locale, and locale -a to verify the local you've set is actually available. (Source)[https://stackoverflow.com/questions/14547631/python-locale-error-unsupported-locale-setting]","title":"Introduction to Troubleshooting"},{"location":"troubleshooting_intro/#introduction-to-troubleshooting-openvoiceos","text":"Coming soon","title":"Introduction to Troubleshooting OpenVoiceOS"},{"location":"troubleshooting_intro/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_intro/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_intro/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_intro/#localeerror-unsupported-locale-setting","text":"This error could from the ovos-cli-client, or other sources. To resolve, ensure that your locale is set correctly, try running raspi-config to set it if your on Raspberry Pi OS (Raspbian). Manually update /etc/default/locale use locale to verify your current locale, and locale -a to verify the local you've set is actually available. (Source)[https://stackoverflow.com/questions/14547631/python-locale-error-unsupported-locale-setting]","title":"locale.Error: unsupported locale setting"},{"location":"troubleshooting_network/","text":"Troubleshooting Networking Architecture Troubleshooting Commands Problem/Fix See Also Troubleshooting Installation","title":"Troubleshooting Network"},{"location":"troubleshooting_network/#troubleshooting-networking","text":"","title":"Troubleshooting Networking"},{"location":"troubleshooting_network/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_network/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_network/#problemfix","text":"See Also Troubleshooting Installation","title":"Problem/Fix"},{"location":"troubleshooting_ovos_core/","text":"Troubleshooting OVOS Core Coming Soon Architecture Troubleshooting Commands Problem/Fix","title":"Troubleshooting OVOS Core"},{"location":"troubleshooting_ovos_core/#troubleshooting-ovos-core","text":"Coming Soon","title":"Troubleshooting OVOS Core"},{"location":"troubleshooting_ovos_core/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_ovos_core/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_ovos_core/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_plugins/","text":"Troubleshooting Plugins Architecture Troubleshooting Commands Gather Data Problem/Fix","title":"Troubleshooting Plugins"},{"location":"troubleshooting_plugins/#troubleshooting-plugins","text":"","title":"Troubleshooting Plugins"},{"location":"troubleshooting_plugins/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_plugins/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_plugins/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_plugins/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_skills/","text":"Troubleshooting Skills Architecture Troubleshooting Commands Gather Data Problem/Fix","title":"Troubleshooting Skills"},{"location":"troubleshooting_skills/#troubleshooting-skills","text":"","title":"Troubleshooting Skills"},{"location":"troubleshooting_skills/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_skills/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_skills/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_skills/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_stt/","text":"Troubleshooting Speech To Text Architecture Troubleshooting Commands Gather Data Problem/Fix","title":"Troubleshooting STT"},{"location":"troubleshooting_stt/#troubleshooting-speech-to-text","text":"","title":"Troubleshooting Speech To Text"},{"location":"troubleshooting_stt/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_stt/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_stt/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_stt/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_tts/","text":"Troubleshooting Text To Speech Architecture Troubleshooting Commands Gather Data Problem/Fix","title":"Troubleshooting TTS"},{"location":"troubleshooting_tts/#troubleshooting-text-to-speech","text":"","title":"Troubleshooting Text To Speech"},{"location":"troubleshooting_tts/#architecture","text":"","title":"Architecture"},{"location":"troubleshooting_tts/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_tts/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_tts/#problemfix","text":"","title":"Problem/Fix"},{"location":"troubleshooting_wakeword/","text":"Troubleshooting Wake Word Architecture OpenVoiceOS uses a plugin (or plugins) to recognize the wake word. In your mycroft.conf file you'll specify the plugin used and what the wakeword is. Troubleshooting Commands Gather Data Verify it's the Wake Word and not the microphone To verify that it is the Wake Word and not the microphone causing the issue, we will get OVOS to ask us a question that we can respond to. In the OVOS cli type in the utterance \"set timer\". OVOS will then ask how long of a timer you would like. Speaking now should result in your utterance being transcribed. If your response is successfully transcribed, it is most likely the Wake Word engine causing the problem. Check your mycroft.conf file to see what plugin and wake word is configured. Look at your mycoft.conf file and verify how your wake word is configured. You can use grep mycroft.conf /var/log/syslog to determine the location of your configuration files that were loaded. Look for the following lines (or similar): \"wake_word\": \"hey_mycroft\" ## this will match a hotword listed later in the config ... \"hotwords\": { \"hey_mycroft\": { ## matches the wake_word \"module\": \"ovos-ww-plugin-precise\", ## what plugin is used \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true, \"sound\": \"snd/start_listening.wav\" }, Verify the Wake Word plugin is loading properly grep wake /var/log/syslog Look for lines similar to: voice - ovos_plugin_manager.wakewords:load_module:85 - INFO - Loading \"wake_up\" wake word via ovos-ww-plugin-pocketsphinx voice - ovos_plugin_manager.wakewords:load_module:92 - INFO - Loaded the Wake Word plugin ovos-ww-plugin-pocketsphinx voice - ovos_plugin_manager.wakewords:load_module:85 - INFO - Loading \"hey_mycroft\" wake word via ovos-ww-plugin-precise voice - ovos_plugin_manager.wakewords:load_module:92 - INFO - Loaded the Wake Word plugin ovos-ww-plugin-precise If you see an error about \"failed to load plugin\" make sure the plugin is installed. Problem/Fix","title":"Troubleshooting Wakeword"},{"location":"troubleshooting_wakeword/#troubleshooting-wake-word","text":"","title":"Troubleshooting Wake Word"},{"location":"troubleshooting_wakeword/#architecture","text":"OpenVoiceOS uses a plugin (or plugins) to recognize the wake word. In your mycroft.conf file you'll specify the plugin used and what the wakeword is.","title":"Architecture"},{"location":"troubleshooting_wakeword/#troubleshooting-commands","text":"","title":"Troubleshooting Commands"},{"location":"troubleshooting_wakeword/#gather-data","text":"","title":"Gather Data"},{"location":"troubleshooting_wakeword/#verify-its-the-wake-word-and-not-the-microphone","text":"To verify that it is the Wake Word and not the microphone causing the issue, we will get OVOS to ask us a question that we can respond to. In the OVOS cli type in the utterance \"set timer\". OVOS will then ask how long of a timer you would like. Speaking now should result in your utterance being transcribed. If your response is successfully transcribed, it is most likely the Wake Word engine causing the problem.","title":"Verify it's the Wake Word and not the microphone"},{"location":"troubleshooting_wakeword/#check-your-mycroftconf-file-to-see-what-plugin-and-wake-word-is-configured","text":"Look at your mycoft.conf file and verify how your wake word is configured. You can use grep mycroft.conf /var/log/syslog to determine the location of your configuration files that were loaded. Look for the following lines (or similar): \"wake_word\": \"hey_mycroft\" ## this will match a hotword listed later in the config ... \"hotwords\": { \"hey_mycroft\": { ## matches the wake_word \"module\": \"ovos-ww-plugin-precise\", ## what plugin is used \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true, \"sound\": \"snd/start_listening.wav\" },","title":"Check your mycroft.conf file to see what plugin and wake word is configured."},{"location":"troubleshooting_wakeword/#verify-the-wake-word-plugin-is-loading-properly","text":"grep wake /var/log/syslog Look for lines similar to: voice - ovos_plugin_manager.wakewords:load_module:85 - INFO - Loading \"wake_up\" wake word via ovos-ww-plugin-pocketsphinx voice - ovos_plugin_manager.wakewords:load_module:92 - INFO - Loaded the Wake Word plugin ovos-ww-plugin-pocketsphinx voice - ovos_plugin_manager.wakewords:load_module:85 - INFO - Loading \"hey_mycroft\" wake word via ovos-ww-plugin-precise voice - ovos_plugin_manager.wakewords:load_module:92 - INFO - Loaded the Wake Word plugin ovos-ww-plugin-precise If you see an error about \"failed to load plugin\" make sure the plugin is installed.","title":"Verify the Wake Word plugin is loading properly"},{"location":"troubleshooting_wakeword/#problemfix","text":"","title":"Problem/Fix"},{"location":"tts_plugins/","text":"TTS Plugins TTS plugins are responsible for converting text into audio for playback List of TTS plugins Plugin Offline Type ovos-tts-plugin-mimic yes FOSS ovos-tts-plugin-mimic2 no API (free) ovos-tts-plugin-mimic3 yes FOSS ovos-tts-plugin-marytts no API (self hosted) neon-tts-plugin-larynx_server no API (self hosted) ovos-tts-server-plugin no API (self hosted) ovos-tts-plugin-pico yes FOSS neon-tts-plugin-glados yes FOSS neon-tts-plugin-mozilla_local yes FOSS neon-tts-plugin-polly no API (key) ovos-tts-plugin-voicerss no API (key) ovos-tts-plugin-google-TX no API (free) ovos-tts-plugin-responsivevoice no API (free) neon-tts-plugin-mozilla_remote no API (self hosted) neon-tts-plugin-tacotron2 yes FOSS ovos-tts-plugin-espeakNG yes FOSS ovos-tts-plugin-cotovia yes FOSS ovos-tts-plugin-catotron no API (self hosted) ovos-tts-plugin-softcatala no API (self hosted) ovos-tts-plugin-SAM yes Abandonware ovos-tts-plugin-beepspeak yes Fun Plugin Template from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"TTS Plugins"},{"location":"tts_plugins/#tts-plugins","text":"TTS plugins are responsible for converting text into audio for playback","title":"TTS Plugins"},{"location":"tts_plugins/#list-of-tts-plugins","text":"Plugin Offline Type ovos-tts-plugin-mimic yes FOSS ovos-tts-plugin-mimic2 no API (free) ovos-tts-plugin-mimic3 yes FOSS ovos-tts-plugin-marytts no API (self hosted) neon-tts-plugin-larynx_server no API (self hosted) ovos-tts-server-plugin no API (self hosted) ovos-tts-plugin-pico yes FOSS neon-tts-plugin-glados yes FOSS neon-tts-plugin-mozilla_local yes FOSS neon-tts-plugin-polly no API (key) ovos-tts-plugin-voicerss no API (key) ovos-tts-plugin-google-TX no API (free) ovos-tts-plugin-responsivevoice no API (free) neon-tts-plugin-mozilla_remote no API (self hosted) neon-tts-plugin-tacotron2 yes FOSS ovos-tts-plugin-espeakNG yes FOSS ovos-tts-plugin-cotovia yes FOSS ovos-tts-plugin-catotron no API (self hosted) ovos-tts-plugin-softcatala no API (self hosted) ovos-tts-plugin-SAM yes Abandonware ovos-tts-plugin-beepspeak yes Fun","title":"List of TTS plugins"},{"location":"tts_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"tts_server/","text":"OpenVoiceOS TTS Server Turn any OVOS TTS plugin into a microservice! Install pip install ovos-tts-server Usage ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello Companion plugin Use with OpenVoiceOS companion plugin Docker Template you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"OpenVoiceOS TTS Server"},{"location":"tts_server/#openvoiceos-tts-server","text":"Turn any OVOS TTS plugin into a microservice!","title":"OpenVoiceOS TTS Server"},{"location":"tts_server/#install","text":"pip install ovos-tts-server","title":"Install"},{"location":"tts_server/#usage","text":"ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello","title":"Usage"},{"location":"tts_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"tts_server/#docker-template","text":"you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"Docker Template"},{"location":"vad_plugins/","text":"VAD Plugins Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not List of VAD plugins Plugin Type ovos-vad-plugin-silero model ovos-vad-plugin-webrtcvad model","title":"VAD Plugins"},{"location":"vad_plugins/#vad-plugins","text":"Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not","title":"VAD Plugins"},{"location":"vad_plugins/#list-of-vad-plugins","text":"Plugin Type ovos-vad-plugin-silero model ovos-vad-plugin-webrtcvad model","title":"List of VAD plugins"}]}